{"paperId": "00008e0684489fec67368a8074e2bc9a7415e3c3", "url": "https://www.semanticscholar.org/paper/00008e0684489fec67368a8074e2bc9a7415e3c3", "title": "Benchmarking Multimodal Large Language Models for Face Recognition", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-16", "authors": [{"authorId": "1411341123", "name": "Hatef Otroshi-Shahreza"}, {"authorId": "2237967482", "name": "Sébastien Marcel"}], "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page."}
{"paperId": "0062f83369af2d4864286edaa910f83701764948", "url": "https://www.semanticscholar.org/paper/0062f83369af2d4864286edaa910f83701764948", "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.14749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-19", "authors": [{"authorId": "3379322", "name": "Wai Keen Vong"}, {"authorId": "2373318", "name": "B. Lake"}], "abstract": "What insights can machine learning bring to understanding human language acquisition? Large language and multimodal models have achieved remarkable capabilities, but their reliance on massive training datasets creates a fundamental mismatch with children, who succeed in acquiring language from comparatively limited input. To help bridge this gap, researchers have increasingly trained neural networks using data similar in quantity and quality to children's input. Taking this approach to the limit, Vong et al. (2024) showed that a multimodal neural network trained on 61 hours of visual and linguistic input extracted from just one child's developmental experience could acquire word-referent mappings. However, whether this approach's success reflects the idiosyncrasies of a single child's experience, or whether it would show consistent and robust learning patterns across multiple children's experiences was not explored. In this article, we applied automated speech transcription methods to the entirety of the SAYCam dataset, consisting of over 500 hours of video data spread across all three children. Using these automated transcriptions, we generated multi-modal vision-and-language datasets for both training and evaluation, and explored a range of neural network configurations to examine the robustness of simulated word learning. Our findings demonstrate that networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures. These results validate the robustness of multimodal neural networks for grounded word learning, while highlighting the individual differences that emerge in how models learn when trained on each child's developmental experiences."}
{"paperId": "00898832db811879e214e664e9fddbf08374a9a2", "url": "https://www.semanticscholar.org/paper/00898832db811879e214e664e9fddbf08374a9a2", "title": "SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2281707946", "name": "Thong Nguyen"}, {"authorId": "2166060981", "name": "Yibin Lei"}, {"authorId": "2360616724", "name": "Jia-Huei Ju"}, {"authorId": "2287929110", "name": "Andrew Yates"}], "abstract": "Visual Document Retrieval (VDR) typically operates as text-to-image retrieval using specialized bi-encoders trained to directly embed document images. We revisit a zero-shot generate-and-encode pipeline: a vision-language model first produces a detailed textual description of each document image, which is then embedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual document encoder. It also scales better to large collections and offers broader multilingual coverage. Analysis shows that modern vision-language models capture complex textual and visual cues with sufficient granularity to act as a reusable semantic proxy. By offloading modality alignment to pretrained vision-language models, our approach removes the need for computationally intensive text-image contrastive training and establishes a strong zero-shot baseline for future VDR systems."}
{"paperId": "00f922a769f9b34d0a5ba201e24f8be4936008d3", "url": "https://www.semanticscholar.org/paper/00f922a769f9b34d0a5ba201e24f8be4936008d3", "title": "History-Aware Multimodal Instruction-Oriented Policies for Navigation Tasks", "venue": "Applied Informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "https://doi.org/10.3390/ai6040075", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6040075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6040075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2355196588", "name": "Renas Mukhametzianov"}, {"authorId": "1778254", "name": "Hidetaka Nambo"}], "abstract": "The rise of large-scale language models and multimodal transformers has enabled instruction-based policies, such as vision-and-language navigation. To leverage their general world knowledge, we propose multimodal annotations for action options and support selection from a dynamic, describable action space. Our framework employs a multimodal transformer that processes front-facing camera images, light detection and ranging (LIDAR) sensor’s point clouds, and tasks as textual instructions to produce a history-aware decision policy for mobile robot navigation. Our approach leverages a pretrained vision–language encoder and integrates it with a custom causal generative pretrained transformer (GPT) decoder to predict action sequences within a state–action history. We propose a trainable attention score mechanism to efficiently select the most suitable action from a variable set of possible options. Action options are text–image pairs and encoded using the same multimodal encoder employed for environment states. This approach of annotating and dynamically selecting actions is applicable to broader multidomain decision-making tasks. We compared two baseline models, ViLT (vision-and-language transformer) and FLAVA (foundational language and vision alignment), and found that FLAVA achieves superior performance within the constraints of 8 GB video memory usage in the training phase. Experiments were conducted in both simulated and real-world environments using our custom datasets for instructed task completion episodes, demonstrating strong prediction accuracy. These results highlight the potential of multimodal, dynamic action spaces for instruction-based robot navigation and beyond."}
{"paperId": "013608d244aef491f0d4615ce551f28bd5a67fac", "url": "https://www.semanticscholar.org/paper/013608d244aef491f0d4615ce551f28bd5a67fac", "title": "Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-30", "authors": [{"authorId": "2118771300", "name": "Ying Yang"}, {"authorId": "2334528648", "name": "Jie Zhang"}, {"authorId": "2208206694", "name": "Xiao Lv"}, {"authorId": "2333428131", "name": "Di Lin"}, {"authorId": "2120489252", "name": "Tao Xiang"}, {"authorId": "2335194321", "name": "Qing Guo"}], "abstract": "While adversarial attacks on vision-and-language pretraining (VLP) models have been explored, generating natural adversarial samples crafted through realistic and semantically meaningful perturbations remains an open challenge. Existing methods, primarily designed for classification tasks, struggle when adapted to VLP models due to their restricted optimization spaces, leading to ineffective attacks or unnatural artifacts. To address this, we propose \\textbf{LightD}, a novel framework that generates natural adversarial samples for VLP models via semantically guided relighting. Specifically, LightD leverages ChatGPT to propose context-aware initial lighting parameters and integrates a pretrained relighting model (IC-light) to enable diverse lighting adjustments. LightD expands the optimization space while ensuring perturbations align with scene semantics. Additionally, gradient-based optimization is applied to the reference lighting image to further enhance attack effectiveness while maintaining visual naturalness. The effectiveness and superiority of the proposed LightD have been demonstrated across various VLP models in tasks such as image captioning and visual question answering."}
{"paperId": "014985747e905fa3e2c182d3e8f132d92936c833", "url": "https://www.semanticscholar.org/paper/014985747e905fa3e2c182d3e8f132d92936c833", "title": "A Survey of Transformer Optimization Techniques: Progress and Challenges from Computational Efficiency to Multimodal Fusion", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.po24682?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.po24682, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-04", "authors": [{"authorId": "2370310540", "name": "Chuhao Xiong"}], "abstract": "Since its proposal in 2017, the Transformer model has achieved revolutionary breakthroughs in natural language processing and even in computer vision tasks. However, its huge number of parameters and high computational complexity have posed substantial difficulties in training and inference efficiency, model knowledge updating, and multimodal information fusion. This paper reviews recent research progress on Transformer optimization techniques, including: (1) Structural optimization and computational efficiency model architecture improvements, pruning compression, and efficient attention mechanisms to reduce computational cost; (2) Parameter-efficient fine-tuning and task adaptation new fine-tuning methods with high parameter efficiency, as well as few-shot and zero-shot learning paradigms, to improve adaptability in low-resource and multi-task scenarios; (3) External knowledge integration incorporating knowledge graphs, retrieval-based external memory, etc., into Transformers to fill knowledge gaps and enhance commonsense reasoning; (4) Multimodal fusion designing cross-modal Transformer architectures and alignment mechanisms to effectively fuse information from modalities such as vision and language. Analyze representative methods, underlying principles, and experimental results for each direction, discuss the main challenges, and predict forthcoming research directions, such as unified efficient attention theories, trustworthy knowledge injection mechanisms, green AI training strategies, and next-generation interpretable Transformer architectures."}
{"paperId": "016a475f4f077aef646d8cccccb14219a672153f", "url": "https://www.semanticscholar.org/paper/016a475f4f077aef646d8cccccb14219a672153f", "title": "SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.12007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-13", "authors": [{"authorId": "2147369639", "name": "P. Setinek"}, {"authorId": "2248231920", "name": "G. Galletti"}, {"authorId": "2367044143", "name": "Thomas Gross"}, {"authorId": "2370858001", "name": "Dominik Schnürer"}, {"authorId": "2283764053", "name": "Johannes Brandstetter"}, {"authorId": "2367044805", "name": "Werner Zellinger"}], "abstract": "Neural surrogates for Partial Differential Equations (PDEs) often suffer significant performance degradation when evaluated on unseen problem configurations, such as novel material types or structural dimensions. Meanwhile, Domain Adaptation (DA) techniques have been widely used in vision and language processing to generalize from limited information about unseen configurations. In this work, we address this gap through two focused contributions. First, we introduce SIMSHIFT, a novel benchmark dataset and evaluation suite composed of four industrial simulation tasks: hot rolling, sheet metal forming, electric motor design and heatsink design. Second, we extend established domain adaptation methods to state of the art neural surrogates and systematically evaluate them. These approaches use parametric descriptions and ground truth simulations from multiple source configurations, together with only parametric descriptions from target configurations. The goal is to accurately predict target simulations without access to ground truth simulation data. Extensive experiments on SIMSHIFT highlight the challenges of out of distribution neural surrogate modeling, demonstrate the potential of DA in simulation, and reveal open problems in achieving robust neural surrogates under distribution shifts in industrially relevant scenarios. Our codebase is available at https://github.com/psetinek/simshift"}
{"paperId": "016f0a920ad8bc5ac925cc20917fc1e436f71702", "url": "https://www.semanticscholar.org/paper/016f0a920ad8bc5ac925cc20917fc1e436f71702", "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-25", "authors": [{"authorId": "2345819164", "name": "Vladan Stojni'c"}, {"authorId": "1944225", "name": "Yannis Kalantidis"}, {"authorId": "2000570582", "name": "Jivr'i Matas"}, {"authorId": "1706195", "name": "Giorgos Tolias"}], "abstract": "We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS"}
{"paperId": "017348e48ab6c2e80686f3b7648f35f732f063d5", "url": "https://www.semanticscholar.org/paper/017348e48ab6c2e80686f3b7648f35f732f063d5", "title": "Boosting Efficient Reinforcement Learning for Vision-and-Language Navigation With Open-Sourced LLM", "venue": "IEEE Robotics and Automation Letters", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LRA.2024.3511402?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LRA.2024.3511402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2274204315", "name": "Jiawei Wang"}, {"authorId": "2268808822", "name": "Teng Wang"}, {"authorId": "2113721415", "name": "Wenzhe Cai"}, {"authorId": "2197777612", "name": "Lele Xu"}, {"authorId": "2243900379", "name": "Changyin Sun"}], "abstract": null}
{"paperId": "01911a0ab9086dc896026315291d049ad686abbb", "url": "https://www.semanticscholar.org/paper/01911a0ab9086dc896026315291d049ad686abbb", "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-11", "authors": [{"authorId": "2380755265", "name": "Yongqiang Yu"}, {"authorId": "2391639782", "name": "Xuhui Li"}, {"authorId": "2397761784", "name": "Hazza Mahmood"}, {"authorId": "2383558633", "name": "Jinxing Zhou"}, {"authorId": "2305123085", "name": "Haodong Hong"}, {"authorId": "2382942328", "name": "Longtao Jiang"}, {"authorId": null, "name": "Zhiqiang Xu"}, {"authorId": "2304606995", "name": "Qi Wu"}, {"authorId": null, "name": "Xiaojun Chang"}], "abstract": "Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions."}
{"paperId": "01ff6c7aab34d76a5be9c095d99fd779e68a87de", "url": "https://www.semanticscholar.org/paper/01ff6c7aab34d76a5be9c095d99fd779e68a87de", "title": "See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.09809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-14", "authors": [{"authorId": "2116785555", "name": "Zhimin Li"}, {"authorId": "2253579462", "name": "Haichao Miao"}, {"authorId": "2355572760", "name": "Xinyuan Yan"}, {"authorId": "2251997858", "name": "Valerio Pascucci"}, {"authorId": "2275614676", "name": "Matthew Berger"}, {"authorId": "2289268771", "name": "Shusen Liu"}], "abstract": "Recent developments in multimodal large language models (MLLM) have equipped language models to reason about vision and language jointly. This permits MLLMs to both perceive and answer questions about data visualization across a variety of designs and tasks. Applying MLLMs to a broad range of visualization tasks requires us to properly evaluate their capabilities, and the most common way to conduct evaluation is through measuring a model's visualization reasoning capability, analogous to how we would evaluate human understanding of visualizations (e.g., visualization literacy). However, we found that in the context of visualization question answering (VisQA), how an MLLM perceives and reasons about visualizations can be fundamentally different from how humans approach the same problem. During the evaluation, even without visualization, the model could correctly answer a substantial portion of the visualization test questions, regardless of whether any selection options were provided. We hypothesize that the vast amount of knowledge encoded in the language model permits factual recall that supersedes the need to seek information from the visual signal. It raises concerns that the current VisQA evaluation may not fully capture the models' visualization reasoning capabilities. To address this, we propose a comprehensive sanity check framework that integrates a rule-based decision tree and a sanity check table to disentangle the effects of\"seeing\"(visual processing) and\"recall\"(reliance on prior knowledge). This validates VisQA datasets for evaluation, highlighting where models are truly\"seeing\", positively or negatively affected by the factual recall, or relying on inductive biases for question answering. Our study underscores the need for careful consideration in designing future visualization understanding studies when utilizing MLLMs."}
{"paperId": "026901c8c3099df6ddb86074019a3ec1db728cc0", "url": "https://www.semanticscholar.org/paper/026901c8c3099df6ddb86074019a3ec1db728cc0", "title": "Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.09230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-08-12", "authors": [{"authorId": "2279767275", "name": "Yutong Wu"}, {"authorId": "2363008978", "name": "Jie Zhang"}, {"authorId": "2302527021", "name": "Yiming Li"}, {"authorId": "2256775799", "name": "Chao Zhang"}, {"authorId": "2323108429", "name": "Qing Guo"}, {"authorId": "2355870120", "name": "Nils Lukas"}, {"authorId": "2311284368", "name": "Tianwei Zhang"}], "abstract": "Vision Language Model (VLM)-based agents are stateful, autonomous entities capable of perceiving and interacting with their environments through vision and language. Multi-agent systems comprise specialized agents who collaborate to solve a (complex) task. A core security property is robustness, stating that the system should maintain its integrity under adversarial attacks. However, the design of existing multi-agent systems lacks the robustness consideration, as a successful exploit against one agent can spread and infect other agents to undermine the entire system's assurance. To address this, we propose a new defense approach, Cowpox, to provably enhance the robustness of multi-agent systems. It incorporates a distributed mechanism, which improves the recovery rate of agents by limiting the expected number of infections to other agents. The core idea is to generate and distribute a special cure sample that immunizes an agent against the attack before exposure and helps recover the already infected agents. We demonstrate the effectiveness of Cowpox empirically and provide theoretical robustness guarantees."}
{"paperId": "027171757e906872dc74e96334203ac8084d7f6f", "url": "https://www.semanticscholar.org/paper/027171757e906872dc74e96334203ac8084d7f6f", "title": "Knowledge Induced Graph Transformer for Visual Commonsense Reasoning", "venue": "2025 International Conference on Emerging Techniques in Computational Intelligence (ICETCI)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICETCI67340.2025.11257936?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICETCI67340.2025.11257936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-21", "authors": [{"authorId": "2395938442", "name": "Pavan Kalyan Pasula"}, {"authorId": "2395937940", "name": "Surya Tejaswi Nyayapathi"}, {"authorId": "145011681", "name": "Kovvur Ram Mohan Rao"}], "abstract": "Visual commonsense reasoning (VCR) is a demanding task that requires models to answer questions about images by leveraging commonsense knowledge. Previous approaches have made progress on VCR by better modeling the complex correlations between vision and language. However, they still lack an explicit mechanism to incorporate broad commonsense knowledge beyond what is contained in the training data. To address this, we propose augmenting a graph transformer framework with ConceptNet, a large knowledge base encoding commonsense relationships between concepts. Specifically, we integrate ConceptNet knowledge into the structure injecting and graph transformer modules, allowing the model to reason using both the visual and linguistic inputs as well as background commonsense knowledge. Our experiments on the VCR benchmark show that incorporating ConceptNet leads to significant performance gains, demonstrating the importance of leveraging structured commonsense knowledge for visual reasoning. The proposed knowledge-enriched model sets a new benchmark for performance on the VCR dataset."}
{"paperId": "0278b9060d6877b608b817e0bbb33fbb5c476f34", "url": "https://www.semanticscholar.org/paper/0278b9060d6877b608b817e0bbb33fbb5c476f34", "title": "A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2025.3621246?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2025.3621246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-14", "authors": [{"authorId": "2363675643", "name": "Weixin Ye"}, {"authorId": "2346143424", "name": "Wei Wang"}, {"authorId": "2367201392", "name": "Yahui Liu"}, {"authorId": "2152601878", "name": "Yue Song"}, {"authorId": "2317011851", "name": "Bin Ren"}, {"authorId": "2385760403", "name": "Wei Bi"}, {"authorId": "2303850502", "name": "Rita Cucchiara"}, {"authorId": "1429806753", "name": "N. Sebe"}], "abstract": "In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable unknown (unk) position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models' robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (e.g., ImageNet-1K) and sentiment analysis for text (e.g., Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via https://github.com/ywxsuperstar/transformerattack."}
{"paperId": "027b64cd99b972f59db34675398d426116960e56", "url": "https://www.semanticscholar.org/paper/027b64cd99b972f59db34675398d426116960e56", "title": "Remote Sensing Image Change Captioning: A Comprehensive Review", "venue": "International Journal of Multimedia Information Retrieval", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13735-025-00375-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13735-025-00375-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-14", "authors": [{"authorId": "2284343062", "name": "Shiwei Zou"}, {"authorId": "2283823914", "name": "Yingmei Wei"}, {"authorId": "2284528508", "name": "Yuxiang Xie"}, {"authorId": "50974488", "name": "Mingrui Lao"}, {"authorId": "3276061", "name": "Xidao Luan"}], "abstract": null}
{"paperId": "02a5592c31d94ce9512e042701c7abeb82e4e140", "url": "https://www.semanticscholar.org/paper/02a5592c31d94ce9512e042701c7abeb82e4e140", "title": "What Does It Take to Build a Performant Selective Classifier?", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-23", "authors": [{"authorId": "29938263", "name": "Stephan Rabanser"}, {"authorId": "1967156", "name": "Nicolas Papernot"}], "abstract": "Selective classifiers improve model reliability by abstaining on inputs the model deems uncertain. However, few practical approaches achieve the gold-standard performance of a perfect-ordering oracle that accepts examples exactly in order of correctness. Our work formalizes this shortfall as the selective-classification gap and present the first finite-sample decomposition of this gap to five distinct sources of looseness: Bayes noise, approximation error, ranking error, statistical noise, and implementation- or shift-induced slack. Crucially, our analysis reveals that monotone post-hoc calibration -- often believed to strengthen selective classifiers -- has limited impact on closing this gap, since it rarely alters the model's underlying score ranking. Bridging the gap therefore requires scoring mechanisms that can effectively reorder predictions rather than merely rescale them. We validate our decomposition on synthetic two-moons data and on real-world vision and language benchmarks, isolating each error component through controlled experiments. Our results confirm that (i) Bayes noise and limited model capacity can account for substantial gaps, (ii) only richer, feature-aware calibrators meaningfully improve score ordering, and (iii) data shift introduces a separate slack that demands distributionally robust training. Together, our decomposition yields a quantitative error budget as well as actionable design guidelines that practitioners can use to build selective classifiers which approximate ideal oracle behavior more closely."}
{"paperId": "02bd553cbf7212427c1e505f68dfa82292dd276c", "url": "https://www.semanticscholar.org/paper/02bd553cbf7212427c1e505f68dfa82292dd276c", "title": "FATE: Feature-Adapted Parameter Tuning for Vision-Language Models", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i9.32975", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i9.32975?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i9.32975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2257421982", "name": "Zhengqin Xu"}, {"authorId": "2293667756", "name": "Zelin Peng"}, {"authorId": "2257369827", "name": "Xiaokang Yang"}, {"authorId": "2273650203", "name": "Wei Shen"}], "abstract": "Following the recent popularity of vision language models, several attempts, e.g., parameter-efficient fine-tuning (PEFT), have been made to extend them to different downstream tasks. Previous PEFT works motivate their methods from the view of introducing new parameters for adaptation but still need to learn this part of weight from scratch, i.e., random initialization. In this paper, we present a novel strategy that incorporates the potential of prompts, e.g., vision features, to facilitate the initial parameter space adapting to new scenarios. We introduce a Feature-Adapted parameTer Efficient tuning paradigm for vision-language models, dubbed as FATE, which injects informative features from the vision encoder into language encoder's parameters space. Specifically, we extract vision features from the last layer of CLIP's vision encoder and, after projection, treat them as parameters for fine-tuning each layer of CLIP's language encoder. By adjusting these feature-adapted parameters, we can directly enable communication between the vision and language branches, facilitating CLIP's adaptation to different scenarios. Experimental results show that FATE exhibits superior generalization performance on 11 datasets with a very small amount of extra parameters and computation."}
{"paperId": "032314f897d69b8e1c91e3f6c2519332a3b445ec", "url": "https://www.semanticscholar.org/paper/032314f897d69b8e1c91e3f6c2519332a3b445ec", "title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.21863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-26", "authors": [{"authorId": "70213349", "name": "Sungjune Park"}, {"authorId": "2565392", "name": "Yeongyun Kim"}, {"authorId": "2337837748", "name": "Se Yeon Kim"}, {"authorId": "2075377906", "name": "Y. Ro"}], "abstract": "Large Vision and Language Models (LVLMs) have shown strong performance across various vision-language tasks in natural image domains. However, their application to remote sensing (RS) remains underexplored due to significant domain differences in visual appearances, object scales, and semantics. These discrepancies hider the effective understanding of RS scenes, which contain rich, multi-level semantic information spanning from coarse-to-fine levels. Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To address this gap, we propose a novel LVLM framework tailored for RS understanding, incorporating two core components: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. First, to align multi-level visual features, we introduce the retrieval-based Semantic Augmentation Module which enriches the visual features with relevant semantics across fine-to-coarse levels (e.g., object- and scene-level information). It is designed to retrieve relevant semantic cues from a RS semantic knowledge database, followed by aggregation of semantic cues with user query and multi-level visual features, resulting in semantically enriched representation across multiple levels. Second, for Semantic-aware Expert Modeling, we design semantic experts, where each expert is responsible for processing semantic representation at different levels separately. This enables hierarchical semantic understanding from coarse to fine levels. Evaluations across multiple RS tasks-including scene classification and VQA, etc.-demonstrate that the proposed framework achieves consistent improvements across multiple semantic levels. This highlights its capability and effectiveness in bridging the gap between general LVLMs and unique demands of RS-specific vision-language understanding."}
{"paperId": "034563a114bd64e5a6e6fda0cce9a2ef3f1e2ca5", "url": "https://www.semanticscholar.org/paper/034563a114bd64e5a6e6fda0cce9a2ef3f1e2ca5", "title": "SMART FARM GUIDE: AN AUTONOMOUS ROBOT FOR OUTDOOR EDUCATIONAL TOURS USING AI VISION AND SPEECH", "venue": "NLP and Machine Learning Trends 2025", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5121/csit.2025.151612?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5121/csit.2025.151612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-08-23", "authors": [{"authorId": "2379729614", "name": "Eric Miller"}, {"authorId": "2378440701", "name": "Jonathan Sahagun"}], "abstract": "This project addresses the challenge of providing consistent, educational farm tours with limited staff. We designed and built an autonomous robot that uses GPS to follow a farm path, a camera to detect nearby plants, and ChatGPT’s vision and language models to describe the plants in real time [1]. The robot plays this information out loud using a textto-speech engine, offering visitors a guided tour experience with minimal human involvement. The system was developed using a Raspberry Pi, Adafruit GPS, and Python code to integrate hardware and APIbased services [2]. We tested the robot’s plant recognition accuracy and audio clarity under real-world conditions. Results showed over 80% detection accuracy and improved voice clarity with slower speech settings. Compared to existing solutions—like static museum guides or indoor service robots—this project offers a dynamic, outdoor-compatible alternative. It combines automation and education in a scalable, engaging way that could benefit farms, gardens, or environmental centers."}
{"paperId": "0403df04da80a11011b816236da96338bf0f2619", "url": "https://www.semanticscholar.org/paper/0403df04da80a11011b816236da96338bf0f2619", "title": "DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-30", "authors": [{"authorId": "2395673151", "name": "Toshiki Katsube"}, {"authorId": "2395675661", "name": "Taiga Fukuhara"}, {"authorId": "2395676296", "name": "Kenichiro Ando"}, {"authorId": "2374364", "name": "Yusuke Mukuta"}, {"authorId": "51215319", "name": "Kohei Uehara"}, {"authorId": "2075436077", "name": "Tatsuya Harada"}], "abstract": "This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling."}
{"paperId": "045ee7d2cb7067e354746c05b669de17fd74a57f", "url": "https://www.semanticscholar.org/paper/045ee7d2cb7067e354746c05b669de17fd74a57f", "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.17645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-21", "authors": [{"authorId": "2371026665", "name": "Shih-Wen Liu"}, {"authorId": "2371016938", "name": "Hsuan-Yu Fan"}, {"authorId": "2370936577", "name": "Wei-Ta Chu"}, {"authorId": "41015732", "name": "Fu-En Yang"}, {"authorId": "2332289950", "name": "Yu-Chiang Frank Wang"}], "abstract": "Automating medical report generation from histopathology images is a critical challenge requiring effective visual representations and domain-specific knowledge. Inspired by the common practices of human experts, we propose an in-context learning framework called PathGenIC that integrates context derived from the training set with a multimodal in-context learning (ICL) mechanism. Our method dynamically retrieves semantically similar whole slide image (WSI)-report pairs and incorporates adaptive feedback to enhance contextual relevance and generation quality. Evaluated on the HistGen benchmark, the framework achieves state-of-the-art results, with significant improvements across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across diverse report lengths and disease categories. By maximizing training data utility and bridging vision and language with ICL, our work offers a solution for AI-driven histopathology reporting, setting a strong foundation for future advancements in multimodal clinical applications."}
{"paperId": "04f94d4f05d9403a42111ac6a77cf07e86605045", "url": "https://www.semanticscholar.org/paper/04f94d4f05d9403a42111ac6a77cf07e86605045", "title": "Adversarial Machine Learning: Analyzing Carlini & Wagner Attacks", "venue": "International Conference on Innovative Mechanisms for Industry Applications", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIMIA67127.2025.11200948?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIMIA67127.2025.11200948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-09-03", "authors": [{"authorId": "2367813199", "name": "S. Panda"}], "abstract": "Adversarial Machine Learning (AML) is a fascinating and fast-growing research direction and area of practical interest. Deployed Machine Learning (ML) models are known to be vulnerable to adversarial inputs crafted by adversaries to provide false inputs to confuse the ML models. Various adversarial attacks that generate adversarial inputs have been proposed. Adversarial examples created by ML models, by all known attacks, should reach a threshold level of efficacy so that the downstream tasks fail with these examples. Therefore, adversarial attacks are a target of interest for a widely deployed downstream learning task in computer vision, natural language processing, etc. Among the wide spectrum of adversarial attacks, the Carlini and Wagner (C&W) attack stands out due to its optimization-based structure and ability to craft perturbations with minimal distortion that are highly effective at fooling deep neural networks. Examine defensive mechanisms such as adversarial training, input processing, and model hardening, and evaluate their effectiveness against the C&W attack. The work contributes to the broader understanding of adversarial robustness in AI by offering detailed exploration of the Carlini and Wagner attack and its implications for secure ML model deployment in real-world applications."}
{"paperId": "0505f7b7532d968c7fafe1eaf0ecfcbeae39c841", "url": "https://www.semanticscholar.org/paper/0505f7b7532d968c7fafe1eaf0ecfcbeae39c841", "title": "AI-Assisted Assessment Platforms with Real-Time Monitoring: A Comprehensive Survey", "venue": "International Journal of Innovative Research in Advanced Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.26562/ijirae.2025.v1211.08?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26562/ijirae.2025.v1211.08, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-11-21", "authors": [{"authorId": "2340138139", "name": "Pallavi Kn"}], "abstract": "The need for smart, safe, and trust worthy proctoring systems has increased due to the widespread shift toward remote learning, online hiring, and extensive digital assessments. Despite their convenience, traditional online exam systems frequently lack strong monitoring and verification features, raising questions about academic integrity, fairness, and authenticity. In order to understand how recent developments in computer vision, natural language processing (NLP), large language models (LLMs), and multimodal learning have changed digital assessments, this paper provides an extensive overview of artificial intelligence (AI)- assisted online proctoring systems. The study examines current scholarly and commercial methods for automated content creation, scalable cloud-based architectures, and real time behavioral monitoring. It also looks at how proctoring techniques are being impacted by generative AI technologies, tackling issues like transparency, privacy protection, cheating detection, and bias reduction. The article ends by pointing out unresolved research issues and suggesting future paths for creating explainable, flexible, and morally sound proctoring systems that guarantee dependability and user confidence."}
{"paperId": "056a582692b3e9da3ab613b356db732572a0552a", "url": "https://www.semanticscholar.org/paper/056a582692b3e9da3ab613b356db732572a0552a", "title": "Generate, Transduct, Adapt: Iterative Transduction with VLMs", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.06031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-10", "authors": [{"authorId": "49194775", "name": "Oindrila Saha"}, {"authorId": "2339668453", "name": "Logan Lawrence"}, {"authorId": "2273354232", "name": "Grant Van Horn"}, {"authorId": "2256725214", "name": "Subhransu Maji"}], "abstract": "Transductive zero-shot learning with vision-language models leverages image-image similarities within the dataset to achieve better classification accuracy compared to the inductive setting. However, there is little work that explores the structure of the language space in this context. We propose GTA-CLIP, a novel technique that incorporates supervision from language models for joint transduction in language and vision spaces. Our approach is iterative and consists of three steps: (i) incrementally exploring the attribute space by querying language models, (ii) an attribute-augmented transductive inference procedure, and (iii) fine-tuning the language and vision encoders based on inferred labels within the dataset. Through experiments with CLIP encoders, we demonstrate that GTA-CLIP, yields an average performance improvement of 8.6% and 3.7% across 12 datasets and 3 encoders, over CLIP and transductive CLIP respectively in the zero-shot setting. We also observe similar improvements in a few-shot setting. We present ablation studies that demonstrate the value of each step and visualize how the vision and language spaces evolve over iterations driven by the transductive learning. Code is released at https://github.com/cvl-umass/GTA-CLIP"}
{"paperId": "05bc7ebd71eacacf36c8fcd8f03e558269cf8f2f", "url": "https://www.semanticscholar.org/paper/05bc7ebd71eacacf36c8fcd8f03e558269cf8f2f", "title": "Efficient Alignment of Unconditioned Action Prior for Language-Conditioned Pick and Place in Clutter", "venue": "IEEE Transactions on Automation Science and Engineering", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-12", "authors": [{"authorId": "2052306731", "name": "Kechun Xu"}, {"authorId": "2303250000", "name": "Xunlong Xia"}, {"authorId": "2349815804", "name": "Kaixuan Wang"}, {"authorId": "2160860889", "name": "Yifei Yang"}, {"authorId": "2267065418", "name": "Yunxuan Mao"}, {"authorId": "2312967397", "name": "Bing Deng"}, {"authorId": "2298046200", "name": "Jieping Ye"}, {"authorId": "2068690302", "name": "Rong Xiong"}, {"authorId": "2248994216", "name": "Yue Wang"}], "abstract": "We study the task of language-conditioned pick and place in clutter, where a robot should grasp a target object in open clutter and move it to a specified place. Some approaches learn end-to-end policies with features from vision foundation models, requiring large datasets. Others combine foundation models in a zero-shot setting, suffering from cascading errors. In addition, they primarily leverage vision and language foundation models, focusing less on action priors. In this paper, we aim to develop an effective policy by integrating foundation priors from vision, language, and action. We propose A2, an action prior alignment method that aligns unconditioned action priors with 3D vision-language priors by learning one attention layer. The alignment formulation enables our policy to train with less data and preserve zero-shot generalization capabilities. We show that a shared policy for both pick and place actions enhances the performance for each task, and introduce a policy adaptation scheme to accommodate the multi-modal nature of actions. Extensive experiments in simulation and the real-world show that our policy achieves higher task success rates with fewer steps for both pick and place tasks in clutter, effectively generalizing to unseen objects and language instructions. Videos and codes are available at https://xukechun.github.io/papers/A2 Note to Practitioners—This research is motivated by the challenge of generalizable policy learning of language-conditioned pick and place in clutter. Solving such a challenge could significantly improve the robot’s level of automation and intelligence in household and industrial pick and place tasks. Existing methods struggle with large data requirements, poor generalization to unseen scenarios, and cascading errors across individual components. To overcome these limitations, we propose to integrate priors from vision, language, and action foundation models by learning-based alignment. Our policy aligns action priors with 3D vision-language priors by learning one attention layer, requiring less data and preserving zero-shot generalization capabilities from foundation models. Experiments show that our method can improve both task success rate and generalization for pick and place tasks in simulation and the real world. In future work, we will incorporate more action foundation models to extend our approach of action prior alignment to a wider range of tasks, offering a promising direction for general manipulation."}
{"paperId": "05ee36c75e2444ca274886185d3681dbb2277483", "url": "https://www.semanticscholar.org/paper/05ee36c75e2444ca274886185d3681dbb2277483", "title": "Generalist YOLO: Towards Real-Time End-to-End Multi-Task Visual Language Models", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV61041.2025.00606?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV61041.2025.00606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2140690746", "name": "Hung-Shuo Chang"}, {"authorId": "2250543210", "name": "Chien-Yao Wang"}, {"authorId": "2249712044", "name": "Richard Robert Wang"}, {"authorId": "2249533680", "name": "Gene Chou"}, {"authorId": "2043281157", "name": "Hongpeng Liao"}], "abstract": "Generalist models, capable of handling multiple modalities and tasks simultaneously, are currently one of the hottest research topics. However, due to interference between different tasks during the training process, existing generalist models require a very large decoder to achieve good results in various tasks, which makes real-time prediction difficult for current generalist models. This paper introduces Generalist YOLO, which takes a significant step towards real-time prediction systems for visual language generalist models. The proposed Generalist YOLO uses a unified encoder to reduce conflicts between different tasks, thereby decreasing the complexity required by the decoder. It also introduces a primary-secondary co-attention mechanism that allows different tasks to learn together more effectively, achieving high efficiency and high accuracy. We propose a semantically consistent asymmetric training strategy, allowing various tasks to benefit from performance improvements brought by the latest research results in various fields. The proposed Generalist YOLO achieves excellent results on various vision and language tasks based on MS COCO. While maintaining high accuracy across all tasks, it is 135 times faster than existing generalist models. The source code is released on GitHub at https://github.com/WongKinYiu/GeneralistYOLO."}
{"paperId": "05f7cbffd749e496304f9de1a097d658679f78a7", "url": "https://www.semanticscholar.org/paper/05f7cbffd749e496304f9de1a097d658679f78a7", "title": "Mathematical Methods in the Context of Artificial Intelligence: Foundations, Approaches, and Applications", "venue": "International Journal For Multidisciplinary Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36948/ijfmr.2025.v07i06.61601?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36948/ijfmr.2025.v07i06.61601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-24", "authors": [{"authorId": "2394763298", "name": "Jayprakash Yadav"}], "abstract": "Artificial Intelligence (AI) has emerged as a transformative field that depends fundamentally on mathematical principles. Mathematical methods provide the structure, logic, and computational framework required for intelligent systems to learn, reason, and make decisions. This research paper examines the essential mathematical foundations of AI, including linear algebra, calculus, probability theory, optimization, discrete mathematics, numerical methods, and information theory. The paper highlights how these mathematical tools contribute to machine learning, neural networks, computer vision, natural language processing, and other AI subfields. It also discusses current challenges, future directions, and the evolving role of mathematical modeling in next-generation AI systems."}
{"paperId": "061cd954e76d870070fddafa338f998bfeb072a0", "url": "https://www.semanticscholar.org/paper/061cd954e76d870070fddafa338f998bfeb072a0", "title": "Mamba Adapter: Efficient Multi-Modal Fusion for Vision-Language Tracking", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3557570?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3557570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2278619368", "name": "Liangtao Shi"}, {"authorId": "2273869508", "name": "Bineng Zhong"}, {"authorId": "2055747023", "name": "Qihua Liang"}, {"authorId": "2220701228", "name": "Xiantao Hu"}, {"authorId": "2277249541", "name": "Zhiyi Mo"}, {"authorId": "2292256220", "name": "Shuxiang Song"}], "abstract": "Utilizing the high-level semantic information of language to compensate for the limitations of vision information is a highly regarded approach in single-object tracking. However, most existing vision-language (VL) trackers employ full-parameter fine-tuning, which can easily lead to catastrophic forgetting. Therefore, they fail to fully exploit the prior knowledge of pre-trained models from upstream tasks, resulting in unsatisfactory tracking performance. To alleviate the above problem, we propose a simple yet effective Vision- Language Tracking pipeline based on Mamba Adapter, named MAVLT, which adopts the idea of parameter-efficient fine-tuning (PEFT) to realize the interaction between vision-language modalities. This novel approach offers the following advantages: 1) The knowledge of the upstream pre-trained model is efficiently inherited by freezing its parameters. This ensures that the VL tracking framework only learns the modules for vision and language interaction, with a focus on the fusion between modalities. 2) The modal interaction between language and vision encoders is flexibly bridged in each encoder layer via proposed mamba adapter, enabling efficient interaction of visual and language information at multiple levels. Extensive experiments on five popular vision-language tracking benchmarks validate the effectiveness of the proposed MAVLT. Particularly, the MAVLT achieves 73.4% AUC score on the LaSOT benchmarks with only 0.18%(0.32M) of the total parameters updates. Code and models are available at https://github.com/GXNU-ZhongLab/MAVLT."}
{"paperId": "06223b0b8ab970767ff67d5fa930c44a24e856d4", "url": "https://www.semanticscholar.org/paper/06223b0b8ab970767ff67d5fa930c44a24e856d4", "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.19433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-24", "authors": [{"authorId": "2371212575", "name": "Lixuan He"}, {"authorId": "2371298951", "name": "Haoyu Dong"}, {"authorId": "2371158424", "name": "Zhenxing Chen"}, {"authorId": "2371004131", "name": "Yangcheng Yu"}, {"authorId": "2307542644", "name": "Jie Feng"}, {"authorId": "2307646274", "name": "Yong Li"}], "abstract": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and>10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav."}
{"paperId": "063421134bb2c61a4b8380f8caa3e42ffe33168f", "url": "https://www.semanticscholar.org/paper/063421134bb2c61a4b8380f8caa3e42ffe33168f", "title": "Augmented memory: Wearable AI assistant supporting daily activities for dementia patients", "venue": "World Journal of Advanced Research and Reviews", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30574/wjarr.2025.26.2.1628?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30574/wjarr.2025.26.2.1628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-30", "authors": [{"authorId": "2391648012", "name": "Manas Sharma"}], "abstract": "This article examines the development of a wearable multimodal AI system designed to support individuals living with dementia through the integration of computer vision, natural language processing, and context-aware large language models. The system aims to address critical gaps in existing assistive technologies by providing real-time cognitive support that enhances independence and quality of life. Through wearable form factors such as smart glasses or camera-equipped pendants, the technology captures and processes environmental cues and social interactions to deliver contextually appropriate reminders and assistance. The article presents a comprehensive framework covering theoretical underpinnings, technical architecture, implementation challenges, and potential healthcare integration pathways. Drawing on extensive research into dementia pathology and current assistive technologies, the proposed system balances edge and cloud computing to optimize performance while maintaining privacy and security. User experience considerations examine form factor preferences across different demographic groups and interface design principles tailored to cognitive limitations. The article highlights significant challenges in computational constraints, connectivity requirements, and social acceptability while offering promising directions for future development, clinical validation, and healthcare system integration."}
{"paperId": "06b0399f2241dc680c0af9dcba6f3274fa14f6fd", "url": "https://www.semanticscholar.org/paper/06b0399f2241dc680c0af9dcba6f3274fa14f6fd", "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "venue": "arXiv.org", "year": 2025, "citationCount": 12, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12972, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-17", "authors": [{"authorId": "2350519685", "name": "Junming Liu"}, {"authorId": "2305204709", "name": "Siyuan Meng"}, {"authorId": "2350497175", "name": "Yanting Gao"}, {"authorId": "2205652631", "name": "Song Mao"}, {"authorId": "26978261", "name": "Pinlong Cai"}, {"authorId": "2352010232", "name": "Guohang Yan"}, {"authorId": "2350503701", "name": "Yirong Chen"}, {"authorId": "152385441", "name": "Zilin Bian"}, {"authorId": "2288590428", "name": "Botian Shi"}, {"authorId": "2350682014", "name": "Ding Wang"}], "abstract": "Multimodal reasoning in Large Language Models (LLMs) struggles with incomplete knowledge and hallucination artifacts, challenges that textual Knowledge Graphs (KGs) only partially mitigate due to their modality isolation. While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal understanding, their practical construction is impeded by semantic narrowness of manual text annotations and inherent noise in visual-semantic entity linkages. In this paper, we propose Vision-align-to-Language integrated Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances LLMs reasoning through cross-modal information supplementation. Specifically, we cascade pre-trained Vision-Language Models (VLMs) to align image features with text, transforming them into descriptions that encapsulate image-specific information. Furthermore, we developed a cross-modal similarity verification mechanism to quantify semantic consistency, effectively filtering out noise introduced during feature alignment. Even without manually annotated image captions, the refined descriptions alone suffice to construct the MMKG. Compared to conventional MMKGs construction paradigms, our approach achieves substantial storage efficiency gains while maintaining direct entity-to-image linkage capability. Experimental results on multimodal reasoning tasks demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."}
{"paperId": "06cfb75deab1324f72bf572fde5ddd2c65519581", "url": "https://www.semanticscholar.org/paper/06cfb75deab1324f72bf572fde5ddd2c65519581", "title": "Multi Algorithm Based Obesity Analysis System Techniques", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem52002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem52002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-23", "authors": [{"authorId": null, "name": "Roopa R"}, {"authorId": "2376886688", "name": "Musthab Shira"}], "abstract": "Obesity is a rapidly growing global health challenge, contributing significantly to the burden of non-communicable diseases such as diabetes, cardiovascular disorders, and joint problems. Traditional obesity assessment methods, predominantly based on Body Mass Index (BMI) and other basic anthropometric measurements, often fail to provide a holistic understanding of the condition due to their inability to incorporate multiple contributing factors. This project presents a Multi-Algorithm-Based Obesity Analysis System that leverages machine learning and statistical techniques to enhance the accuracy and depth of obesity prediction and risk assessment. By integrating data from various domains—including physical activity, dietary habits, genetic predispositions, psychological well-being, and clinical parameters—the system employs a range of algorithms such as Decision Trees, Support Vector Machines, and Neural Networks to uncover complex patterns and interactions within the data. The architecture fuses both phenotyping and genotyping inputs to offer a personalized, real-time risk analysis and intervention strategy. This intelligent system empowers healthcare professionals, researchers, and individuals by providing actionable insights, thus facilitating early prevention and effective obesity management [4].\n\nKeywords: Sign language, information retrieval, computer vision, natural language processing, accessibility, deaf individuals"}
{"paperId": "0846261231ab9b3b6cdabd7c61ed40808cd54077", "url": "https://www.semanticscholar.org/paper/0846261231ab9b3b6cdabd7c61ed40808cd54077", "title": "Toward Intelligent Ad Breaks: A Survey and Taxonomy of AI-Driven Ad Placement in Streaming Media", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3610662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3610662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2380990182", "name": "Waruna De Silva"}, {"authorId": "2284379138", "name": "Anil Fernando"}], "abstract": "Streaming media growth has increased demand for intelligent, non-intrusive placement of advertisements that balance monetization targets with viewer experience. Traditional rule-based heuristics such as scene change or silence discovery fail to capture contemporary video consumption diversity, complexity, and cognitive variability. Here, we present a comprehensive summary of artificial intelligence (AI) methods for optimizing ad break placement within streaming systems. We introduce a new three-phase taxonomy Data, Decision, and Delivery organizing state-of-the-art techniques within computer vision, natural language processing, affective computing, and reinforcement learning. AdBreakScore(t), a cognitively and emotionally guided model of appropriateness evaluation for advertisements, underpins this work with additions to model ethical and computational constraints. We analyze multimodal scene interpretation, viewer engagement prediction, and adaptable scheduling to illustrate prospects and trade-offs along technical, cognitive, and regulatory axes. We conclude with future directions neurophysiological engagement modeling, generative storyline alignment, and edge-compliant delivery aiming to advance development of viewer-centered, adaptive, and ethically principled systems for placing advertisements within an ever-changing digital media landscape."}
{"paperId": "08fffc296d461a48245afbe69f0013648d97bc24", "url": "https://www.semanticscholar.org/paper/08fffc296d461a48245afbe69f0013648d97bc24", "title": "AIGeN-Llama: An Adversarial Approach for Instruction Generation in VLN using Llama2 Model", "venue": "Italian Research Conference on Digital Library Management Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2238669510", "name": "Niyati Rawal"}, {"authorId": "1843795", "name": "L. Baraldi"}, {"authorId": "2303850502", "name": "Rita Cucchiara"}], "abstract": null}
{"paperId": "09263534f83bda7ce75dd37b101ea40c530bfad8", "url": "https://www.semanticscholar.org/paper/09263534f83bda7ce75dd37b101ea40c530bfad8", "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.07739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-06-09", "authors": [{"authorId": "2367550951", "name": "Jing Zhong"}, {"authorId": "2291142283", "name": "Jun Yin"}, {"authorId": "2302402308", "name": "Peilin Li"}, {"authorId": "2291560239", "name": "Pengyu Zeng"}, {"authorId": "2366144804", "name": "Miao Zang"}, {"authorId": "2366069714", "name": "Ran Luo"}, {"authorId": "2290813528", "name": "Shuai Lu"}], "abstract": "Architectural cultures across regions are characterized by stylistic diversity, shaped by historical, social, and technological contexts in addition to geograph-ical conditions. Understanding architectural styles requires the ability to describe and analyze the stylistic features of different architects from various regions through visual observations of architectural imagery. However, traditional studies of architectural culture have largely relied on subjective expert interpretations and historical literature reviews, often suffering from regional biases and limited ex-planatory scope. To address these challenges, this study proposes three core contributions: (1) We construct a professional architectural style dataset named ArchDiffBench, which comprises 1,765 high-quality architectural images and their corresponding style annotations, collected from different regions and historical periods. (2) We propose ArchiLense, an analytical framework grounded in Vision-Language Models and constructed using the ArchDiffBench dataset. By integrating ad-vanced computer vision techniques, deep learning, and machine learning algo-rithms, ArchiLense enables automatic recognition, comparison, and precise classi-fication of architectural imagery, producing descriptive language outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show that ArchiLense achieves strong performance in architectural style recognition, with a 92.4% con-sistency rate with expert annotations and 84.5% classification accuracy, effec-tively capturing stylistic distinctions across images. The proposed approach transcends the subjectivity inherent in traditional analyses and offers a more objective and accurate perspective for comparative studies of architectural culture."}
{"paperId": "094541f5f93ca0de4ef01099fa2f94c464452b19", "url": "https://www.semanticscholar.org/paper/094541f5f93ca0de4ef01099fa2f94c464452b19", "title": "Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.00797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-01", "authors": [{"authorId": "48757581", "name": "Ruyu Liu"}, {"authorId": "2383165044", "name": "Dongxu Zhuang"}, {"authorId": "2216538936", "name": "Jianhua Zhang"}, {"authorId": "120984895", "name": "Arega Getaneh Abate"}, {"authorId": "2242436521", "name": "Per Sieverts Nielsen"}, {"authorId": "2384800918", "name": "Ben Wang"}, {"authorId": "2373548292", "name": "Xiufeng Liu"}], "abstract": "Building facades represent a significant untapped resource for solar energy generation in dense urban environments, yet assessing their photovoltaic (PV) potential remains challenging due to complex geometries and semantic com ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an automated framework that transforms street-view photographs into quantitative PV deployment assessments. The approach combines com puter vision and artificial intelligence techniques to address three key challenges: perspective distortion correction, semantic understanding of facade elements, and spatial reasoning for PV layout optimization. Our four-stage pipeline processes images through geometric rectification, zero-shot semantic segmentation, Large Language Model (LLM) guided spatial reasoning, and energy simulation. Validation across 80 buildings in four countries demonstrates ro bust performance with mean area estimation errors of 6.2%± 2.8% compared to expert annotations. The auto mated assessment requires approximately 100 seconds per building, a substantial gain in efficiency over manual methods. Simulated energy yield predictions confirm the method's reliability and applicability for regional poten tial studies, urban energy planning, and building-integrated photovoltaic (BIPV) deployment. Code is available at: https:github.com/CodeAXu/Solar-PV-Installation"}
{"paperId": "0947742702add0a14e5ba83d0f04d1d9e4edb5f7", "url": "https://www.semanticscholar.org/paper/0947742702add0a14e5ba83d0f04d1d9e4edb5f7", "title": "Multimodal Disease Detection MedBot", "venue": "International Conference on Computing Communication Control and automation", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCUBEA65967.2025.11284195?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCUBEA65967.2025.11284195, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-22", "authors": [{"authorId": null, "name": "Aditya Dhir"}, {"authorId": null, "name": "Saksham Jain"}, {"authorId": null, "name": "Tanmay Sankuratri"}, {"authorId": null, "name": "Sumitra Motade"}], "abstract": null}
{"paperId": "0aad518f8d1cef5dd6a96eac9c3f951c44b5a62d", "url": "https://www.semanticscholar.org/paper/0aad518f8d1cef5dd6a96eac9c3f951c44b5a62d", "title": "Enhancing Text-based Image Captioning through Text-aware Self-critical Training", "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3783990?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3783990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-11", "authors": [{"authorId": "2152452738", "name": "Jing Wang"}, {"authorId": "2262429701", "name": "Lingwu Meng"}, {"authorId": "2192672187", "name": "Jinhui Tang"}, {"authorId": null, "name": "Xiangyang Ji"}], "abstract": "Text-based image captioning (TextCaptioning) aims to depict an image by comprehending the scene texts and other visual cues it encompasses. To endow captioning models with text-reading capabilities, a common strategy is to leverage external OCR systems. However, OCR tokens produced by these systems often exhibit inaccuracies in real-world scenarios, posing significant challenges for TextCaptioning. These inaccuracies can introduce disparities between scene texts in images and words in sentences, thereby undermining the carefully established semantic consistency between vision and language in captioning models. To tackle this challenge, we introduce the Text-aware Self-Critical Training (TSCT) method, which leverages a text-aware reward along with the REINFORCE algorithm to optimize TextCaptioning models. By providing appropriate rewards for individual words, TSCT enables more refined model optimization and helps mitigate the negative impacts of unreliable OCR tokens. Additionally, we enhance the language model by integrating token type information of input words, which is obtained by collecting a sentence dataset based upon the original TextCaps dataset. This additional information helps alleviate the ambiguity in matching sentence words with the standard vocabulary or scene texts caused by unreliable OCR tokens, thereby improving the quality of learned word embeddings and OCR token embeddings. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches, leading to a significant enhancement of CIDEr-D from 117.1% to 132.6%."}
{"paperId": "0ae4ce4004321dcc110e678f41e8685c914691c8", "url": "https://www.semanticscholar.org/paper/0ae4ce4004321dcc110e678f41e8685c914691c8", "title": "SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-13", "authors": [{"authorId": "2264605902", "name": "Xiangyu Shi"}, {"authorId": "2323374275", "name": "Zerui Li"}, {"authorId": "2323371268", "name": "Wenqi Lyu"}, {"authorId": "2350316951", "name": "Jiatong Xia"}, {"authorId": "1757942", "name": "Feras Dayoub"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2292302699", "name": "Qi Wu"}], "abstract": "Vision-and-Language Navigation (VLN) in continuous environments requires agents to interpret natural language instructions while navigating unconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage approach: a waypoint predictor to generate waypoints and a navigator to execute movements. However, current waypoint predictors struggle with spatial awareness, while navigators lack historical reasoning and backtracking capabilities, limiting adaptability. We propose a zero-shot VLN-CE framework integrating an enhanced waypoint predictor with a Multi-modal Large Language Model (MLLM)-based navigator. Our predictor employs a stronger vision encoder, masked cross-attention fusion, and an occupancy-aware loss for better waypoint quality. The navigator incorporates history-aware reasoning and adaptive path planning with backtracking, improving robustness. Experiments on R2R-CE and MP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in zero-shot settings, demonstrating competitive results compared to fully supervised methods. Real-world validation on Turtlebot 4 further highlights its adaptability."}
{"paperId": "0b2a2e9fe58d4616fae9b025e13c3466fffbafa9", "url": "https://www.semanticscholar.org/paper/0b2a2e9fe58d4616fae9b025e13c3466fffbafa9", "title": "OFMU: Optimization-Driven Framework for Machine Unlearning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-26", "authors": [{"authorId": "2382763577", "name": "Sadia Asif"}, {"authorId": "2343636956", "name": "Mohammad Mohammadi Amiri"}], "abstract": "Large language models deployed in sensitive applications increasingly require the ability to unlearn specific knowledge, such as user requests, copyrighted materials, or outdated information, without retraining from scratch to ensure regulatory compliance, user privacy, and safety. This task, known as machine unlearning, aims to remove the influence of targeted data (forgetting) while maintaining performance on the remaining data (retention). A common approach is to formulate this as a multi-objective problem and reduce it to a single-objective problem via scalarization, where forgetting and retention losses are combined using a weighted sum. However, this often results in unstable training dynamics and degraded model utility due to conflicting gradient directions. To address these challenges, we propose OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting while preserving retention through a hierarchical structure. Our method enforces forgetting via an inner maximization step that incorporates a similarity-aware penalty to decorrelate the gradients of the forget and retention objectives, and restores utility through an outer minimization step. To ensure scalability, we develop a two-loop algorithm with provable convergence guarantees under both convex and non-convex regimes. We further provide a rigorous theoretical analysis of convergence rates and show that our approach achieves better trade-offs between forgetting efficacy and model utility compared to prior methods. Extensive experiments across vision and language benchmarks demonstrate that OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility."}
{"paperId": "0b5a5c91d4482a99031483ddcc08a3e85e2d32e1", "url": "https://www.semanticscholar.org/paper/0b5a5c91d4482a99031483ddcc08a3e85e2d32e1", "title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations", "venue": "arXiv.org", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-20", "authors": [{"authorId": "2308072132", "name": "Zeping Liu"}, {"authorId": "2351883269", "name": "Fan Zhang"}, {"authorId": "2351669607", "name": "Junfeng Jiao"}, {"authorId": "2135307417", "name": "Ni Lao"}, {"authorId": "2240534117", "name": "Gengchen Mai"}], "abstract": "Advancements in vision and language foundation models have inspired the development of geo-foundation models (GeoFMs), enhancing performance across diverse geospatial tasks. However, many existing GeoFMs primarily focus on overhead remote sensing (RS) data while neglecting other data modalities such as ground-level imagery. A key challenge in multimodal GeoFM development is to explicitly model geospatial relationships across modalities, which enables generalizability across tasks, spatial scales, and temporal contexts. To address these limitations, we propose GAIR, a novel multimodal GeoFM architecture integrating overhead RS data, street view (SV) imagery, and their geolocation metadata. We utilize three factorized neural encoders to project an SV image, its geolocation, and an RS image into the embedding space. The SV image needs to be located within the RS image's spatial footprint but does not need to be at its geographic center. In order to geographically align the SV image and RS image, we propose a novel implicit neural representations (INR) module that learns a continuous RS image representation and looks up the RS embedding at the SV image's geolocation. Next, these geographically aligned SV embedding, RS embedding, and location embedding are trained with contrastive learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial tasks spanning RS image-based, SV image-based, and location embedding-based benchmarks. Experimental results demonstrate that GAIR outperforms state-of-the-art GeoFMs and other strong baselines, highlighting its effectiveness in learning generalizable and transferable geospatial representations."}
{"paperId": "0ba66792733af44ecd93af4f37cde8f258f7a5dd", "url": "https://www.semanticscholar.org/paper/0ba66792733af44ecd93af4f37cde8f258f7a5dd", "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-02", "authors": [{"authorId": "2264605902", "name": "Xiangyu Shi"}, {"authorId": "2323374275", "name": "Zerui Li"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2292302699", "name": "Qi Wu"}], "abstract": "Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation."}
{"paperId": "0bc9d30796352b6633401a1af36d27772b1cdfe2", "url": "https://www.semanticscholar.org/paper/0bc9d30796352b6633401a1af36d27772b1cdfe2", "title": "Yo’Chameleon: Personalized Vision and Language Generation", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-29", "authors": [{"authorId": "2279870002", "name": "Thao Nguyen"}, {"authorId": "2265586470", "name": "Krishna Kumar Singh"}, {"authorId": "2358106110", "name": "Jing Shi"}, {"authorId": "2265648617", "name": "Trung Bui"}, {"authorId": "2272231489", "name": "Yong Jae Lee"}, {"authorId": "2351822426", "name": "Yuheng Li"}], "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo’Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo’Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo’Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a \"soft-positive\" image generation approach to enhance image quality in a few-shot setting. Our qualitative and quantitative analyses reveal that Yo’Chameleon can learn concepts more efficiently using fewer tokens and effectively encode visual attributes, outperforming prompting baselines."}
{"paperId": "0bee580c3dcbfdbed94acaf9afef33f320ad9e74", "url": "https://www.semanticscholar.org/paper/0bee580c3dcbfdbed94acaf9afef33f320ad9e74", "title": "Between Monocular and Panoramic: Multi-View Vision-and-Language Navigation", "venue": "2025 International Conference on Image and Video Processing (ICIVP)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIVP66296.2025.00031?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIVP66296.2025.00031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-04-23", "authors": [{"authorId": "2389486112", "name": "Minghan Li"}], "abstract": "Vision-and-Language Navigation (VLN) aims to enable agents to navigate 3D environments based on natural language instructions. However, sim-to-real transfer remains a significant challenge, particularly for monocular robots. While panoramic VLN models exhibit strong performance, their deployment on monocular robots is hindered by hardware limitations, and existing monocular models lack the capability to meet practical demands. To address this, we propose an innovative transfer method that equips monocular robots with panoramic perception and semantic understanding, enabling seamless migration of high-performance panoramic models to monocular systems. Specifically, we leverage multi-view image inputs and introduce a multi-map adaptive fusion module to update navigable maps, predicting agent-centered navigable waypoints. Additionally, we propose a dynamic ray sampling NeRF to generate new view representations of these waypoints. These methods expand the limited field of view of monocular robots and significantly enhance navigation performance in real-world scenarios. Extensive experiments on the R2R-CE benchmark demonstrate that our approach significantly outperforms the current state-of-the-art (SOTA) monocular VLN methods."}
{"paperId": "0c409e36467d3828252fb958bd4497226d4ac968", "url": "https://www.semanticscholar.org/paper/0c409e36467d3828252fb958bd4497226d4ac968", "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling", "venue": "arXiv.org", "year": 2025, "citationCount": 24, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.05240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-07", "authors": [{"authorId": "2255922850", "name": "Meng Wei"}, {"authorId": "2372455959", "name": "Chenyang Wan"}, {"authorId": "2282087053", "name": "Xiqian Yu"}, {"authorId": "2257008641", "name": "Tai Wang"}, {"authorId": "2360919920", "name": "Yuqiang Yang"}, {"authorId": "2307735489", "name": "Xiaohan Mao"}, {"authorId": "2243446820", "name": "Chenming Zhu"}, {"authorId": "2362197963", "name": "Wenzhe Cai"}, {"authorId": "2311307527", "name": "Hanqing Wang"}, {"authorId": "2236662733", "name": "Yilun Chen"}, {"authorId": "2253851118", "name": "Xihui Liu"}, {"authorId": "2277447920", "name": "Jiangmiao Pang"}], "abstract": "Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \\href{https://streamvln.github.io/}{https://streamvln.github.io/}."}
{"paperId": "0c41bf71f4601d486fd2ffa0f126711d31077f9a", "url": "https://www.semanticscholar.org/paper/0c41bf71f4601d486fd2ffa0f126711d31077f9a", "title": "Cervical Cancer Prediction Using Machine Learning", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem52005?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem52005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-23", "authors": [{"authorId": "2376818808", "name": "V. Gl"}, {"authorId": null, "name": "Sana G"}], "abstract": "Cervical cancer is one of the deadliest diseases among women globally, particularly in low-income countries where early detection and treatment are limited. This project proposes a Machine Learning-based Prediction System to detect the risk of cervical cancer at an early stage. By analyzing various factors such as age, sexual health, hormonal usage, genetic history, and prior diagnoses, the system applies classification techniques to estimate the risk category. A Decision Tree classifier is employed to model patterns and identify frequent combinations of high-risk attributes. The system predicts the likelihood and stage of cervical cancer by assigning risk scores based on patient inputs and classifying them into risk levels: low, intermediate, high, and very high. This automated approach supports medical practitioners by improving diagnostic accuracy and enabling timely intervention. The project is built using Python, MySQL, and Anaconda, offering a scalable and interpretable diagnostic tool for early-stage cervical cancer prediction.\n\n\n\n\nKeywords: Sign language, information retrieval, computer vision, natural language processing, accessibility, deaf individuals."}
{"paperId": "0cd8fae5df1fe6c40a8033dc3feb5c3b7a69dbbb", "url": "https://www.semanticscholar.org/paper/0cd8fae5df1fe6c40a8033dc3feb5c3b7a69dbbb", "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-13", "authors": [{"authorId": "2331429520", "name": "Rui Hu"}, {"authorId": "2261901203", "name": "Lianghui Zhu"}, {"authorId": "2309086659", "name": "Yuxuan Zhang"}, {"authorId": "46806979", "name": "Tianheng Cheng"}, {"authorId": "2313352836", "name": "Lei Liu"}, {"authorId": "2309179209", "name": "Heng Liu"}, {"authorId": "7826997", "name": "Longjin Ran"}, {"authorId": "2309115012", "name": "Xiaoxin Chen"}, {"authorId": "2257432695", "name": "Wenyu Liu"}, {"authorId": "2320181046", "name": "Xinggang Wang"}], "abstract": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \\times$ faster than GLaMM."}
{"paperId": "0cf38e5c84fa9bba61bfc7848ad690d3644c5134", "url": "https://www.semanticscholar.org/paper/0cf38e5c84fa9bba61bfc7848ad690d3644c5134", "title": "Video Summarization Using Vision and Language Transformer Models", "venue": "International Journal of Research Publication and Reviews", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55248/gengpi.6.0125.0654?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55248/gengpi.6.0125.0654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2319463333", "name": "S. Naikwadi"}], "abstract": null}
{"paperId": "0d5aa9734c218f8e69aec923da17541036251b99", "url": "https://www.semanticscholar.org/paper/0d5aa9734c218f8e69aec923da17541036251b99", "title": "PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning", "venue": "Design Automation Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-24", "authors": [{"authorId": "2317116613", "name": "Yisu Wang"}, {"authorId": "2318257137", "name": "Ruilong Wu"}, {"authorId": "2363506343", "name": "Xinjiao Li"}, {"authorId": "2317117820", "name": "Dirk Kutscher"}], "abstract": "Large-scale deep neural networks (DNN) exhibit excellent performance for various tasks. As DNNs and datasets grow, distributed training becomes extremely time-consuming and demands larger clusters. A main bottleneck is the resulting gradient aggregation overhead. While gradient compression and sparse collective communication techniques are commonly employed to alleviate network load, many gradient compression schemes do not achieve acceleration of the training process while also preserving accuracy. This paper introduces PacTrain, a novel framework that accelerates distributed training by combining pruning with sparse gradient compression. Active pruning of the neural network makes the model weights and gradients sparse. By ensuring the global knowledge of the gradient sparsity among all distributed training workers, we can perform lightweight compression communication without harming accuracy. We show that the PacTrain compression scheme achieves a near-optimal compression strategy while remaining compatible with the allreduce primitive. Experimental evaluations show that PacTrain improves training throughput by 1.25 to $8.72 \\times$ compared to state-of-the-art compression-enabled systems for representative vision and language models training tasks under bandwidth-constrained conditions."}
{"paperId": "0d6054999760cc7f7bc95eac42efb995063358e0", "url": "https://www.semanticscholar.org/paper/0d6054999760cc7f7bc95eac42efb995063358e0", "title": "Integrating Vision-Language Models for Enhanced Robotic Grasping and Interaction Using RGB Image and Prompt", "venue": "International Journal of Mechanical Engineering and Robotics Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18178/ijmerr.14.5.500-510?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18178/ijmerr.14.5.500-510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2381351158", "name": "Nguyen Khac"}, {"authorId": "2762715", "name": "Nguyen Truong Thinh"}], "abstract": "—Object detection and grasping is one of the critical challenges in robotic research, particularly when working in complex environments with diverse objects in terms of shape and position. Although methods using RGB images have shown promising results in simpler scenarios, they still face numerous issues in more complex scenes, especially when objects overlap. Furthermore, prior research has primarily focused on object grasping, without focusing on addressing the interaction capabilities between robots and users during the grasping process. Recent advancements in vision-language models have opened up significant potential for the development of human-robot interaction systems based on multimodal data. This paper presents an integrated model combining computer vision and language models to enhance object detection and grasping capabilities in real-world environments. The proposed approach consists of three key steps (1) identifying the locations of objects and generating segmentation masks using a visual-language model; (2) grasp candidates are predicted from the generated masks and bounding boxes via the Grasp Detection Head; and (3) the candidates are optimized and refined using the Grasp Refinement Head. The integration of vision-language models in the proposed approach not only enhances the ability of robot to understand the semantics of language, enabling more accurate grasping decisions, but also strengthens the interaction capabilities of robot with users. Experimental results demonstrate that the proposed model achieves higher grasping accuracy compared to existing methods, particularly in complex scenes with multiple objects. Additionally, the model also shows its ability to understand complex contexts through Interactive Grasp experiments. "}
{"paperId": "0e4806061b5c8aeafaf71a7e5f632b70b63db6ed", "url": "https://www.semanticscholar.org/paper/0e4806061b5c8aeafaf71a7e5f632b70b63db6ed", "title": "Interaction Is Worth More Explanations: Improving Human–Object Interaction Representation With Propositional Knowledge", "venue": "IEEE Transactions on Cognitive and Developmental Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCDS.2024.3496566?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCDS.2024.3496566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2115412640", "name": "Feng Yang"}, {"authorId": "2296521796", "name": "Yichao Cao"}, {"authorId": "2267864193", "name": "Xuanpeng Li"}, {"authorId": "2154822943", "name": "Weigong Zhang"}], "abstract": "Detecting human–object interactions (HOI) presents a formidable challenge, necessitating the discernment of intricate, high-level relationships between humans and objects. Recent studies have explored HOI vision-and-language modeling (HOI-VLM), which leverages linguistic information inspired by cross-modal technology. Despite its promise, current methodologies face challenges due to the constraints of limited annotation vocabularies and suboptimal word embeddings, which hinder effective alignment with visual features and consequently, the efficient transfer of linguistic knowledge. In this work, we propose a novel cross-modal framework that leverages external propositional knowledge which harmonize annotation text with a broader spectrum of world knowledge, enabling a more explicit and unambiguous representation of complex semantic relationships. Additionally, considering the prevalence of multiple complexities due to the symbiotic or distinctive relationships inherent in one HO pair, along with the identical interactions occurring with diverse HO pairs (e.g., “human ride bicycle” versus “human ride horse”). The challenge lies in understanding the subtle differences and similarities between interactions involving different objects or occurring in varied contexts. To this end, we propose the Jaccard contrast strategy to simultaneously optimize cross-modal representation consistency across HO pairs (especially for cases where multiple interactions occur), which encompasses both vision-to-vision and vision-to-knowledge alignment objectives. The effectiveness of our proposed method is comprehensively validated through extensive experiments, showcasing its superiority in the field of HOI analysis."}
{"paperId": "0e9cb7d882fff0d6123de3dbf0178f5a15c028aa", "url": "https://www.semanticscholar.org/paper/0e9cb7d882fff0d6123de3dbf0178f5a15c028aa", "title": "HiMix: Reducing Computational Complexity in Large Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.10318, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-17", "authors": [{"authorId": "2321660605", "name": "Xuange Zhang"}, {"authorId": "2151286366", "name": "Dengjie Li"}, {"authorId": "2340780408", "name": "Bo Liu"}, {"authorId": "2337691742", "name": "Zenghao Bao"}, {"authorId": "2337775276", "name": "Yao Zhou"}, {"authorId": "2340906187", "name": "Baisong Yang"}, {"authorId": "2341025195", "name": "Zhongying Liu"}, {"authorId": "2332548623", "name": "Yujie Zhong"}, {"authorId": "2333622350", "name": "Zheng Zhao"}, {"authorId": "2340683940", "name": "Tongtong Yuan"}], "abstract": "Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models(LVLMs) have achieved prominent performance across a wide range of scenarios. However, the excessive computational complexity limits the widespread use of these models in practical applications. We argue that one main bottleneck in computational complexity is caused by the involvement of redundant vision sequences in model computation. This is inspired by a reassessment of the efficiency of vision and language information transmission in the language decoder of LVLMs. Then, we propose a novel hierarchical vision-language interaction mechanism called Hierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the language sequence undergoes full forward propagation, while the vision sequence interacts with the language at specific stages within each language decoder layer. It is striking that our approach significantly reduces computational complexity with minimal performance loss. Specifically, HiMix achieves a 10x reduction in the computational cost of the language decoder across multiple LVLM models while maintaining comparable performance. This highlights the advantages of our method, and we hope our research brings new perspectives to the field of vision-language understanding. Project Page: https://xuange923.github.io/HiMix"}
{"paperId": "0eb4d8dbf3adfbe9b04a53f92e984118fddcabd9", "url": "https://www.semanticscholar.org/paper/0eb4d8dbf3adfbe9b04a53f92e984118fddcabd9", "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2387858442", "name": "Hongyu Ding"}, {"authorId": "2387174677", "name": "Ziming Xu"}, {"authorId": "2387025654", "name": "Yudong Fang"}, {"authorId": "2386988750", "name": "You Wu"}, {"authorId": "2341259073", "name": "Zixuan Chen"}, {"authorId": "2323526727", "name": "Jieqi Shi"}, {"authorId": "2055851838", "name": "Jing Huo"}, {"authorId": "2317875372", "name": "Yifan Zhang"}, {"authorId": "2386218319", "name": "Yang Gao"}], "abstract": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment."}
{"paperId": "0f30f667035352220102670c5b52f5133dfc579f", "url": "https://www.semanticscholar.org/paper/0f30f667035352220102670c5b52f5133dfc579f", "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.05622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-08", "authors": [{"authorId": "2325910749", "name": "Weichen Zhang"}, {"authorId": "2327008211", "name": "Chen Gao"}, {"authorId": "2360267526", "name": "Shiquan Yu"}, {"authorId": "2346979508", "name": "Ruiying Peng"}, {"authorId": "2215234748", "name": "Baining Zhao"}, {"authorId": "2360233511", "name": "Qian Zhang"}, {"authorId": "2327970730", "name": "Jinqiang Cui"}, {"authorId": "2300421419", "name": "Xinlei Chen"}, {"authorId": "2356253660", "name": "Yong Li"}], "abstract": "Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \\href{https://github.com/VinceOuti/CityNavAgent}{link}."}
{"paperId": "0f45de92cf245428246f1e0bd5a691afe42406f8", "url": "https://www.semanticscholar.org/paper/0f45de92cf245428246f1e0bd5a691afe42406f8", "title": "AI-Driven Pedagogical Approaches in Science Classrooms: Enhancing Digital Literacy and Learning Outcomes", "venue": "2025 First International Conference on Advances in Computer Science, Electrical, Electronics, and Communication Technologies (CE2CT)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CE2CT64011.2025.10939398?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CE2CT64011.2025.10939398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-02-21", "authors": [{"authorId": "2353258548", "name": "K. Agrawal"}, {"authorId": "2353258533", "name": "Asma B. Shahin"}, {"authorId": "2353315044", "name": "Sushil Kumar"}, {"authorId": "2324458774", "name": "Shilpa"}, {"authorId": "2309507998", "name": "S. Thamizharasan"}, {"authorId": "145112298", "name": "T. Babu"}], "abstract": "A high quality education in science classroom is crucial to foster equitable societal progress. However significant disparities for undeserved and marginalised community. Solutions using artificial intelligence bridges the cap through enhanced teaching methodologies to promote fair access to learning in science classroom. The present study explores the AI powered pedagogical approaches in science education. It examines the AI technologies including computer vision, natural language processing and machine learning techniques utilised for personalised learning delivering feedback at real time and boost a student engagement across various background. AI driven approaches can Taylor instruction to individual paces and learning style that ensure personalised support for every student. Platforms also offer additional assistance to learners facing challenges fostering equity and inclusivity in science education. Moreover, AI driven assessment tools provide teachers with valuable insights in student performance helping them to pinpoint the areas that requires improvement and targeted interventions. It enables collaborative learning environment that allows the student to interact regardless of geographical constraints thereby expanding access to science education with enhanced digital literacy and learning outcomes."}
{"paperId": "0f9b36c50e48a7dd663b1853a3738c5f94eff751", "url": "https://www.semanticscholar.org/paper/0f9b36c50e48a7dd663b1853a3738c5f94eff751", "title": "Losing our Tail - Again: On (Un)Natural Selection And Multilingual Large Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.03933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-05", "authors": [{"authorId": "2373033695", "name": "Eva Vanmassenhove"}], "abstract": "Multilingual Large Language Models (LLMs) considerably changed how technologies can influence language. While previous technologies could mediate or assist humans, there is now a tendency to offload the task of writing itself to these technologies, enabling them to change our linguistic ecosystem more directly. While they provide us quick access to information and impressively fluent output, beneath their apparent sophistication lies a subtle, more insidious threat: the gradual decline and loss of linguistic diversity. With this opinion piece, I explore how model collapse, with a particular focus on translation technology, can lead to the loss of linguistic forms, grammatical features, and cultural nuance. Model collapse refers to the eventual consequence of self-consuming training loops, where models reinforce their own biases and lose linguistic diversity. Drawing on recent work in Computer Vision, Natural Language Processing (NLP) and Machine Translation (MT), I argue that the tails of our linguistic distributions are vanishing, and with them, the narratives and identities they carry. This is a call to resist linguistic flattening and to reimagine NLP as a field that encourages, values and protects expressive multilingual lexical and linguistic diversity and creativity."}
{"paperId": "0f9b6dff51281b13a3900ab2ba1a1cf1df198077", "url": "https://www.semanticscholar.org/paper/0f9b6dff51281b13a3900ab2ba1a1cf1df198077", "title": "Food Image Classification and Recipe Recommendation for South Sumatran Cuisine Using EfficientNetB1", "venue": "JURNAL TEKNIK INFORMATIKA", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15408/jti.v18i2.46449?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15408/jti.v18i2.46449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-30", "authors": [{"authorId": "2385602000", "name": "Farhah Salsabillah"}, {"authorId": "2385665208", "name": "Ade Silvia Handayani"}, {"authorId": "2216351513", "name": "Nurhajar Anugraha"}], "abstract": "Visual-based food classification and recipe recommendation systems remain underexplored in the context of local culinary traditions. To address this gap, a system was developed using the EfficientNetB1 architecture of Convolutional Neural Networks (CNN), integrated with a Large Language Model (LLM) to generate South Sumatran recipes from food images, adapting suggestions to classification results. The model was trained using transfer learning on eight food ingredient classes selected for their prevalence in local cuisine. It achieved a validation accuracy of 98.2% and a test accuracy of 98%, with average precision, recall, and F1-score all exceeding 98%, indicating consistent and reliable performance. The system was deployed as a web-based application, DapoerKito, allowing users to upload food images, receive classification results, and obtain generated recipe suggestions. LLM-generated recipes are produced instantly, matched to ingredients, and shown in a clear format. These findings demonstrate the value of integrating computer vision and language generation in an AI-based platform that supports usability and cultural relevance. In addition to its technical capabilities, the system contributes to the digital preservation of regional culinary heritage through interactive AI. This CNN–LLM integration offers a novel approach for advancing food AI with diverse ingredients, personalized nutrition, and multilingual support."}
{"paperId": "10cd5f77f36bb3d64cf4a750a6406513edecce72", "url": "https://www.semanticscholar.org/paper/10cd5f77f36bb3d64cf4a750a6406513edecce72", "title": "Sensing Differently: Unifying Vision, Language, Posture and Tactile in Robotic Perception", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11247209?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11247209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-19", "authors": [{"authorId": "144111479", "name": "Yanmin Zhou"}, {"authorId": "2318118678", "name": "Yiyang Jin"}, {"authorId": "1643699086", "name": "Rong Jiang"}, {"authorId": "2153900162", "name": "Xin Li"}, {"authorId": "2031467037", "name": "Hongrui Sang"}, {"authorId": "2224153707", "name": "Shuo Jiang"}, {"authorId": "2260614650", "name": "Zhipeng Wang"}, {"authorId": "2260639385", "name": "Bin He"}], "abstract": "Multi-modal fusion perception enhances robotic performance in complex tasks by providing more comprehensive information than single modality. While tactile and proprioceptive sensing are effective for direct contact tasks like grasping, current research mainly focuses on vision-language fusion, neglecting other embodied modalities. The primary challenges of this limitation are the difficulty in generating natural language labels for embodied information like tactile and proprioception and aligning them with vision and language. To address this, we introduce VLaPT, a novel multi-modal grasping dataset that aligns vision and language (VL) with posture and tactile (PT), enabling robots to sense differently from environment to self. VLaPT includes 75 objects, 1,533 grasps, and over 78K synchronized vision-language-posturetactile pairs. The dataset incorporates structured, rich-text descriptions generated using modality-level language annotation templates, ensuring effective cross-modality alignment. Leveraging this dataset, we trained a lightweight multi-modal alignment framework, CLIP-ME, which enhances the performance of several downstream tasks with only a 5% increase in parameters. The VLaPT is publicly available in https://huggingface.co/datasets/xsdfasfgsa/VLaPT."}
{"paperId": "10d14166944f022dba43dd75b37ba40c92203b3a", "url": "https://www.semanticscholar.org/paper/10d14166944f022dba43dd75b37ba40c92203b3a", "title": "Multi-Modal One-Shot Federated Ensemble Learning for Medical Data with Vision Large Language Model", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-06", "authors": [{"authorId": "31204411", "name": "Naibo Wang"}, {"authorId": "2297204028", "name": "Yuchen Deng"}, {"authorId": "2297601373", "name": "Shichen Fan"}, {"authorId": "2297647164", "name": "Jianwei Yin"}, {"authorId": "2297246982", "name": "See-Kiong Ng"}], "abstract": "Federated learning (FL) has attracted considerable interest in the medical domain due to its capacity to facilitate collaborative model training while maintaining data privacy. However, conventional FL methods typically necessitate multiple communication rounds, leading to significant communication overhead and delays, especially in environments with limited bandwidth. One-shot federated learning addresses these issues by conducting model training and aggregation in a single communication round, thereby reducing communication costs while preserving privacy. Among these, one-shot federated ensemble learning combines independently trained client models using ensemble techniques such as voting, further boosting performance in non-IID data scenarios. On the other hand, existing machine learning methods in healthcare predominantly use unimodal data (e.g., medical images or textual reports), which restricts their diagnostic accuracy and comprehensiveness. Therefore, the integration of multi-modal data is proposed to address these shortcomings. In this paper, we introduce FedMME, an innovative one-shot multi-modal federated ensemble learning framework that utilizes multi-modal data for medical image analysis. Specifically, FedMME capitalizes on vision large language models to produce textual reports from medical images, employs a BERT model to extract textual features from these reports, and amalgamates these features with visual features to improve diagnostic accuracy. Experimental results show that our method demonstrated superior performance compared to existing one-shot federated learning methods in healthcare scenarios across four datasets with various data distributions. For instance, it surpasses existing one-shot federated learning approaches by more than 17.5% in accuracy on the RSNA dataset when applying a Dirichlet distribution with ($\\alpha$ = 0.3)."}
{"paperId": "11267e5bb5f42abca46b1aaba968faf86562616a", "url": "https://www.semanticscholar.org/paper/11267e5bb5f42abca46b1aaba968faf86562616a", "title": "Teaching about Accessibility in Computer Science Education", "venue": "Inroads", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3729877?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3729877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2290779288", "name": "Richard E. Ladner"}, {"authorId": "2257053563", "name": "Stephanie Ludi"}, {"authorId": "2355842009", "name": "Robert J. Domanski"}], "abstract": "Accessibility, in the context of computer science, is about making computing products accessible to people with disabilities. This means designing hardware and software products that can be used effectively by people who have difficulty reading a computer screen, hearing computer prompts, or controlling the keyboard, mouse, or touchscreen. Thus, accessibility topics should be woven into any course about human-facing applications or websites, such as app and web design/development, software engineering, and human-computer interaction. In addition, accessibility is about creating technical solutions to accessibility problems that people with disabilities encounter in everyday living. These technical solutions may include the use of artificial intelligence, computer vision, natural language processing, or other CS topics. Thus, accessibility topics can be included in technical courses, particularly those that incorporate projects where students attempt to solve accessibility problems using techniques taught in the course. There are practical, intellectual, and social reasons to integrate accessibility into computer science curriculum. From a practical standpoint, employers increasingly include accessibility knowledge in job descriptions because they want their products and services to be accessible to more customers and for legal compliance. From an intellectual standpoint, technical solutions to many accessibility problems often require creativity and a multi-disciplinary approach that includes understanding user needs integrated with technical knowledge. From a social standpoint, accessibility is an important topic in addressing inclusivity and an attractive topic for those students who enter the field to do social good, leading to a broader mix of students in terms of gender, race, ethnicity, and ability. In this paper we define more technically what accessibility and disability mean, enumerate the major technologies that disabled people use for accessibility, discuss the accessibility standards used throughout the world, discuss the importance of accessibility research, and give advice on what accessibility topics can be used in several computer science courses. Throughout the paper we list some resources that teachers can use to add accessibility to their courses and to make their courses more accessible to disabled students."}
{"paperId": "11f913c27d8b9e283e3e4d655be6065a8d1a543e", "url": "https://www.semanticscholar.org/paper/11f913c27d8b9e283e3e4d655be6065a8d1a543e", "title": "KN-VLM: KNowledge-guided Vision-and-Language Model for visual abductive reasoning", "venue": "Multimedia Systems", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-01683-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-01683-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-09", "authors": [{"authorId": "2349968520", "name": "Kuo Tan"}, {"authorId": "1993592720", "name": "Zhaobo Qi"}, {"authorId": "2277274900", "name": "Jianping Zhong"}, {"authorId": "2337758661", "name": "Yuanrong Xu"}, {"authorId": "2337766467", "name": "Weigang Zhang"}], "abstract": null}
{"paperId": "11fe7b9236477c5ed162b0746d2083067c13f706", "url": "https://www.semanticscholar.org/paper/11fe7b9236477c5ed162b0746d2083067c13f706", "title": "Steering CLIP's vision transformer with sparse autoencoders", "venue": "arXiv.org", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2355082098", "name": "Sonia Joseph"}, {"authorId": "2355309836", "name": "Praneet Suresh"}, {"authorId": "2355080491", "name": "Ethan Goldfarb"}, {"authorId": "2203793946", "name": "Lorenz Hufe"}, {"authorId": "52164591", "name": "Yossi Gandelsman"}, {"authorId": "2355082065", "name": "Robert Graham"}, {"authorId": "2355080520", "name": "Danilo Bzdok"}, {"authorId": "2242938198", "name": "Wojciech Samek"}, {"authorId": "2311697786", "name": "Blake Richards"}], "abstract": "While vision models are highly capable, their internal mechanisms remain poorly understood -- a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by training SAEs on CLIP's vision transformer and uncover key differences between vision and language processing, including distinct sparsity patterns for SAEs trained across layers and token types. We then provide the first systematic analysis on the steerability of CLIP's vision transformer by introducing metrics to quantify how precisely SAE features can be steered to affect the model's output. We find that 10-15\\% of neurons and features are steerable, with SAEs providing thousands more steerable features than the base model. Through targeted suppression of SAE features, we then demonstrate improved performance on three vision disentanglement tasks (CelebA, Waterbirds, and typographic attacks), finding optimal disentanglement in middle model layers, and achieving state-of-the-art performance on defense against typographic attacks."}
{"paperId": "120fe69006023d0fcb0b793700ecdc31777d8899", "url": "https://www.semanticscholar.org/paper/120fe69006023d0fcb0b793700ecdc31777d8899", "title": "'Hi AirStar, Guide Me to the Badminton Court.'", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.04430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-07-06", "authors": [{"authorId": "2265646739", "name": "Ziqin Wang"}, {"authorId": "2371144611", "name": "Jinyu Chen"}, {"authorId": "2372418376", "name": "Xiangyi Zheng"}, {"authorId": "2372472052", "name": "Qinan Liao"}, {"authorId": "2292210560", "name": "Linjiang Huang"}, {"authorId": "2373989817", "name": "Si Liu"}], "abstract": "Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base.It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability. Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent.The supplementary PPT is available at https://buaa-colalab.github.io/airstar.github.io."}
{"paperId": "125704b2b8d6b48ae0f46e72c8e91b3fbd36d9b6", "url": "https://www.semanticscholar.org/paper/125704b2b8d6b48ae0f46e72c8e91b3fbd36d9b6", "title": "Integrated Multimodal AI Architecture: Cross-Modal Attention Mechanisms Unifying Text, Visual, And Audio Data Streams For Enterprise Content Analysis", "venue": "Journal of International Crisis and Risk Communication Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63278/jicrcr.vi.3203?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63278/jicrcr.vi.3203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-20", "authors": [{"authorId": "2378542945", "name": "Naganarendar Chitturi"}], "abstract": "Multimodal artificial intelligence is a revolutionary paradigm change in business content understanding, extending past conventional unimodal systems toward architectures that can process and correlate text, visual, audio, and video information at the same time within integrated computational environments. Transformer architectures modified with cross-modal attention mechanisms allow substantive interactions between disparate types of data through common semantic spaces and adaptive attention mechanisms. Implementation issues of data heterogeneity, quality assurance across modalities, management of computational resources, and enterprise scalability are met with innovative solutions such as dynamic time warping algorithms, cascaded quality filters, and distributed processing architectures. Enterprise applications such as intelligent document processing, multimedia customer insights, automated quality control, cross-modal search systems, and integrated decision support address practical impact in various industries. Technical foundations are focused on uniform representation learning that maps disparate modalities into common semantic spaces where distances encode concept similarity instead of surface features. Sophisticated preprocessing pipelines use uniform language instructions to represent vision focused tasks so that they can be customized flexibly at various levels of granularity. Industrial and quality control uses are assisted by sensor networks that can process heterogeneous data across several monitoring points, while multimedia customer understanding uses a single-vision-language model with competitive performance metrics on standard benchmarks. Directions for the future involve efficient bootstrapping methods using frozen pre-trained models, general purpose frameworks processing arbitrary inputs and outputs with linear scaling, and strategic deployment considerations prioritizing foundation model progress from vision and language communities."}
{"paperId": "126b4bbda791acb46eb2cd7bc15f1fe7d8274ddc", "url": "https://www.semanticscholar.org/paper/126b4bbda791acb46eb2cd7bc15f1fe7d8274ddc", "title": "FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19421, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2367750197", "name": "Songqi Zhou"}, {"authorId": "2278583468", "name": "Zeyuan Liu"}, {"authorId": "2260996732", "name": "Benben Jiang"}], "abstract": "Ensuring fairness in machine learning models is a critical challenge. Existing debiasing methods often compromise performance, rely on static correction strategies, and struggle with data sparsity, particularly within minority groups. Furthermore, their utilization of sensitive attributes is often suboptimal, either depending excessively on complete attribute labeling or disregarding these attributes entirely. To overcome these limitations, we propose FairNet, a novel framework for dynamic, instance-level fairness correction. FairNet integrates a bias detector with conditional low-rank adaptation (LoRA), which enables selective activation of the fairness correction mechanism exclusively for instances identified as biased, and thereby preserve performance on unbiased instances. A key contribution is a new contrastive loss function for training the LoRA module, specifically designed to minimize intra-class representation disparities across different sensitive groups and effectively address underfitting in minority groups. The FairNet framework can flexibly handle scenarios with complete, partial, or entirely absent sensitive attribute labels. Theoretical analysis confirms that, under moderate TPR/FPR for the bias detector, FairNet can enhance the performance of the worst group without diminishing overall model performance, and potentially yield slight performance improvements. Comprehensive empirical evaluations across diverse vision and language benchmarks validate the effectiveness of FairNet."}
{"paperId": "12888182caf7975e748c3756be4039bf439242fb", "url": "https://www.semanticscholar.org/paper/12888182caf7975e748c3756be4039bf439242fb", "title": "AI-Driven Automation: Transforming Industry 5.0 withMachine Learning and Advanced Technologies", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.62311/nesx/rr225?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.62311/nesx/rr225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["CaseReport"], "publicationDate": "2025-03-12", "authors": [{"authorId": "2295364655", "name": "Murali Krishna Pasupuleti"}], "abstract": "Abstract:\nThis article delves into the transformative role of artificial intelligence (AI) and machine learning (ML) in shaping Industry 5.0, a paradigm centered on human- machine collaboration, sustainability, and resilient industrial ecosystems. Beginning with the evolution from Industry 4.0 to Industry 5.0, it examines core AI technologies, including predictive analytics, natural language processing, and computer vision, which drive advancements in manufacturing, quality control, and adaptive logistics. Key discussions include the integration of collaborative robots (cobots) that enhance human productivity, AI-driven sustainability practices for energy and resource efficiency, and predictive maintenance models that reduce downtime. Addressing ethical challenges, the Article highlights the importance of data privacy, unbiased algorithms, and the environmental responsibility of intelligent automation. Through case studies across manufacturing, healthcare, and energy sectors, readers gain insights into real-world applications of AI and ML, showcasing their impact on efficiency, quality, and safety. The Article concludes with future directions, emphasizing emerging technologies like quantum computing, human-machine synergy, and the sustainable vision for Industry 5.0, where intelligent automation not only drives innovation but also aligns with ethical and social values for a resilient industrial future.\nKeywords:\nIndustry 5.0, intelligent automation, AI, machine learning, sustainability, human- machine collaboration, cobots, predictive maintenance, quality control, ethical AI, data privacy, Industry 4.0, computer vision, natural language processing, energy efficiency, adaptive logistics, environmental responsibility, industrial ecosystems, quantum computing."}
{"paperId": "12b75f30278b26263d336c5c5486aeec3ff53d38", "url": "https://www.semanticscholar.org/paper/12b75f30278b26263d336c5c5486aeec3ff53d38", "title": "Stochastic feature selection with annealing and its applications to streaming data", "venue": "Journal of nonparametric statistics (Print)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10485252.2025.2456767?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10485252.2025.2456767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-30", "authors": [{"authorId": "2110693329", "name": "Lizhe Sun"}, {"authorId": "2285172483", "name": "A. Barbu"}], "abstract": "Feature selection is an important topic in high-dimensional statistics and machine learning, for prediction and understanding the underlying phenomena. It has many applications in computer vision, natural language processing, bioinformatics, etc. However, most feature selection methods in the literature have been proposed for offline learning, and the existing online feature selection methods have theoretical and practical limitations in true support recovery. This paper proposes two novel online feature selection methods by stochastic gradient descent with a hard thresholding operator. The proposed methods can simultaneously select the relevant features and build linear regression or classification models based on the selected variables. The theoretical justification is provided for the consistency of the proposed methods. Numerical experiments on simulated and real sparse datasets show that the proposed methods compare favourably with state-of-the-art online methods from the literature."}
{"paperId": "12d18016e33b67ad99cbd874fca856c7ac42a480", "url": "https://www.semanticscholar.org/paper/12d18016e33b67ad99cbd874fca856c7ac42a480", "title": "CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.17847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-11", "authors": [{"authorId": "2387679532", "name": "Yichen Yan"}, {"authorId": "2386824695", "name": "Ming Zhong"}, {"authorId": "2386846154", "name": "Qi Zhu"}, {"authorId": "2387888677", "name": "Xiaoling Gu"}, {"authorId": "2376520174", "name": "Jinpeng Chen"}, {"authorId": "2388868875", "name": "Huan Li"}], "abstract": "Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity. We introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling efficient and scalable data selection. In our experiments, we trained the CoIDO scorer using only 20 percent of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20 percent subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2 percent of the performance of full-data fine-tuning, on average."}
{"paperId": "131a25c039a73c2ccaef861bc2cf61d153d46ed9", "url": "https://www.semanticscholar.org/paper/131a25c039a73c2ccaef861bc2cf61d153d46ed9", "title": "Perceiving Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models", "venue": "", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.05626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-05-08", "authors": [{"authorId": "9432300", "name": "Aarti Ghatkesar"}, {"authorId": "66960240", "name": "Uddeshya Upadhyay"}, {"authorId": "2360174310", "name": "Ganesh Venkatesh"}], "abstract": "Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks."}
{"paperId": "13c4ed53078592a8266ec095c270667f148cfd8a", "url": "https://www.semanticscholar.org/paper/13c4ed53078592a8266ec095c270667f148cfd8a", "title": "Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.21520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-29", "authors": [{"authorId": "2325912933", "name": "Zijian Zhang"}, {"authorId": "2277413954", "name": "Xiaochen Zhang"}, {"authorId": "2321931603", "name": "Yang Zhou"}, {"authorId": "2325900078", "name": "Zhimin Lin"}, {"authorId": "2307318560", "name": "Peng Yan"}], "abstract": "Vision Large Language Models (VLLMs) have improved multi-modal understanding and visual question answering (VQA), but still suffer from hallucinated answers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these issues by incorporating external information, yet challenges remain in visual context comprehension, multi-source retrieval, and multi-turn interactions. To address these challenges, Meta constructed the CRAG-MM benchmark and launched the CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This paper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl team. We use a single model for each task, with key methods including data augmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve automatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win second place in Task3 after human evaluation."}
{"paperId": "13e147bb49469a26f7190a2b1b1332daf285c076", "url": "https://www.semanticscholar.org/paper/13e147bb49469a26f7190a2b1b1332daf285c076", "title": "Visual and Textual Commonsense-Enhanced Layout Learning for Vision-and-Language Navigation", "venue": "IEEE Transactions on Automation Science and Engineering", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASE.2025.3604018?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASE.2025.3604018, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2135423959", "name": "Fang Gao"}, {"authorId": "2261687707", "name": "Lei Shi"}, {"authorId": "2261756985", "name": "Jingfeng Tang"}, {"authorId": "2324991081", "name": "Jiabao Wang"}, {"authorId": "2244166456", "name": "Shaodong Li"}, {"authorId": "2261659139", "name": "Shengheng Ma"}, {"authorId": "2258009330", "name": "Jun Yu"}], "abstract": "In the Vision-and-Language Navigation (VLN) task, an agent must comprehend natural language instructions and execute precise navigation in complex environments. While significant progress has been made in the VLN field, the limited availability of navigation data hinders existing methods from fully learning the commonsense relationships between rooms and landmarks, which are crucial for environmental understanding and successful navigation. To address this issue, this work proposes a Visual and Textual Commonsense-Enhanced Layout Learning Model (ViTeC). We leverage the open-world knowledge embedded in large models by utilizing ChatGPT and BLIP-2 to provide commonsense information about environments. Specifically, BLIP-2 analyzes the room type corresponding to each panoramic image, while ChatGPT infers and provides knowledge about the most common landmarks within each room type. Moreover, to compensate for the agent’s lack of commonsense at the visual level, we employ Stable Diffusion to generate commonsense-based visual images, enhancing the agent’s visual perception. To ensure the agent effectively learns commonsense about the environment, we designed a Text Commonsense Layout Learning Module and a Visual Commonsense Layout Learning Module. These modules help the agent acquire environmental commonsense from both linguistic and visual perspectives, enabling it to utilize commonsense information effectively during navigation, thereby improving its environmental understanding and reasoning capabilities. Experimental results demonstrate that ViTeC achieves strong performance on REVERIE, R2R, and SOON datasets, exhibiting good generalization ability in complex environments. This validates the effectiveness of ViTeC in enhancing the agent’s environmental understanding and navigation capabilities. Note to Practitioners—In real-world applications, service robots often lack commonsense knowledge of environmental layouts, leading to inefficient navigation in complex scenarios, particularly in smart homes, medical assistance, and retail guidance. To address this issue, our research integrates knowledge from large models and visual generation techniques, enabling navigation agents to leverage commonsense information about room layouts and the distribution of common objects to enhance path planning. The advantage of this approach lies in improving the agent’s understanding of the environment, thereby reducing unnecessary exploration and increasing navigation accuracy. However, its effectiveness depends on the comprehensiveness of the knowledge base and the agent’s visual reasoning capabilities, which may pose adaptability challenges in highly dynamic or non-standard environments. In the future, this method can be further enhanced through online knowledge updates, adaptive learning, and multimodal data fusion, equipping robots with stronger navigation capabilities in a wider range of real-world scenarios."}
{"paperId": "1469f920acd91c421fd613896e7623471184c59b", "url": "https://www.semanticscholar.org/paper/1469f920acd91c421fd613896e7623471184c59b", "title": "DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.24896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-29", "authors": [{"authorId": "2277670574", "name": "Xi Chen"}, {"authorId": "2291490010", "name": "Hongxun Yao"}, {"authorId": "2211956117", "name": "Zhaopan Xu"}, {"authorId": "2291070246", "name": "Kui Jiang"}], "abstract": "Source-free active domain adaptation (SFADA) enhances knowledge transfer from a source model to an unlabeled target domain using limited manual labels selected via active learning. While recent domain adaptation studies have introduced Vision-and-Language (ViL) models to improve pseudo-label quality or feature alignment, they often treat ViL-based and data supervision as separate sources, lacking effective fusion. To overcome this limitation, we propose Dual Active learning with Multimodal (DAM) foundation model, a novel framework that integrates multimodal supervision from a ViL model to complement sparse human annotations, thereby forming a dual supervisory signal. DAM initializes stable ViL-guided targets and employs a bidirectional distillation mechanism to foster mutual knowledge exchange between the target model and the dual supervisions during iterative adaptation. Extensive experiments demonstrate that DAM consistently outperforms existing methods and sets a new state-of-the-art across multiple SFADA benchmarks and active learning strategies."}
{"paperId": "14c3f9f3a36e91497335fd5a35f474e373b48cf4", "url": "https://www.semanticscholar.org/paper/14c3f9f3a36e91497335fd5a35f474e373b48cf4", "title": "Harnessing large vision and language models in agriculture: a review", "venue": "Frontiers in Plant Science", "year": 2025, "citationCount": 16, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12436425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-09-02", "authors": [{"authorId": "2313645044", "name": "Hongyan Zhu"}, {"authorId": "2313692692", "name": "Shuai Qin"}, {"authorId": "2313638673", "name": "Min Su"}, {"authorId": "2313803392", "name": "Chengzhi Lin"}, {"authorId": "2313697200", "name": "Anjie Li"}, {"authorId": "2313701042", "name": "Junfeng Gao"}], "abstract": "Introduction Agriculture is a cornerstone of human society but faces significant challenges, including pests, diseases, and the need for increased production efficiency. Large models, encompassing large language models, large vision models, and multimodal large language models, have shown transformative potential in various domains. This review aims to explore the potential applications of these models in agriculture to address existing problems and improve production. Methods We conduct a systematic review of the development trajectories and key capabilities of large models. A bibliometric analysis of literature from Web of Science and arXiv is performed to quantify the current research focus and identify the gap between the potential and the application of large models in the agricultural sector. Results Our analysis confirms that agriculture is an emerging but currently underrepresented field for large model research. Nevertheless, we identify and categorize promising applications, including tailored models for agricultural question-answering, robotic automation, and advanced image analysis from remote sensing and spectral data. These applications demonstrate significant potential to solve complex, nuanced agricultural tasks. Discussion This review culminates in a pragmatic framework to guide the choice between large and traditional models, balancing data availability against deployment constraints. We also highlight critical challenges, including data acquisition, infrastructure barriers, and the significant ethical considerations for responsible deployment. We conclude that while tailored large models are poised to greatly enhance agricultural efficiency and yield, realizing this future requires a concerted effort to overcome the existing technical, infrastructural, and ethical hurdles."}
{"paperId": "152696dfc3a9bbbf70c4935fc72e7945910fb878", "url": "https://www.semanticscholar.org/paper/152696dfc3a9bbbf70c4935fc72e7945910fb878", "title": "2-D GAF-Enhanced Multimodal Vision Language Model for Breathing Patterns Analysis via ECG Sensing", "venue": "IEEE Sensors Journal", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2025.3617772?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2025.3617772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-01", "authors": [{"authorId": "2266841922", "name": "Amine Bechar"}, {"authorId": "2273995819", "name": "Abbes Amira"}, {"authorId": "2287972997", "name": "Adel Oulefki"}, {"authorId": "1842426", "name": "Youssef Elmir"}, {"authorId": "2130461741", "name": "Yassine Himeur"}], "abstract": "The use of vision large language models (VLLMs) for the analysis and diagnosis of respiratory patterns is affected by limited specialist training data and the lack of integrated medical expertise. This article introduces a method that uses a multimodal VLLM to analyze electrocardiogram (ECG) data transformed to Gramian angular fields (GAFs) 2-D image to determine respiratory patterns. Using advanced Shimmer3 sensing, five VLLMs are fine-tuned using four different ECG datasets for training, each containing 5000 samples, to improve the interpretation of ECG signals. VLLMs are then evaluated against an independent set of data to determine normal, rapid, and difficult breathing patterns. The low-rank adaptation method is used to optimize VLLMs for the analysis of ECG signals obtained from the shimmer sensing platform. Results show that Idefics outperforms other fine-tuned VLLMs with a BLEU score of 0.572 for 2-D ECG signals and 0.549 for 1-D ECG signals, which demonstrates that the 2-D image representation of the GAF improves the visualization of ECG respiratory patterns."}
{"paperId": "153de6cd8c13907d937010719ea6d747351d1aef", "url": "https://www.semanticscholar.org/paper/153de6cd8c13907d937010719ea6d747351d1aef", "title": "Egocentric Action-aware Inertial Localization in Point Clouds", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-20", "authors": [{"authorId": "2290240304", "name": "Mingfang Zhang"}, {"authorId": "1899753", "name": "Ryo Yonetani"}, {"authorId": "48355651", "name": "Yifei Huang"}, {"authorId": "2310610152", "name": "Liangyang Ouyang"}, {"authorId": "2310627664", "name": "Ruicong Liu"}, {"authorId": "2287850073", "name": "Yoichi Sato"}], "abstract": "This paper presents a novel inertial localization framework named Egocentric Action-aware Inertial Localization (EAIL), which leverages egocentric action cues from head-mounted IMU signals to localize the target individual within a 3D point cloud. Human inertial localization is challenging due to IMU sensor noise that causes trajectory drift over time. The diversity of human actions further complicates IMU signal processing by introducing various motion patterns. Nevertheless, we observe that some actions captured by the head-mounted IMU correlate with spatial environmental structures (e.g., bending down to look inside an oven, washing dishes next to a sink), thereby serving as spatial anchors to compensate for the localization drift. The proposed EAIL framework learns such correlations via hierarchical multi-modal alignment with vision-language guidance. By assuming that the 3D point cloud of the environment is available, it contrastively learns modality encoders that align short-term egocentric action cues in IMU signals with local environmental features in the point cloud. The learning process is enhanced using concurrently collected vision and language signals to improve multimodal alignment. The learned encoders are then used in reasoning the IMU data and the point cloud over time and space to perform inertial localization. Interestingly, these encoders can further be utilized to recognize the corresponding sequence of actions as a by-product. Extensive experiments demonstrate the effectiveness of the proposed framework over state-of-the-art inertial localization and inertial action recognition baselines."}
{"paperId": "157a61208570f2aa022fe7967c6b5093389e9e9b", "url": "https://www.semanticscholar.org/paper/157a61208570f2aa022fe7967c6b5093389e9e9b", "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation", "venue": "International Conference on Intelligent Computing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-30", "authors": [{"authorId": "2359156440", "name": "Dongsheng Yang"}, {"authorId": "2375126910", "name": "Meiling Zhu"}, {"authorId": "2374469388", "name": "Yinfeng Yu"}], "abstract": "Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency."}
{"paperId": "16c39d1ac20e33b38d81ce02d01bcdc72c4f9a47", "url": "https://www.semanticscholar.org/paper/16c39d1ac20e33b38d81ce02d01bcdc72c4f9a47", "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-09", "authors": [{"authorId": "2325096817", "name": "Angela van Sprang"}, {"authorId": "151492509", "name": "Laurens Samson"}, {"authorId": "2397486657", "name": "Ana Lucic"}, {"authorId": "2325047660", "name": "Erman Acar"}, {"authorId": "1682828", "name": "S. Ghebreab"}, {"authorId": "2397487119", "name": "Yuki M. Asano"}], "abstract": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs."}
{"paperId": "16fcbe44a9fd88d8e5c8eb32ba9d5d27d7bfa776", "url": "https://www.semanticscholar.org/paper/16fcbe44a9fd88d8e5c8eb32ba9d5d27d7bfa776", "title": "VISTA-OCR: Towards generative and interactive end to end OCR models", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-04", "authors": [{"authorId": "2353956067", "name": "Laziz Hamdi"}, {"authorId": "2353955528", "name": "Amine Tamasna"}, {"authorId": "2353956546", "name": "Pascal Boisson"}, {"authorId": "2353956070", "name": "Thierry Paquet"}], "abstract": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis OCR), a lightweight architecture that unifies text detection and recognition within a single generative model. Unlike conventional methods that require separate branches with dedicated parameters for text recognition and detection, our approach leverages a Transformer decoder to sequentially generate text transcriptions and their spatial coordinates in a unified branch. Built on an encoder-decoder architecture, VISTA-OCR is progressively trained, starting with the visual feature extraction phase, followed by multitask learning with multimodal token generation. To address the increasing demand for versatile OCR systems capable of advanced tasks, such as content-based text localization \\ref{content_based_localization}, we introduce new prompt-controllable OCR tasks during pre-training.To enhance the model's capabilities, we built a new dataset composed of real-world examples enriched with bounding box annotations and synthetic samples. Although recent Vision Large Language Models (VLLMs) can efficiently perform these tasks, their high computational cost remains a barrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$ variant processes both handwritten and printed documents with only 150M parameters, interactively, by prompting. Extensive experiments on multiple datasets demonstrate that VISTA-OCR achieves better performance compared to state-of-the-art specialized models on standard OCR tasks while showing strong potential for more sophisticated OCR applications, addressing the growing need for interactive OCR systems. All code and annotations for VISTA-OCR will be made publicly available upon acceptance."}
{"paperId": "17808587cde05e96315be4b48c92d7db1bb71922", "url": "https://www.semanticscholar.org/paper/17808587cde05e96315be4b48c92d7db1bb71922", "title": "Boosting remote semantic segmentation using vision-and-language foundation model", "venue": "The Visual Computer", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00371-025-03968-9?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00371-025-03968-9, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-28", "authors": [{"authorId": "2357226148", "name": "Qiuyue Zhang"}, {"authorId": "2319175224", "name": "Zhiwang Zhang"}, {"authorId": "2319209281", "name": "Shiting Wen"}, {"authorId": "2319130932", "name": "Chaoyi Pang"}, {"authorId": "2369283955", "name": "Fangyu Wu"}], "abstract": null}
{"paperId": "17b60373834f9e4e33c742d44d5b113c944377a3", "url": "https://www.semanticscholar.org/paper/17b60373834f9e4e33c742d44d5b113c944377a3", "title": "Artificial intelligence technology and its application in intelligent robotics", "venue": "International Conference on Power Electronics and Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3066779?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3066779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference", "Review"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2369541834", "name": "Chenglin Wang"}, {"authorId": "2370166165", "name": "Xiuhua Xia"}, {"authorId": "2369499330", "name": "Ligang Xiu"}, {"authorId": "2239157511", "name": "Heung Kou"}], "abstract": "With the rapid development of artificial intelligence technology, intelligent robots are increasingly used in many fields. AI's core technologies, including machine learning, computer vision, natural language processing, and deep learning, empower robots to improve their perception, decision-making, and execution capabilities. This paper first reviews the development of artificial intelligence technology, and summarizes the key technologies and applications of AI in the field of intelligent robots. The application of machine learning algorithms in autonomous learning and adaptability of intelligent robots is discussed, especially in path planning, object recognition, intelligent interaction and multi-robot collaboration. This article then explores the innovative applications of AI-powered intelligent robots in industrial automation, medical assistance, home services, agriculture, and analyzes current technology challenges such as computing power, data security, ethics, and human collaboration. Finally, this paper looks forward to the potential of AI technology in future intelligent robots, and puts forward research directions on optimizing algorithms, enhancing robot autonomy, and improving human-computer interaction experience. Through the in-depth analysis of the combination of AI technology and intelligent robots, this paper provides theoretical support and practical guidance for the further development of intelligent robots."}
{"paperId": "17cec1ed929a7d9b10e0466901ac93659ff846ec", "url": "https://www.semanticscholar.org/paper/17cec1ed929a7d9b10e0466901ac93659ff846ec", "title": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.05740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-05", "authors": [{"authorId": "2179685150", "name": "Lennart Maack"}, {"authorId": "1490903196", "name": "J. Grass"}, {"authorId": "2397179125", "name": "Lisa-Marie Toscha"}, {"authorId": "2333078837", "name": "Nathaniel Melling"}, {"authorId": "2249094520", "name": "Alexander Schlaefer"}], "abstract": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding."}
{"paperId": "18204d9e2c8329be68bfd33348bb4c241d38771b", "url": "https://www.semanticscholar.org/paper/18204d9e2c8329be68bfd33348bb4c241d38771b", "title": "Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.03056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-03", "authors": [{"authorId": "150045991", "name": "Behnam Parsaeifard"}, {"authorId": "2358235", "name": "Martin Hlosta"}, {"authorId": "1898270", "name": "P. Bergamin"}], "abstract": "With the rise of online learning, the demand for efficient and consistent assessment in mathematics has significantly increased over the past decade. Machine Learning (ML), particularly Natural Language Processing (NLP), has been widely used for autograding student responses, particularly those involving text and/or mathematical expressions. However, there has been limited research on autograding responses involving students'handwritten graphs, despite their prevalence in Science, Technology, Engineering, and Mathematics (STEM) curricula. In this study, we implement multimodal meta-learning models for autograding images containing students'handwritten graphs and text. We further compare the performance of Vision Large Language Models (VLLMs) with these specially trained metalearning models. Our results, evaluated on a real-world dataset collected from our institution, show that the best-performing meta-learning models outperform VLLMs in 2-way classification tasks. In contrast, in more complex 3-way classification tasks, the best-performing VLLMs slightly outperform the meta-learning models. While VLLMs show promising results, their reliability and practical applicability remain uncertain and require further investigation."}
{"paperId": "18ae965de944325ceef3d863687320caa8ec03fb", "url": "https://www.semanticscholar.org/paper/18ae965de944325ceef3d863687320caa8ec03fb", "title": "Emotion Analysis of Children’s Drawings", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3606359?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3606359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "32656225", "name": "S. Çiftçi"}, {"authorId": "72076274", "name": "Hasari Karci"}, {"authorId": "2279921015", "name": "Neval Karaca"}, {"authorId": "2294457581", "name": "Büşra Söylemez"}, {"authorId": "2379465672", "name": "Halil Koçakoğlu"}], "abstract": "This study investigates children’s emotional expression through the computational analysis of their drawings. Prior research has frequently relied on proprietary or small-scale datasets, often limited to digital drawings produced on tablets or smartphones, which may not fully capture the natural characteristics of hand-drawn expression. To address these gaps, we introduce KIDO, a novel publicly available dataset of scanned children’s drawings paired with reflections written by the children on whether their drawings represent happiness or sadness. The dataset contains contributions from 5,430 students, with balanced representation across primary and secondary education levels and additional metadata on gender and educational background. For our experiments, we employed CNN-based and Vision Transformer–based models for emotion recognition, obtaining F1-scores of up to 79.03% from drawings, while BERT-based models achieved accuracies of up to 97.47% in classifying children’s written reflections. We further trained a generative model on the KIDO dataset to synthesize emotionally expressive drawings, with subjective evaluations indicating that children often preferred the AI-generated images to those drawn by their peers. KIDO serves as a comprehensive benchmark for advancing research in the fields of computer vision, natural language processing, psychology, and education, and is publicly available at github.com/serdarciftci/KIDO"}
{"paperId": "1955ed17760e8926cfcb3195a1b8e2e5860b303b", "url": "https://www.semanticscholar.org/paper/1955ed17760e8926cfcb3195a1b8e2e5860b303b", "title": "Analytical-Chemistry-Informed Transformer for Infrared Spectra Modeling", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i16.33917?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i16.33917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2110444524", "name": "Shiluo Huang"}, {"authorId": "2355326374", "name": "Yining Jin"}, {"authorId": "2112344030", "name": "W. Jin"}, {"authorId": "2278152729", "name": "Ying Mu"}], "abstract": "Infrared (IR) spectroscopy is a fundamental technique in analytical chemistry. Recently, deep learning (DL) has drawn great interest as the modeling method of infrared spectral data. However, unlike vision or language tasks, IR spectral data modeling is faced with the problem of calibration transfer and has distinctive characteristics. Introducing the prior knowledge of IR spectroscopy could guide the DL methods to learn representations aligned with the domain-invariant characteristics of spectra, and thus improve the performance. Despite such potential, there is a notable absence of DL methods that incorporate such inductive bias. To this end, we propose Analytical-Chemistry-Informed Transformer (ACT) with two modules informed by the field knowledge in analytical chemistry. First, ACT includes learnable spectral processing inspired by chemometrics, which comprises spectral pre-processing, tokenization, and post-processing. Second, a straightforward yet effective representation learning mechanism, namely spectral-attention, is incorporated into ACT. Spectral-attention utilizes the intra-spectral and inter-spectral correlations to extract spectral representations. Empirical results show that ACT has achieved competitive results in 9 analytical tasks covering applications across pharmacy, chemistry, and agriculture. Compared with existing networks, ACT reduces the root mean square error of prediction (RMSEP) by more than 20% in calibration transfer tasks. These results indicate that DL methods in IR spectroscopy could benefit from the integration of prior knowledge in analytical chemistry."}
{"paperId": "19829fd90144554681fe08fa3d0b72693b5d65eb", "url": "https://www.semanticscholar.org/paper/19829fd90144554681fe08fa3d0b72693b5d65eb", "title": "MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-06-06", "authors": [{"authorId": "2134590813", "name": "A. Condez"}, {"authorId": "2187308615", "name": "Diogo Tavares"}, {"authorId": "2366009770", "name": "João Magalhães"}], "abstract": "Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content---a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values."}
{"paperId": "19e7d97f0863cf6d519e1904e604d11ce3a3ec33", "url": "https://www.semanticscholar.org/paper/19e7d97f0863cf6d519e1904e604d11ce3a3ec33", "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.26642, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-30", "authors": [{"authorId": "2349957939", "name": "Zhuoyang Liu"}, {"authorId": "2258602418", "name": "Jiaming Liu"}, {"authorId": "2246750884", "name": "Jiadong Xu"}, {"authorId": "2359146831", "name": "Nuowei Han"}, {"authorId": "2332540856", "name": "Chenyang Gu"}, {"authorId": "2383319906", "name": "Hao Chen"}, {"authorId": "2303804385", "name": "Kaichen Zhou"}, {"authorId": "2275104296", "name": "Renrui Zhang"}, {"authorId": "2383001456", "name": "Kai Chin Hsieh"}, {"authorId": "2112562410", "name": "Kun Wu"}, {"authorId": "1939695", "name": "Zhengping Che"}, {"authorId": "2371141386", "name": "Jian Tang"}, {"authorId": "2242835846", "name": "Shanghang Zhang"}], "abstract": "Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla"}
{"paperId": "19e8e90aa36fd4eb185ab473f1a2c50f3f519261", "url": "https://www.semanticscholar.org/paper/19e8e90aa36fd4eb185ab473f1a2c50f3f519261", "title": "Visual Large Language Models for Graphics Understanding: A Case Study on Floorplan Images", "venue": "ACM Symposium on Document Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3704268.3748681?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3704268.3748681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2025-08-27", "authors": [{"authorId": "2339000478", "name": "Valeria Nardoni"}, {"authorId": "2378127476", "name": "Kimiya Noor Ali"}, {"authorId": "51227108", "name": "Zahra Ziran"}, {"authorId": "2350578001", "name": "Simone Marinai"}], "abstract": "This study explores the use of Vision Large Language Models (VLLMs) for identifying items in complex graphical documents. In particular, we focus on looking for furniture objects (e.g. beds, tables, and chairs) and structural items (doors and windows) in floorplan images. We evaluate one object detection model (YOLO) and state-of-the-art VLLMs on two datasets featuring diverse floorplan layouts and symbols. The experiments with VLLMs are performed with a zero-shot setting, meaning the models are tested without any training or fine-tuning, as well as with a few-shot approach, where examples of items to be found in the image are given to the models in the prompt. The results highlight the strengths and limitations of VLLMs in recognizing architectural elements, providing guidance for future research in the use multimodal vision-language models for graphics recognition."}
{"paperId": "1a2626699188016f640face96f0c90383e7f8d32", "url": "https://www.semanticscholar.org/paper/1a2626699188016f640face96f0c90383e7f8d32", "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.13019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-17", "authors": [{"authorId": "2142458237", "name": "Liuyi Wang"}, {"authorId": "2374113260", "name": "Xinyuan Xia"}, {"authorId": "2374267555", "name": "Hui Zhao"}, {"authorId": "2311307527", "name": "Hanqing Wang"}, {"authorId": "2257008641", "name": "Tai Wang"}, {"authorId": "2236662733", "name": "Yilun Chen"}, {"authorId": "2920326", "name": "Chengju Liu"}, {"authorId": "2288041201", "name": "Qijun Chen"}, {"authorId": "2277447920", "name": "Jiangmiao Pang"}], "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/."}
{"paperId": "1b3fe9a77e7bd8f6c6e92274208670a0e55dce99", "url": "https://www.semanticscholar.org/paper/1b3fe9a77e7bd8f6c6e92274208670a0e55dce99", "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-09", "authors": [{"authorId": "2211732585", "name": "Rocktim Jyoti Das"}, {"authorId": "2332305613", "name": "Harsh Singh"}, {"authorId": "2313115173", "name": "Diana Turmakhan"}, {"authorId": "2384764013", "name": "Muhammad Abdullah Sohail"}, {"authorId": "2334521542", "name": "Mingfei Han"}, {"authorId": "2026545715", "name": "Preslav Nakov"}, {"authorId": "2282533814", "name": "Fabio Pizzati"}, {"authorId": "2378706619", "name": "Ivan Laptev"}], "abstract": "Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page."}
{"paperId": "1be7b92719cdc7a94bd687988b466116558ebf73", "url": "https://www.semanticscholar.org/paper/1be7b92719cdc7a94bd687988b466116558ebf73", "title": "Multi-scale dual-stream visual feature extraction and graph reasoning for visual question answering", "venue": "Applied intelligence (Boston)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-025-06325-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-025-06325-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-18", "authors": [{"authorId": "66738425", "name": "Abdulganiyu Abdu Yusuf"}, {"authorId": "2267209046", "name": "Chong Feng"}, {"authorId": "2267103407", "name": "Xianling Mao"}, {"authorId": "2330405690", "name": "Xinyan Li"}, {"authorId": "2350925013", "name": "Yunusa Haruna"}, {"authorId": "71731337", "name": "R. Duma"}], "abstract": null}
{"paperId": "1c2d955f5bafedcfd7712578b8b81bbb4de15a46", "url": "https://www.semanticscholar.org/paper/1c2d955f5bafedcfd7712578b8b81bbb4de15a46", "title": "Surprisal reveals diversity gaps in image captioning and different scorers change the story", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-06", "authors": [{"authorId": "51933789", "name": "N. Ilinykh"}, {"authorId": "2253597989", "name": "Simon Dobnik"}], "abstract": "We quantify linguistic diversity in image captioning with surprisal variance - the spread of token-level negative log-probabilities within a caption set. On the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs, decoded with greedy and nucleus sampling, to human captions. Measured with a caption-trained n-gram LM, humans display roughly twice the surprisal variance of models, but rescoring the same captions with a general-language model reverses the pattern. Our analysis introduces the surprisal-based diversity metric for image captioning. We show that relying on a single scorer can completely invert conclusions, thus, robust diversity evaluation must report surprisal under several scorers."}
{"paperId": "1c3cc65128ba43942b860b18582774e79f4a4071", "url": "https://www.semanticscholar.org/paper/1c3cc65128ba43942b860b18582774e79f4a4071", "title": "Why Foundation Models in Pathology Are Failing", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2510.23807?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2510.23807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2272729027", "name": "H. R. Tizhoosh"}], "abstract": null}
{"paperId": "1c72d10b7fa6618f867e617db2525800a830bcaf", "url": "https://www.semanticscholar.org/paper/1c72d10b7fa6618f867e617db2525800a830bcaf", "title": "A Deep Dive into Training Algorithms for Deep Belief Networks", "venue": "Journal of Information Systems Engineering & Management", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52783/jisem.v10i13s.2021?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52783/jisem.v10i13s.2021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-25", "authors": [{"authorId": "2348175459", "name": "Dr.S. Manohar"}], "abstract": "Deep Belief Networks (DBNs) have emerged as powerful tools for feature learning, representation, and generative modeling. This paper presents a comprehensive exploration of the various training algorithms employed in the training of DBNs. DBNs, composed of multiple layers of stochastic hidden units, have found applications in diverse domains such as computer vision, natural language processing, and bioinformatics. The paper begins by delving into the pre-training phase, where Restricted Boltzmann Machines (RBMs) play a central role. We review the Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) algorithms, shedding light on their strengths and weaknesses in initializing deep networks. Emphasis is placed on their applicability to different data types and scales. Moving to the fine-tuning stage, the paper explores the use of backpropagation with gradient descent, discussing modern optimization techniques, including stochastic gradient descent and adaptive learning rate methods. We also examine regularization techniques like dropout and weight decay to address overfitting concerns. Furthermore, we discuss architectural variants of DBNs, such as Convolutional Deep Belief Networks (CDBNs) for image data and Recurrent DBNs for sequential data. We highlight the adaptation of DBNs for specific tasks, including classification, regression, clustering, and generative modeling."}
{"paperId": "1c7a66f5b41a3972918b72e70cc76c1347a5e269", "url": "https://www.semanticscholar.org/paper/1c7a66f5b41a3972918b72e70cc76c1347a5e269", "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-17", "authors": [{"authorId": "2392877122", "name": "Tyler Loakman"}, {"authorId": "2324792330", "name": "Joseph James"}, {"authorId": "2279451646", "name": "Chenghua Lin"}], "abstract": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone."}
{"paperId": "1c926f252c433b99626251263e0000865dfd21f1", "url": "https://www.semanticscholar.org/paper/1c926f252c433b99626251263e0000865dfd21f1", "title": "Synergizing Intelligence: A Comparative Study of Robotics Components and AI Integration", "venue": "International Journal for Sciences and Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.71097/ijsat.v16.i2.4102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.71097/ijsat.v16.i2.4102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-28", "authors": [{"authorId": "2358309442", "name": "Amisha Anil Aher"}, {"authorId": "2358316307", "name": "Nandini Satish Kotkar"}, {"authorId": "2357652690", "name": "Ratna Sadashiv Chaudhari"}], "abstract": "The integration of Artificial Intelligence (AI) with robotics is changing the way machines perceive, learn, and interact with their surroundings. This paper examines how AI improves robotic capabilities, allowing for autonomous decision-making, precision, and flexibility in a variety of industries including automation, agriculture, healthcare, military, and education. By merging classic robotic components with modern AI techniques such as computer vision, natural language processing and robots are evolving into intelligent agents capable of complicated tasks and human-like interactions. \nThe connection of robotics and AI, as well as its effects on organisational and economic dynamics, are summarised in this article. A comparison of techniques used in robotics is presented, along with an exploration of the relationship between robotics and artificial intelligence, and their impact on economic and organizational dynamics. The study contributes to a better understanding of how AI is applied in robotics and how it could shape the future of technology and society."}
{"paperId": "1cc3c84926d9ca2c252e33309d62557a4d3d44cd", "url": "https://www.semanticscholar.org/paper/1cc3c84926d9ca2c252e33309d62557a4d3d44cd", "title": "VLEER: Vision and Language Embeddings for Explainable Whole Slide Image Representation", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-28", "authors": [{"authorId": "2268724842", "name": "A. Nguyen"}, {"authorId": "2189380650", "name": "Keunho Byeon"}, {"authorId": "2291541794", "name": "Kyungeun Kim"}, {"authorId": "2252103832", "name": "Jin Tae Kwak"}], "abstract": "Recent advances in vision-language models (VLMs) have shown remarkable potential in bridging visual and textual modalities. In computational pathology, domain-specific VLMs, which are pre-trained on extensive histopathology image-text datasets, have succeeded in various downstream tasks. However, existing research has primarily focused on the pre-training process and direct applications of VLMs on the patch level, leaving their great potential for whole slide image (WSI) applications unexplored. In this study, we hypothesize that pre-trained VLMs inherently capture informative and interpretable WSI representations through quantitative feature extraction. To validate this hypothesis, we introduce Vision and Language Embeddings for Explainable WSI Representation (VLEER), a novel method designed to leverage VLMs for WSI representation. We systematically evaluate VLEER on three pathological WSI datasets, proving its better performance in WSI analysis compared to conventional vision features. More importantly, VLEER offers the unique advantage of interpretability, enabling direct human-readable insights into the results by leveraging the textual modality for detailed pathology annotations, providing clear reasoning for WSI-level pathology downstream tasks."}
{"paperId": "1d697fbab27c67292bdc9cf13cade27502b4696f", "url": "https://www.semanticscholar.org/paper/1d697fbab27c67292bdc9cf13cade27502b4696f", "title": "LIBA: Language Instructed Multi-granularity Bridge Assistant for 3D Visual Grounding", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i8.32875?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i8.32875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2309584222", "name": "Yuan Wang"}, {"authorId": "47002847", "name": "Yali Li"}, {"authorId": "2321614924", "name": "Eastman Z. Y. Wu"}, {"authorId": "2255485475", "name": "Shen Wang"}], "abstract": "3D Vision Grounding (3D-VG) seeks to unravel referential language and identify targets in 3D physical world. Prevailing methods align with the 2D-VG's pipeline to pinpoint the referred object in a categorical multi-modal reasoning manner. However, the geometric complexities of 3D scenes and the nuanced syntactic structures of language, exacerbates the \\textbf{granularity inconsistency} of point cloud and text features, hindering the development of 3D-VG systems in complex scenarios. Towards this issue, we propose LIBA, a Language-Instructed multi-granularity Bridge Assistant tailored for 3D-VG task. LIBA tackles this issue as follows. (1) \\textit{How to establish a multi-granularity 3D vision-text feature alignment in a unified model}? We advance a bilateral Dynamic Bridge Adapter (DBA) build multi-granularity interaction of 3D vision and language backnones during feature extraction. We further develop the Language-aware Cross-scale Object Modulation (LCOM) module to integrate multi-scale point cloud features modulated by language information. (2) After aligning multi-modal features, \\textit{how to fully harness language model's knowledge to bolster vision concepts understanding}? A LLM-guided Hierarchical Query Selection (LLM-HQS) module incorporates world knowledge of Large Language Model~(LLM) to ground the target referral via an Attribute-then-Relation reasoning process. In this manner, our LIBA inherits reasoning prowess and world knowledge of LLM to bridge point clouds and texts at multiple granularities. Experiments on ScanRefer and Nr3D/Sr3D benchmarks substantiate the superiority of our LIBA, trumping state-of-the-arts by a considerable margin."}
{"paperId": "1d72f5f6695bb270c3c12d06938228f53e24704f", "url": "https://www.semanticscholar.org/paper/1d72f5f6695bb270c3c12d06938228f53e24704f", "title": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks", "venue": "IEEE transactions on multimedia", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-18", "authors": [{"authorId": "2277447303", "name": "Siqi Zhang"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2291994430", "name": "Qunbo Wang"}, {"authorId": "26982950", "name": "Longteng Guo"}, {"authorId": "2350698764", "name": "Zhihua Wei"}, {"authorId": "2331583727", "name": "Jing Liu"}], "abstract": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent."}
{"paperId": "1d78499fa60ea7d0d44ab348e37bba568a3eb9e4", "url": "https://www.semanticscholar.org/paper/1d78499fa60ea7d0d44ab348e37bba568a3eb9e4", "title": "Political Discourse Space: Critical Cognitive Analysis on Inauguration Speech of President Prabowo", "venue": "Salud, Ciencia y Tecnología", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.56294/saludcyt20251645?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.56294/saludcyt20251645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-18", "authors": [{"authorId": "118081244", "name": "Murni Fidiyanti"}, {"authorId": "2282039309", "name": "Suhartono Suhartono"}, {"authorId": "2092265551", "name": "Syamsul Sodiq"}], "abstract": "Objectives: This study aims to analyze the inauguration speech of President Prabowo through the lens of Critical Cognitive Analysis, focusing on the interplay between language and ideological representation in political discourse. The research seeks to understand how Prabowo positions himself within the political landscape and addresses both national and international challenges.Methods: Employing a qualitative approach grounded in a Critical Cognitive Framework, the study analyzes linguistic elements extracted from online news portals. Data collection and analysis are executed through an interactive model, allowing for simultaneous data presentation, reduction, and conclusion drawing. The analysis incorporates the theoretical foundations of Critical Discourse Analysis (CDA) and Cognitive Linguistics.Results: The findings reveal that Prabowo’s speech strategically positions him as a leader with a broad vision, utilizing language to frame national identity, collective goals, and public expectations. The use of deictic expressions and references to historical figures enhances his rhetorical impact, reinforcing his legitimacy as a leader committed to the welfare of the Indonesian people.Conclusions: Prabowo's inauguration speech exemplifies the effective use of language as a tool for ideological framing and public mobilization. It significantly contributes to understanding the role of political discourse in shaping societal perceptions and collective identity."}
{"paperId": "1d849feabccf8f1fbd47d27d682f562701d2876b", "url": "https://www.semanticscholar.org/paper/1d849feabccf8f1fbd47d27d682f562701d2876b", "title": "Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.23148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2387867822", "name": "Aryan Mathur"}, {"authorId": "2387874665", "name": "Asaduddin Ahmed"}], "abstract": "Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy's failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents."}
{"paperId": "1d9be376c0b59524effb999774b64ffe04a57d91", "url": "https://www.semanticscholar.org/paper/1d9be376c0b59524effb999774b64ffe04a57d91", "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-09", "authors": [{"authorId": "2149190087", "name": "Hui Xu"}, {"authorId": "2144264714", "name": "Zhuoyang Liu"}, {"authorId": "2022570319", "name": "Yixiang Luomei"}, {"authorId": "2292898696", "name": "Feng Xu"}], "abstract": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices."}
{"paperId": "1e178f0c5bb9709ae5c7bdb60ecd76f00b0fcd86", "url": "https://www.semanticscholar.org/paper/1e178f0c5bb9709ae5c7bdb60ecd76f00b0fcd86", "title": "Transformers without Normalization", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 72, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-13", "authors": [{"authorId": "2336019460", "name": "Jiachen Zhu"}, {"authorId": "2281064954", "name": "Xinlei Chen"}, {"authorId": "2058350112", "name": "Kaiming He"}, {"authorId": "2265899558", "name": "Yann LeCun"}, {"authorId": "2349957935", "name": "Zhuang Liu"}], "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(αx), as a dropin replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks."}
{"paperId": "1e794d1e979de5edb183866fed39ce34b5fa2f52", "url": "https://www.semanticscholar.org/paper/1e794d1e979de5edb183866fed39ce34b5fa2f52", "title": "Vision-to-Voice: AI for generating Description & Audio of Visual Content", "venue": "International research journal of innovations in engineering and technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.47001/irjiet/2025.iccis-202533?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.47001/irjiet/2025.iccis-202533, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2368001065", "name": "P. Jayanth"}, {"authorId": "2365102656", "name": "K. L. Sree"}, {"authorId": "2368005179", "name": "K. K. Kumar Reddy"}, {"authorId": "2368006880", "name": "G. O. Prakash"}, {"authorId": "2367824695", "name": "G. R. Prasad"}], "abstract": "Abstract - The seamless transformation of visual content into descriptive text and naturalistic speech, termed Vision-to-Voice, represents a significant interdisciplinary advancement at the intersection of computer vision, natural language processing (NLP), and speech synthesis. This paper explores the development of an end-to-end Vision-to-Voice pipeline, encompassing visual scene understanding, semantic description generation, and highquality speech synthesis, thereby enabling AI systems to narrate visual content for human users. The proposed methodology integrates Transformer-based image captioning models with context-aware linguistic augmentation and neural vocoders trained for expressive speech synthesis, ensuring fluent and expressive audio descriptions for visual content. While individual advancements in image captioning and TTS are well documented, their seamless fusion into an end-to-end, realtime system presents unique research and engineering challenges, including context preservation across modalities, maintaining linguistic fluency, and ensuring audio naturalness. This paper addresses these gaps through a unified encoder-decoder captioning module with Bahdanau Attention, followed by a Tacotron 2-based Melspectrogram generation module and HiFi-GAN-based waveform synthesis module. Extensive experimentation and evaluations using standard datasets, including Flickr8K and LJSpeech, demonstrate the efficacy of the proposed system in terms of caption quality (BLEU) and audio naturalness (MOS scores). The Vision-to-Voice system holds promising applications in assistive technologies, multimedia enrichment, and automated video annotation systems, thereby contributing to both academic research and real-world accessibility solutions."}
{"paperId": "1ea76ea5c1510a348bfa19264d1cf38015474809", "url": "https://www.semanticscholar.org/paper/1ea76ea5c1510a348bfa19264d1cf38015474809", "title": "Integrating Semantic Knowledge for Enhanced Weakly-Supervised Group Activity Recognition", "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APSIPAASC65261.2025.11249338?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APSIPAASC65261.2025.11249338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-22", "authors": [{"authorId": "153701906", "name": "Muhammad Adi Nugroho"}, {"authorId": "2281955282", "name": "Jinyoung Park"}, {"authorId": "2394999177", "name": "Yeeun Seong"}, {"authorId": "2276574578", "name": "Changick Kim"}], "abstract": "Large Vision-Language Models (LVLMs) have recently emerged as powerful tools for jointly modeling vision and language information, enhancing semantic representations. In this work, we introduce a method that utilizes high-level semantic knowledge of LVLM into the Weakly-Supervised Group Activity Recognition (WSGAR) task. Our Semantic Integrating Activity Recognition Network (SInAR-Net) enriches visual representations with language-based semantic information to construct complex spatio-temporal actor relationship. For efficient learning, we transfer the semantic knowledge via pre-extracted text features of contextual information generated through multiple prompt generations. Then, we construct multi-modal relationship between visual actor features and semantic text features using our semantic integration module. We ensure no drastic additional calculation cost by alleviating the use of LVLM in the inference stage, as our semantic integration module intrinsically learned the cross-modal semantic knowledge. Experiments on WSGAR benchmarks demonstrate competitive performance of our method, and ablation studies show the effectiveness of our novel semantic understanding components."}
{"paperId": "1ec23590206302428d9a22cec37af829f00764fe", "url": "https://www.semanticscholar.org/paper/1ec23590206302428d9a22cec37af829f00764fe", "title": "Vision-and-Language Navigation: A Comprehensive Review of Tasks, Methods, and Challenges", "venue": "Scientific Journal of Intelligent Systems Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54691/j1xcw420?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54691/j1xcw420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-29", "authors": [{"authorId": "2389503219", "name": "Anqi Song"}, {"authorId": "2389715356", "name": "Aobing Yin"}, {"authorId": "2389466602", "name": "Chenghe Kong"}, {"authorId": "2390306905", "name": "Yuhuan Xie"}], "abstract": "Vision-and-Language Navigation (VLN) is a core challenge in embodied AI, which aims to develop agents capable of understanding natural language instructions and navigating autonomously in visual environments. This survey systematically reviews the task paradigms and cutting-edge progress in the VLN field. We first propose a four-quadrant taxonomy based on environment, interaction, and instruction modality (Indoor, Outdoor, Interactive, and Multimodal-instruction Navigation), using this framework to deeply analyze the core characteristics, evaluation metrics, and technical challenges of various representative datasets. Furthermore, we provide a detailed review of mainstream technical methods, including classical modular paradigms, end-to-end learning (reinforcement learning and imitation learning), pre-training and transfer learning strategies, as well as advanced methods based on memory and graph structures, discussing their respective advantages, disadvantages, and applicable scenarios. Finally, we summarize the current challenges faced by the field, such as simulation-to-reality transfer, long-horizon planning, interactive reasoning, and embodied learning, and prospect future research directions. This survey aims to provide researchers with a clear technological panorama, promoting the development of VLN technology towards more general, robust, and practical applications."}
{"paperId": "1f037229be2a212543cfa04d8aea0a4c83b478d7", "url": "https://www.semanticscholar.org/paper/1f037229be2a212543cfa04d8aea0a4c83b478d7", "title": "Federated Multimodal Learning with Dual Adapters and Selective Pruning for Communication and Computational Efficiency", "venue": "IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-10", "authors": [{"authorId": "2256480959", "name": "Duy Phuong Nguyen"}, {"authorId": "2249845183", "name": "J. P. Muñoz"}, {"authorId": "2265579816", "name": "Tanya Roosta"}, {"authorId": "2266204425", "name": "Ali Jannesari"}], "abstract": "Federated Learning (FL) enables collaborative learning across distributed clients while preserving data privacy. However, FL faces significant challenges when dealing with heterogeneous data distributions, which can lead to suboptimal global models that fail to generalize across diverse clients. In this work, we propose a novel framework designed to tackle these challenges by introducing a dual-adapter approach. The method utilizes a larger local adapter for client-specific personalization and a smaller global adapter to facilitate efficient knowledge sharing across clients. Additionally, we incorporate a pruning mechanism to reduce communication overhead by selectively removing less impactful parameters from the local adapter. Through extensive experiments on a range of vision and language tasks, our method demonstrates superior performance compared to existing approaches. It achieves higher test accuracy, lower performance variance among clients, and improved worst-case performance, all while significantly reducing communication and computation costs. Overall, the proposed method addresses the critical trade-off between model personalization and generalization, offering a scalable solution for real-world FL applications."}
{"paperId": "202cc3ca2c97e97001cb1b3e4dfa34f3164e3232", "url": "https://www.semanticscholar.org/paper/202cc3ca2c97e97001cb1b3e4dfa34f3164e3232", "title": "Text-guided Generation of Efficient Personalized Inspection Plans", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.02917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-03", "authors": [{"authorId": "2276673092", "name": "Xingpeng Sun"}, {"authorId": "1831485", "name": "Zherong Pan"}, {"authorId": "2180685", "name": "Xifeng Gao"}, {"authorId": "2265617518", "name": "Kui Wu"}, {"authorId": "2248354403", "name": "Aniket Bera"}], "abstract": "We propose a training-free, Vision-Language Model (VLM)-guided approach for efficiently generating trajectories to facilitate target inspection planning based on text descriptions. Unlike existing Vision-and-Language Navigation (VLN) methods designed for general agents in unknown environments, our approach specifically targets the efficient inspection of known scenes, with widespread applications in fields such as medical, marine, and civil engineering. Leveraging VLMs, our method first extracts points of interest (POIs) from the text description, then identifies a set of waypoints from which POIs are both salient and align with the spatial constraints defined in the prompt. Next, we interact with the VLM to iteratively refine the trajectory, preserving the visibility and prominence of the POIs. Further, we solve a Traveling Salesman Problem (TSP) to find the most efficient visitation order that satisfies the order constraint implied in the text description. Finally, we apply trajectory optimization to generate smooth, executable inspection paths for aerial and underwater vehicles. We have evaluated our method across a series of both handcrafted and real-world scanned environments. The results demonstrate that our approach effectively generates inspection planning trajectories that adhere to user instructions."}
{"paperId": "20559af6ea4d959db8947acc19fcb7e11744020f", "url": "https://www.semanticscholar.org/paper/20559af6ea4d959db8947acc19fcb7e11744020f", "title": "Breaking the Brain: Adversarial Attacks and the Fragility of Modern AI Models", "venue": "2025 3rd World Conference on Communication & Computing (WCONF)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WCONF64849.2025.11233266?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WCONF64849.2025.11233266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-25", "authors": [{"authorId": "2253867394", "name": "Aparna Achanta"}], "abstract": "The rapid adoption of large-scale AI models—spanning computer vision, natural language processing (NLP), and multimodal reasoning—has significantly expanded their real-world influence, from autonomous systems to conversational agents. Their increasing integration into high-stakes domains has created substantial benefits in automation, decision-making, and content generation. However, the fragility of these models in the presence of adversarial inputs poses a profound threat to their reliability and safety. Even minimal perturbations, whether visual noise or syntactically benign text modifications, can lead to severe misclassifications or unsafe outputs. This paper presents a systematic study of the adversarial vulnerabilities of state-of-the-art AI models, including ViT-B/16, SAM, GPT-4, LLaMA 3, and GPT-4V. We evaluate the effectiveness of various attack strategies—such as diffusion-based image perturbations, prompt injection, and cross-modal inconsistency probes—and analyze their impact on model accuracy, alignment integrity, and semantic consistency. Furthermore, we benchmark the limitations of current defenses, including adversarial training, diffusion purification, and prompt hardening. Our findings demonstrate that robustness remains inconsistent and context-dependent, necessitating the development of dynamic, modality-aware defense mechanisms. This work contributes toward a more robust foundation for the deployment of secure and trustworthy AI systems."}
{"paperId": "2058d6f9f0ade1a242c3420f6f9ce395733aaac6", "url": "https://www.semanticscholar.org/paper/2058d6f9f0ade1a242c3420f6f9ce395733aaac6", "title": "Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-16", "authors": [{"authorId": "2146477013", "name": "Hanyu Duan"}, {"authorId": "2321877978", "name": "Yi Yang"}, {"authorId": "2267242940", "name": "Ahmed Abbasi"}, {"authorId": "1805674", "name": "K. Tam"}], "abstract": "This paper introduces Ready2Unlearn, a learning-time optimization approach designed to facilitate future unlearning processes. Unlike the majority of existing unlearning efforts that focus on designing unlearning algorithms, which are typically implemented reactively when an unlearning request is made during the model deployment phase, Ready2Unlearn shifts the focus to the training phase, adopting a\"forward-looking\"perspective. Building upon well-established meta-learning principles, Ready2Unlearn proactively trains machine learning models with unlearning readiness, such that they are well prepared and can handle future unlearning requests in a more efficient and principled manner. Ready2Unlearn is model-agnostic and compatible with any gradient ascent-based machine unlearning algorithms. We evaluate the method on both vision and language tasks under various unlearning settings, including class-wise unlearning and random data unlearning. Experimental results show that by incorporating such preparedness at training time, Ready2Unlearn produces an unlearning-ready model state, which offers several key advantages when future unlearning is required, including reduced unlearning time, improved retention of overall model capability, and enhanced resistance to the inadvertent recovery of forgotten data. We hope this work could inspire future efforts to explore more proactive strategies for equipping machine learning models with built-in readiness towards more reliable and principled machine unlearning."}
{"paperId": "213a03b9fc1c6e1dffd50998caeb4999f52931f5", "url": "https://www.semanticscholar.org/paper/213a03b9fc1c6e1dffd50998caeb4999f52931f5", "title": "A Comprehensive System for Real-Time Sign Language Translation into Text and Speech", "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem40428?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem40428, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-07", "authors": [{"authorId": "2339127911", "name": "Shashidhara H.V"}, {"authorId": "2259364038", "name": "J. R"}, {"authorId": "2339096226", "name": "Karthik V"}, {"authorId": "2339128663", "name": "Kiran I S"}, {"authorId": "2339127725", "name": "Hemanth I R"}], "abstract": "Effective communication remains a challenge for individuals who rely on sign language as their primary mode of expression, especially in interactions with non-sign language users. This research explores an innovative system that converts sign language gestures into text and subsequently into synthesized speech, enabling seamless and inclusive communication. Leveraging advancements in computer vision, natural language processing (NLP), and speech synthesis, the proposed model captures real-time sign gestures, translates them into structured textual data, and outputs audible speech with high accuracy.\n\nThe study delves into key technologies, including machine learning algorithms for gesture recognition, dynamic language modelling for text interpretation, and scalable speech synthesis techniques for voice output. This paper aims to provide a comprehensive framework addressing the linguistic and technical complexities of sign-to-text-to-speech conversion, emphasizing its potential impact on accessibility and societal integration for the deaf and hard-of-hearing communities."}
{"paperId": "21b88d065c403f54e172a92b01fd5f2ef56cdf45", "url": "https://www.semanticscholar.org/paper/21b88d065c403f54e172a92b01fd5f2ef56cdf45", "title": "Multimodal AI Architectures: Integrating Vision and Language for Enhanced Scene Understanding", "venue": "International Journal of Multidisciplinary Research in Science, Engineering and Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15680/ijmrset.2025.0805180?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15680/ijmrset.2025.0805180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-30", "authors": [{"authorId": "2364838864", "name": "AbdulHuq Mohammed"}], "abstract": "The integration of vision and language has emerged as a transformative frontier in artificial intelligence,\nenabling systems to achieve human-like comprehension of complex scenes by synthesizing multimodal data. This paper\nexplores cutting-edge advancements in multimodal architectures, focusing on their ability to bridge visual and linguistic\nmodalities for tasks such as visual question answering (VQA), image captioning, and cross-modal retrieval. A key\ninnovation lies in two-stage vision processing, where hierarchical visual features are preserved through intermediate\nlayer outputs and fused with language models via strategically placed cross-attention mechanisms. For instance, Meta’s\nMLLaMA employs a 32-layer vision encoder followed by an 8-layer global encoder with gated attention, concatenating\nmulti-scale features to enrich visual representations. Recent trends highlight the prominence of transformer-based\nframeworks and joint embedding spaces, as seen in models like CLIP and Flamingo, which leverage contrastive\nlearning to align text and image semantics. These architectures enable zero-shot generalization, outperforming taskspecific models in novel domains. Meanwhile, graph neural networks (GNNs) are gaining traction for modeling nonEuclidean relationships in multimodal data, particularly in medical imaging and robotics. Fusion techniques remain\ncentral to multimodal integration, with early, late, and hybrid approaches balancing computational efficiency and deep\nmodality interaction. Cross-modal attention mechanisms, as in the Meshed-Memory Transformer (\\(M^2\\)), enhance\nimage captioning by dynamically weighting visual and textual features."}
{"paperId": "224530aae71918c6d4b4e08772372a4ecd59f6e3", "url": "https://www.semanticscholar.org/paper/224530aae71918c6d4b4e08772372a4ecd59f6e3", "title": "Survey on Data-driven Intelligent Computing and Its Applications", "venue": "Frontiers in Computing and Intelligent Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54097/k0n97f44?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54097/k0n97f44, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-29", "authors": [{"authorId": "2327840287", "name": "Jiaqi Zhang"}, {"authorId": "2327996350", "name": "Dongpo Zhang"}, {"authorId": "2378192174", "name": "Xiaoqian Huang"}], "abstract": "In the era of big data with the rapid development of information technology, data presents the characteristics of massive, diverse and high-speed. Traditional computing paradigms face challenges such as low efficiency and poor adaptability when dealing with large-scale, high-dimensional and complex data, and data-driven intelligent computing emerges. With the help of machine learning, deep learning and other technologies, this computing mode can automatically learn knowledge and build models from data to realize intelligent analysis and decision-making of complex problems. Intelligent computing models include supervised learning, unsupervised learning and reinforcement learning. In practical applications, image recognition and computer vision, natural language processing, intelligent recommendation systems and other fields have achieved remarkable results, but they also face challenges such as data quality, model interpretability, computing resource requirements, and privacy security. Data-driven intelligent computing will develop in the direction of integration with knowledge graph, edge computing, cloud computing, and cross-domain application expansion, and continue to promote the intelligent process in various fields."}
{"paperId": "225b11a55cf3ff795c5fbf1ed5d235885348d85e", "url": "https://www.semanticscholar.org/paper/225b11a55cf3ff795c5fbf1ed5d235885348d85e", "title": "Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in MLLM", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.14735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2281259541", "name": "Chenkun Tan"}, {"authorId": "2278501965", "name": "Pengyu Wang"}, {"authorId": "2330229349", "name": "Shaojun Zhou"}, {"authorId": "2258680333", "name": "Botian Jiang"}, {"authorId": "2278452197", "name": "Zhaowei Li"}, {"authorId": "2109797247", "name": "Dong Zhang"}, {"authorId": "2157203756", "name": "Xinghao Wang"}, {"authorId": "2278545938", "name": "Yaqian Zhou"}, {"authorId": "2294872225", "name": "Xipeng Qiu"}], "abstract": "Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at https://github.com/fnlp-vision/DPA."}
{"paperId": "22823663525d8666d8f0dc8213315ee4b13ffd29", "url": "https://www.semanticscholar.org/paper/22823663525d8666d8f0dc8213315ee4b13ffd29", "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2313394505", "name": "Guiping Cao"}, {"authorId": "2363571288", "name": "Tao Wang"}, {"authorId": "67048823", "name": "Wen-Fong Huang"}, {"authorId": "2269303984", "name": "Xiangyuan Lan"}, {"authorId": "2313352493", "name": "Jianguo Zhang"}, {"authorId": "2262155110", "name": "Dongmei Jiang"}], "abstract": "Open-Ended object Detection (OED) is a novel and challenging task that detects objects and generates their category names in a free-form manner, without requiring additional vocabularies during inference. However, the existing OED models, such as GenerateU, require large-scale datasets for training, suffer from slow convergence, and exhibit limited performance. To address these issues, we present a novel and efficient Open-Det framework, consisting of four collaborative parts. Specifically, Open-Det accelerates model training in both the bounding box and object name generation process by reconstructing the Object Detector and the Object Name Generator. To bridge the semantic gap between Vision and Language modalities, we propose a Vision-Language Aligner with V-to-L and L-to-V alignment mechanisms, incorporating with the Prompts Distiller to transfer knowledge from the VLM into VL-prompts, enabling accurate object name generation for the LLM. In addition, we design a Masked Alignment Loss to eliminate contradictory supervision and introduce a Joint Loss to enhance classification, resulting in more efficient training. Compared to GenerateU, Open-Det, using only 1.5% of the training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs. 149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher performance (+1.0% in APr). The source codes are available at: https://github.com/Med-Process/Open-Det."}
{"paperId": "2293aa45357bf8db55b249d631f4d0749e585f7c", "url": "https://www.semanticscholar.org/paper/2293aa45357bf8db55b249d631f4d0749e585f7c", "title": "Role Of Artificial Intelligence and Open Access in Digital Libraries", "venue": "Journal of International Commercial Law and Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.61336/jiclt/25-01-29?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.61336/jiclt/25-01-29, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-21", "authors": [{"authorId": "2368376496", "name": "Muskaan Arora"}], "abstract": "With digital repositories becoming as crucial instruments for archiving and disseminating scholarly content, open access (OA) in academic publication has emerged as a key component in the democratization of knowledge. However, maintaining, finding, and effectively using these resources has become more difficult due to the exponential rise of information. Artificial intelligence (AI) provides creative answers to these problems, revolutionizing the operation of digital repositories and improving the sharing of knowledge. Accessibility, personalization, search and discovery, and metadata management are all being transformed by AI technologies. Semantic Scholar and Deep AI enhance search capabilities, while Google Dataset Search, Tagtog, and Data Cite Fabrica automate metadata creation and standardization. Modern digital archives are increasingly characterized by personalized user experiences that offer collaborative opportunities based on user activity. Chatbots driven by AI and Zotero help users locate materials and navigate repositories. AI is essential to overcoming issues like vision impairments, language hurdles, and other obstacles by making digital repositories more accessible and inclusive. However, it is necessary to address ethical issues including openness, data privacy, and prejudice reduction. The goal of universal access to knowledge is becoming more and more attainable with the adoption of AI-driven solutions, enabling scholars, educators, and students everywhere."}
{"paperId": "22a1720446be4a6a6a31c9e1a663c2aee3003c9d", "url": "https://www.semanticscholar.org/paper/22a1720446be4a6a6a31c9e1a663c2aee3003c9d", "title": "Exploring Data Modalities and Advances in Related AI Technologies for Oral Cancer Detection", "venue": "IET Image Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1049/ipr2.70223?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/ipr2.70223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2384802575", "name": "Sahil Sharma"}, {"authorId": "2384620272", "name": "Seema Wazarkar"}, {"authorId": "2384618793", "name": "Geeta Kasana"}], "abstract": "Oral cancer diagnosis represents a significant public health burden; late‐stage detection of oral cancer is a major issue for ineffective treatment. Multimodal approaches from artificial intelligence have emerged as a pretty promising approach to address this challenge. In this paper, a comprehensive review of recent studies of oral cancer detection across varied data modalities that utilise technologies such as computer vision, natural language processing, acoustics analysis, Internet of Things, and machine learning and Deep Learning (DL) is presented. Across the reviewed literature, unique datasets spanning imaging, histopathology, spectroscopy, and clinical text are identified and represented. Reported performance metrics vary by modality, such as image‐based DL methods, which achieved accuracies between 91% and 99% and area under the curve values up to 0.95, spectroscopy‐based approaches reported accuracies above 92%. These results highlight the diagnostic potential of varied data modalities for future research direction, and small, imbalanced datasets, lack of external validation, and personalisation are major concerns to be addressed."}
{"paperId": "22d851bf126dadbf2225142b72f0548a01b86072", "url": "https://www.semanticscholar.org/paper/22d851bf126dadbf2225142b72f0548a01b86072", "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2362288891", "name": "Yuanze Hu"}, {"authorId": "2358774942", "name": "Zhaoxin Fan"}, {"authorId": "2362696188", "name": "Xinyu Wang"}, {"authorId": "2362319987", "name": "Gen Li"}, {"authorId": "2362450906", "name": "Ye Qiu"}, {"authorId": "2362317840", "name": "Zhichao Yang"}, {"authorId": "2362263747", "name": "Wenjun Wu"}, {"authorId": "2352988316", "name": "Kejian Wu"}, {"authorId": "2362290272", "name": "Yifan Sun"}, {"authorId": "2365362846", "name": "Xiaotie Deng"}, {"authorId": "2362281949", "name": "Jin Dong"}], "abstract": "Lightweight Vision-Language Models (VLMs) are indispensable for resource-constrained applications. The prevailing approach to aligning vision and language models involves freezing both the vision encoder and the language model while training small connector modules. However, this strategy heavily depends on the intrinsic capabilities of the language model, which can be suboptimal for lightweight models with limited representational capacity. In this work, we investigate this alignment bottleneck through the lens of mutual information, demonstrating that the constrained capacity of the language model inherently limits the Effective Mutual Information (EMI) between multimodal inputs and outputs, thereby compromising alignment quality. To address this challenge, we propose TinyAlign, a novel framework inspired by Retrieval-Augmented Generation, which strategically retrieves relevant context from a memory bank to enrich multimodal inputs and enhance their alignment. Extensive empirical evaluations reveal that TinyAlign significantly reduces training loss, accelerates convergence, and enhances task performance. Remarkably, it allows models to achieve baseline-level performance with only 40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our work thus offers a practical pathway for developing more capable lightweight VLMs while introducing a fresh theoretical lens to better understand and address alignment bottlenecks in constrained multimodal systems."}
{"paperId": "22de2a66f3ccd7ce63cb7c686bd1aeb2dc9c312d", "url": "https://www.semanticscholar.org/paper/22de2a66f3ccd7ce63cb7c686bd1aeb2dc9c312d", "title": "cantnlp@DravidianLangTech2025: A Bag-of-Sounds Approach to Multimodal Hate Speech Detection", "venue": "Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-10", "authors": [{"authorId": "2349565272", "name": "Sidney Wong"}, {"authorId": "2349964108", "name": "Andrew Li"}], "abstract": "This paper presents the systems and results for the Multimodal Social Media Data Analysis in Dravidian Languages (MSMDA-DL) shared task at the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages (DravidianLangTech-2025). We took a `bag-of-sounds' approach by training our hate speech detection system on the speech (audio) data using transformed Mel spectrogram measures. While our candidate model performed poorly on the test set, our approach offered promising results during training and development for Malayalam and Tamil. With sufficient and well-balanced training data, our results show that it is feasible to use both text and speech (audio) data in the development of multimodal hate speech detection systems."}
{"paperId": "233fc7b102a89841d9251e66f931cac0f7c64784", "url": "https://www.semanticscholar.org/paper/233fc7b102a89841d9251e66f931cac0f7c64784", "title": "The Origin of Self-Attention: Pairwise Affinity Matrices in Feature Selection and the Emergence of Self-Attention", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.14560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-19", "authors": [{"authorId": "2373495265", "name": "Giorgio Roffo"}], "abstract": "The self-attention mechanism, now central to deep learning architectures such as Transformers, is a modern instance of a more general computational principle: learning and using pairwise affinity matrices to control how information flows through a model. This paper traces the conceptual origins of self-attention across multiple domains, including computer vision, natural language processing, and graph learning, through their shared reliance on an affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS) as a foundational approach that generalizes the idea of affinity-based weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS defines A either through domain knowledge or by learning, and computes feature relevance through multi-hop propagation over the affinity graph. From this perspective, self-attention can be seen as a special case of Inf-FS: it uses a single-hop affinity computation where A is dynamically built from token similarities. We argue that the underlying structure, reasoning over pairwise relationships, is preserved across both approaches, and the key differences lie in how the affinity matrix is defined and applied. By situating self-attention within the broader paradigm of affinity-based computation, we unify several strands of machine learning research and highlight a common mathematical foundation that underpins diverse models and tasks."}
{"paperId": "237bdef513fabb59e5de8654d1e3b80a0b422c8c", "url": "https://www.semanticscholar.org/paper/237bdef513fabb59e5de8654d1e3b80a0b422c8c", "title": "Look Around Before Locating: Considering Content and Structure Information for Visual Grounding", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i2.32158", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i2.32158?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i2.32158, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2293905897", "name": "Shiyi Zheng"}, {"authorId": "2223220272", "name": "Peizhi Zhao"}, {"authorId": "2258550721", "name": "Zhilong Zheng"}, {"authorId": "2355094877", "name": "Peihang He"}, {"authorId": "2321670770", "name": "Haonan Cheng"}, {"authorId": "2284411084", "name": "Yi Cai"}, {"authorId": "2111526601", "name": "Qingbao Huang"}], "abstract": "As a long-term challenge and fundamental requirement in vision and language tasks, visual grounding aims to localize a target referred by a natural language query. The regional annotations form a superficial correlation between the subject of expression and some common visual entities, which hinder models from comprehending the linguistic content and structure. However, current one-stage methods struggle to uniformly model the visual and linguistic structure due to the structural gap between continuous image patches and discrete text tokens. In this paper, we propose a semi-structured reasoning framework for visual grounding to gradually comprehend the linguistic content and structure. Specifically, we devise a cross-modal content alignment module to effectively align unlabeled contextual information into a stable semantic space corrected by token-level prior knowledge obtained with CLIP. A multi-branch modulated localization module is also established to obtain modulation grounding by linguistic structure. Through a soft split mechanism, our method can destructure the expression into a fixed semi-structure (i.e., subject and context) while ensuring the completeness of linguistic content. Our method is thus capable of building a semi-structured reasoning system to effectively comprehend the linguistic content and structure by content alignment and structure modulated grounding. Experimental results on five widely-used datasets validate the performance improvements of our proposed method."}
{"paperId": "2391902554e6beb697c7dcaf40ccf5bda6c1945f", "url": "https://www.semanticscholar.org/paper/2391902554e6beb697c7dcaf40ccf5bda6c1945f", "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.23362, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-07-31", "authors": [{"authorId": "2302475019", "name": "Ji Ma"}, {"authorId": "2134989055", "name": "Wei Suo"}, {"authorId": "2279282868", "name": "Peng Wang"}, {"authorId": "2290443425", "name": "Yanning Zhang"}], "abstract": "Although large vision-language models (LVLMs) have demonstrated impressive capabilities in multi-modal understanding and reasoning, their practical applications are still limited by massive model parameters and high computational costs. Recent efforts from natural language processing (NLP) have shown the effectiveness of layer pruning, offering a plausible training-free compression solution. However, due to the modality divergence between vision and language, it is unclear whether these NLP techniques are still effective in LVLMs. In this paper, we empirically prove that directly applying these layer pruning methods to LVLMs is ineffective. Through extensive experiments, we find that non-essential vision-language (VL) tokens and inter-layer feature gaps pose critical challenges to pruning layers in LVLMs. Based on these insights, we propose a novel framework Short-LVLM (SVL) that can utilize important VL tokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only achieves a superior trade-off between performance and efficiency but also exhibits several potential advantages,i.e., training-free, model-agnostic, and highly compatible. The code for this work is publicly available at https://github.com/ASGO-MM/Short-LVLM."}
{"paperId": "23df631c193a0ba95ddb7d5ab51fd399dc96eb5e", "url": "https://www.semanticscholar.org/paper/23df631c193a0ba95ddb7d5ab51fd399dc96eb5e", "title": "UMPA: Unified multi-modal prompt with adapter for vision-language models", "venue": "Multimedia Systems", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-01707-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-01707-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-21", "authors": [{"authorId": "2346805413", "name": "Zhengwei Jin"}, {"authorId": "2346938910", "name": "Yun Wei"}], "abstract": null}
{"paperId": "241d1578a996c9766da68db019ead1af0d2ed5e2", "url": "https://www.semanticscholar.org/paper/241d1578a996c9766da68db019ead1af0d2ed5e2", "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13338, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-17", "authors": [{"authorId": "2387941114", "name": "Yunze Leng"}, {"authorId": "2978590", "name": "Rohan Ghosh"}, {"authorId": "1770486", "name": "M. Motani"}], "abstract": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization."}
{"paperId": "244f6af9365dd151c86be3bfe1244e160788993f", "url": "https://www.semanticscholar.org/paper/244f6af9365dd151c86be3bfe1244e160788993f", "title": "Probabilistic Stability Guarantees for Feature Attributions", "venue": "arXiv.org", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-18", "authors": [{"authorId": "2322244644", "name": "Helen Jin"}, {"authorId": "2322095239", "name": "Anton Xue"}, {"authorId": "1667413380", "name": "Weiqiu You"}, {"authorId": "2301158773", "name": "Surbhi Goel"}, {"authorId": "2328413216", "name": "Eric Wong"}], "abstract": "Stability guarantees have emerged as a principled way to evaluate feature attributions, but existing certification methods rely on heavily smoothed classifiers and often produce conservative guarantees. To address these limitations, we introduce soft stability and propose a simple, model-agnostic, sample-efficient stability certification algorithm (SCA) that yields non-trivial and interpretable guarantees for any attribution method. Moreover, we show that mild smoothing achieves a more favorable trade-off between accuracy and stability, avoiding the aggressive compromises made in prior certification methods. To explain this behavior, we use Boolean function analysis to derive a novel characterization of stability under smoothing. We evaluate SCA on vision and language tasks and demonstrate the effectiveness of soft stability in measuring the robustness of explanation methods."}
{"paperId": "24658d26647e5a2123e7f3e0e71ede50cccc47d1", "url": "https://www.semanticscholar.org/paper/24658d26647e5a2123e7f3e0e71ede50cccc47d1", "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-03", "authors": [{"authorId": "2316486825", "name": "Vernon Toh"}, {"authorId": "2066312627", "name": "Yew Ken Chia"}, {"authorId": "32528506", "name": "Deepanway Ghosal"}, {"authorId": "1746416", "name": "Soujanya Poria"}], "abstract": "The releases of OpenAI's o-[n] series, such as o1, o3, and o4-mini, mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, models like o3 have demonstrated strong performance on benchmarks like the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models (including o1, o3, and o4-mini) on challenging multimodal puzzles from PuzzleVQA and AlgoPuzzleVQA, which demand fine-grained visual perception. Our results reveal that o-[n] series, particularly later iterations like o3 and o4-mini, significantly outperform the GPT-[n] series and show strong scalability in multimodal reasoning. Nonetheless, despite these substantial advancements and the superior capabilities demonstrated by the o-[n] series, our findings highlight that even these leading models face persistent challenges. Difficulties are particularly evident in tasks requiring precise visual perception, robust compositional reasoning across multiple visual attributes, and solving complex algorithmic or highly combinatorial puzzles, indicating critical areas for future AGI development. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available at https://github.com/declare-lab/LLM-PuzzleTest."}
{"paperId": "246ea2d7c1035975aa773cf9a0ed9d95b571fbd8", "url": "https://www.semanticscholar.org/paper/246ea2d7c1035975aa773cf9a0ed9d95b571fbd8", "title": "Action-Aware Visual-Textual Alignment for Long-Instruction Vision-and-Language Navigation", "venue": "ACM Trans. Multim. Comput. Commun. Appl.", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3748656?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3748656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-15", "authors": [{"authorId": "2280101795", "name": "Bowen Huang"}, {"authorId": "2157200309", "name": "Yanwei Zheng"}, {"authorId": "2279836263", "name": "Chuanlin Lan"}, {"authorId": "2372394416", "name": "Dongchen Sui"}, {"authorId": "29979215", "name": "Xinpeng Zhao"}, {"authorId": "2354175153", "name": "Xiao Zhang"}, {"authorId": "2239129392", "name": "Mengbai Xiao"}, {"authorId": "2239137290", "name": "Dongxiao Yu"}], "abstract": "Traditional Vision-and-Language Navigation (VLN) requires an agent to navigate to a target location solely based on visual observations, guided by natural language instructions. Compared to this task, long-instruction VLN involves longer instructions, extended trajectories, and the need to consider more contextual information for global path planning. As a result, it is more challenging and requires accurately aligning the instructions with the agent’s current visual observations, which is accompanied by two significant issues. Firstly, there is a misalignment between actions. The visual observations of the agent at each step lack explicit action-related details, while the instructions contain action-oriented words. Secondly, there is a misalignment between global instructions and local visual observations. The instructions describe the entire navigation trajectory, whereas the agent’s visual observations only provide localized information about a specific position along the trajectory. To address these issues, this article introduces the Action-Perception Alignment Framework (APAF). In this framework, we first design the Action-Contextual Encoding Module (ACEM), which enriches the agent’s visual perception by encoding potential actions with relative heading and elevation angles. We then propose the Dynamic Instruction Weighting Module (DIWM), which adjusts the importance of instruction words based on the agent’s current visual observations, emphasizing those words most relevant to the agent’s visual observations. Our approach significantly outperforms existing methods, achieving state-of-the-art results with improvements of 8.5% and 4.0% in Success Rate (SR) on the long-instruction R4R and RxR datasets, respectively."}
{"paperId": "255903df3cd7fc6fa8a0b306adfe3509e9025708", "url": "https://www.semanticscholar.org/paper/255903df3cd7fc6fa8a0b306adfe3509e9025708", "title": "SpatialGPT: Zero-Shot Vision-and-Language Navigation via Spatial CoT over Structured Spatial Memory", "venue": "Proceedings of the 33rd ACM International Conference on Advances in Geographic Information Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3748636.3762753?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3748636.3762753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2398043357", "name": "Zhiqiang Jiang"}, {"authorId": "2284521464", "name": "Xin Wang"}], "abstract": "Vision-and-Language Navigation (VLN) is a challenging multimodal task in which an autonomous agent must navigate unknown environments by following natural language instructions. Recent zero-shot VLN approaches leverage Large Language Models (LLMs), such as GPT, to interpret instructions and visual inputs for navigation inference without environment-specific training. However, these methods rely solely on the inherent spatial reasoning abilities of LLMs, which often fail to align panoramic observations with language instructions in zero-shot settings. To address this limitation, we propose SpatialGPT, a novel GPT-based VLN agent that incorporates spatial domain knowledge and the Chain-of-Thought (CoT) paradigm to enhance spatial reasoning. SpatialGPT integrates a Directional Connected Landmark List and a Spatial Knowledge Graph to jointly model local and global visual context as structured spatial memory. Built on this memory, we introduce a Synchronize-Align-Backtrack reasoning chain that synchronizes with instruction progress, aligns panoramic views to determine the next action, retrieves alternative paths or infers new frontiers during backtracking. Extensive experiments on the Room-to-Room (R2R) benchmark demonstrate that SpatialGPT achieves state-of-the-art zero-shot performance across all evaluation metrics, showcasing its enhanced spatial reasoning capabilities and strong generalization as an LLM-based VLN agent. The source code is available at SpatialGPT (GitHub)1."}
{"paperId": "2560c4cfbbdc625b07d4ede3734684a68ac1785e", "url": "https://www.semanticscholar.org/paper/2560c4cfbbdc625b07d4ede3734684a68ac1785e", "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective", "venue": "arXiv.org", "year": 2025, "citationCount": 32, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.01925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-02", "authors": [{"authorId": "2260345704", "name": "Yifan Zhong"}, {"authorId": "2216334065", "name": "Fengshuo Bai"}, {"authorId": "1993661033", "name": "Shaofei Cai"}, {"authorId": "2309173012", "name": "Xuchuan Huang"}, {"authorId": "2372780793", "name": "Zhang Chen"}, {"authorId": "2372419303", "name": "Xiaowei Zhang"}, {"authorId": "2373740667", "name": "Yuanfei Wang"}, {"authorId": "2372354017", "name": "Shaoyang Guo"}, {"authorId": "2372777135", "name": "Tianrui Guan"}, {"authorId": "2372679728", "name": "Ka Nam Lui"}, {"authorId": "2374351031", "name": "Zhiquan Qi"}, {"authorId": "2348218846", "name": "Yitao Liang"}, {"authorId": "2261728034", "name": "Yuanpei Chen"}, {"authorId": "2239164053", "name": "Yaodong Yang"}], "abstract": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \\textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence."}
{"paperId": "25725b51d5846eb3df12aad5d8d111b66caffac4", "url": "https://www.semanticscholar.org/paper/25725b51d5846eb3df12aad5d8d111b66caffac4", "title": "Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-10", "authors": [{"authorId": "2352941099", "name": "Shalini Maiti"}, {"authorId": "3377447", "name": "L. Agapito"}, {"authorId": "2283931239", "name": "Filippos Kokkinos"}], "abstract": "Rapid advancements in text-to-3D generation require robust and scalable evaluation metrics that align closely with human judgment, a need unmet by current metrics such as PSNR and CLIP, which require ground-truth data or focus only on prompt fidelity. To address this, we introduce Gen3DEval, a novel evaluation framework that leverages vision large language models (vLLMs) specifically fine-tuned for 3D object quality assessment. Gen3DEval evaluates text fidelity, appearance, and surface quality by analyzing 3D surface normals, without requiring ground-truth comparisons, bridging the gap between automated metrics and user preferences. Compared to state-of-the-art task-agnostic models, Gen3DEval demonstrates superior performance in user-aligned evaluations, placing it as a comprehensive and accessible benchmark for future research on text-to-3D generation. The project page can be found here: https://shalini-maiti.github.io/gen3deval.github.io/."}
{"paperId": "2594a5d94cdb26731302fb0b8695592bd1dec3fd", "url": "https://www.semanticscholar.org/paper/2594a5d94cdb26731302fb0b8695592bd1dec3fd", "title": "A vision–language foundation model for precision oncology", "venue": "Nature", "year": 2025, "citationCount": 126, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41586-024-08378-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41586-024-08378-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-08", "authors": [{"authorId": "50747083", "name": "Jinxi Xiang"}, {"authorId": "72541440", "name": "Xiyue Wang"}, {"authorId": "2339426691", "name": "Xiaoming Zhang"}, {"authorId": "2339328723", "name": "Yinghua Xi"}, {"authorId": "87545010", "name": "Feyisope R. Eweje"}, {"authorId": "2339423088", "name": "Yijiang Chen"}, {"authorId": "2335997101", "name": "Yuchen Li"}, {"authorId": "46351434", "name": "Colin P. Bergstrom"}, {"authorId": "2307131857", "name": "Matthew Gopaulchan"}, {"authorId": "2339437763", "name": "Ted Kim"}, {"authorId": "2256160770", "name": "Kun-Hsing Yu"}, {"authorId": "2321898075", "name": "Sierra Willens"}, {"authorId": "2339377373", "name": "F. Olguin"}, {"authorId": "2351041530", "name": "Jeffrey J Nirschl"}, {"authorId": "2238418190", "name": "J. Neal"}, {"authorId": "2272359485", "name": "Maximilian Diehn"}, {"authorId": "2308551679", "name": "Sen Yang"}, {"authorId": "2339541162", "name": "Ruijiang Li"}], "abstract": null}
{"paperId": "25984f671c083ef27b1502b480ad1965b4633192", "url": "https://www.semanticscholar.org/paper/25984f671c083ef27b1502b480ad1965b4633192", "title": "A Review on Meta-Learning: How Artificial Intelligence and Machine Learning Can Learn to Adapt Quickly", "venue": "2025 International Conference on Electronics and Renewable Systems (ICEARS)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICEARS64219.2025.10941123?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICEARS64219.2025.10941123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference", "Review"], "publicationDate": "2025-02-11", "authors": [{"authorId": "2353256125", "name": "Shrikant Khadse"}, {"authorId": "70297813", "name": "Palash M. Gourshettiwar"}, {"authorId": "2341402591", "name": "Adesh Pawar"}], "abstract": "Meta-learning aims to create Artificial Intelligence (AI) systems that can adapt to new tasks and improve their performance over time without extensive retraining. The advent of meta-learning paradigms has fundamentally changed how Artificial Intelligence (AI) and Machine Learning (ML) are handled, constantly reshaping these fields. Research study reviews the broad field of meta-learning, highlighting its critical function in bridging the AI-ML divide to improve adaptability. The study starts with a review of past developments, following the development of meta-learning algorithms and their underpinning theories. The historical view lays the groundwork for understanding the course of this dynamic regulation, spanning from early efforts in probabilistic program induction to modern discoveries in model-agnostic meta-learning. The study explores multiple meta-learning applications in AI and ML, revealing how they affect financial forecasting, computer vision, natural language processing, autonomous cars, health informatics, robotics, and personalized recommendation systems. When seen from the perspective of adaptive intelligence, meta-learning may be effectively applied to conditions with limited data, hyperparameter optimization, few-shot learning, and quick adaption in reinforcement learning settings."}
{"paperId": "263b9ec234d0e8c3237e583df136bfdc08fde9fc", "url": "https://www.semanticscholar.org/paper/263b9ec234d0e8c3237e583df136bfdc08fde9fc", "title": "LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-16", "authors": [{"authorId": "2101805", "name": "G. Vessio"}], "abstract": "Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA."}
{"paperId": "26566163ea9d16ff337a4e8cf5ab07e0362ff8b4", "url": "https://www.semanticscholar.org/paper/26566163ea9d16ff337a4e8cf5ab07e0362ff8b4", "title": "An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.15531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-17", "authors": [{"authorId": null, "name": "Joao Daniel Silva"}, {"authorId": "2283772665", "name": "João Magalhães"}, {"authorId": "2977931", "name": "D. Tuia"}, {"authorId": "2283771934", "name": "Bruno Martins"}], "abstract": "The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach."}
{"paperId": "26bbf4ebff0fa00271a2ac892bfc305d44586ee6", "url": "https://www.semanticscholar.org/paper/26bbf4ebff0fa00271a2ac892bfc305d44586ee6", "title": "General Scene Adaptation for Vision-and-Language Navigation", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.17403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-29", "authors": [{"authorId": "2305123085", "name": "Haodong Hong"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2257126395", "name": "Sen Wang"}, {"authorId": "2330744710", "name": "Jiajun Liu"}, {"authorId": "2304606995", "name": "Qi Wu"}], "abstract": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits."}
{"paperId": "2723efa83cf8b01334cb6d9e11683d05ff15f297", "url": "https://www.semanticscholar.org/paper/2723efa83cf8b01334cb6d9e11683d05ff15f297", "title": "NaLaFormer: Norm-Aware Linear Attention for Transformer Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.21137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-26", "authors": [{"authorId": "2342418284", "name": "Weikang Meng"}, {"authorId": "2316154141", "name": "Yadan Luo"}, {"authorId": "2371073362", "name": "Liangyu Huo"}, {"authorId": "2302160627", "name": "Yaowei Wang"}, {"authorId": "2342504843", "name": "Xin Li"}, {"authorId": "2336213360", "name": "Zheng Zhang"}], "abstract": "Linear attention has emerged as a viable alternative to softmax attention by reducing complexity from quadratic to linear in sequence length. To preserve two fundamental properties of softmax, non-negativity and entropy reduction, current works employ various linearly separatable kernel functions with $L1$ normalization instead of softmax operator. However, query norms are neglected by the normalization operation in linear attention, such degradation heavily leads to an entropy gap. Meanwhile, existing works inhibit negative values of query and key vectors resulting in a missing inner-product interactions after being mapped. To address these dual challenges, we propose a novel Norm-Aware Linear Attention mechanism serving to restore norm-guided dynamic spikiness and recover kernel-perturbed norm distributions. Specifically, we first decouple query and key matrices into two components: norm and direction, to achieve norm-aware spikiness control and norm consistency, respectively. We mathematically reveal that the extent of entropy reduction varies with the query norm in softmax normalization, motivating a query-norm aware kernel function for dynamic control over entropy reduction. Furthermore, to ensure norm consistency and enforce non-negativity constraints, we employ a norm-preserving mapping to project all elements of the angular matrix into positive values, leveraging cosine similarity to inhibit dimensions with opposite directions. We conduct extensive experiments demonstrating that the NaLaFormer improves performance on vision and language tasks, enhancing both expressiveness and efficiency by up to 4.2\\%."}
{"paperId": "27360088b344f13c917c230eac0546ab4e0ec140", "url": "https://www.semanticscholar.org/paper/27360088b344f13c917c230eac0546ab4e0ec140", "title": "A Vision-Language–Guided Multimodal Fusion Network for Glottic Carcinoma Early Diagnosis: Model Development and Validation Study", "venue": "JMIR Medical Informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12507326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-08", "authors": [{"authorId": "2338804563", "name": "Zhaohui Jin"}, {"authorId": "2336953553", "name": "Yi Shuai"}, {"authorId": "2305032754", "name": "Yun Li"}, {"authorId": "2384803357", "name": "Mianmian Chen"}, {"authorId": "2384065259", "name": "Yumeng Liu"}, {"authorId": "2238928153", "name": "W. Lei"}, {"authorId": "2315985080", "name": "Xiaomao Fan"}], "abstract": "Abstract Background Early diagnosis and intervention in glottic carcinoma (GC) can significantly improve long-term prognosis. However, the accurate diagnosis of early GC is challenging due to its morphological similarity to vocal cord dysplasia, with the difficulty further exacerbated in medically underserved areas. Objective This study aims to address the limitations of existing technologies by designing a vision-language multimodal model, providing a more efficient and accurate early diagnostic method for GC. Methods The data used in this study were sourced from the information system of the First Affiliated Hospital of Sun Yat-sen University, comprising laryngoscopy reports and 5796 laryngoscopic images from 404 patients with glottic lesions. We propose a vision-language–guided multimodal fusion network (VLMF-Net) based on a large vision-language model for the early automated diagnosis of GC. The text processing module of this model uses the pretrained Large Language Model Meta AI (LLaMa) to generate text vector representations, while the image processing module uses a pretrained vision transformer to extract features from laryngoscopic images, achieving cross-modal alignment through the Q-Former module. By leveraging a feature fusion module, deep integration of text and image features is achieved, ultimately enabling classification diagnosis. To validate the model’s performance, the study selected contrastive language-image pretraining (CLIP), bootstrapping language-image pretraining with frozen image encoders and large language models (BLIP-2), a large-scale image and noisy-text embedding (ALIGN), and vision-and-language transformer (VILT) as baseline methods for experimental evaluation on the same dataset, with comprehensive performance assessment conducted using accuracy, recall, precision, F1-score, and area under the curve. Results We found that on the internal test set, the VLMF-Net model significantly outperformed existing methods with an accuracy of 77.6% (CLIP: 70.5%; BLIP-2: 71.5%; ALIGN: 67.3%; and VILT: 64.3%), achieving a 6.1-percentage point improvement over the best baseline model (BLIP-2). On the external test set, our method also demonstrated robust performance, achieving an accuracy of 73.9%, which is 4.6 percentage points higher than the second-best model (BLIP-2: 69.3%). This indicates that our model surpasses these methods in the early diagnosis of GC and exhibits strong generalization ability and robustness. Conclusions The proposed VLMF-Net model can be effectively used for the early diagnosis of GC, helping to address the challenges in its early detection."}
{"paperId": "27943200429d34dd0f1fe804c1f12a572de867d2", "url": "https://www.semanticscholar.org/paper/27943200429d34dd0f1fe804c1f12a572de867d2", "title": "Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.03207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-05", "authors": [{"authorId": "2238209678", "name": "Ting Lei"}, {"authorId": "2296362308", "name": "Shaofeng Yin"}, {"authorId": "8559994", "name": "Qingchao Chen"}, {"authorId": "2175354364", "name": "Yuxin Peng"}, {"authorId": "2295761978", "name": "Yang Liu"}], "abstract": "Open Vocabulary Human-Object Interaction (HOI) detection aims to detect interactions between humans and objects while generalizing to novel interaction classes beyond the training set. Current methods often rely on Vision and Language Models (VLMs) but face challenges due to suboptimal image encoders, as image-level pre-training does not align well with the fine-grained region-level interaction detection required for HOI. Additionally, effectively encoding textual descriptions of visual appearances remains difficult, limiting the model's ability to capture detailed HOI relationships. To address these issues, we propose INteraction-aware Prompting with Concept Calibration (INP-CC), an end-to-end open-vocabulary HOI detector that integrates interaction-aware prompts and concept calibration. Specifically, we propose an interaction-aware prompt generator that dynamically generates a compact set of prompts based on the input scene, enabling selective sharing among similar interactions. This approach directs the model's attention to key interaction patterns rather than generic image-level semantics, enhancing HOI detection. Furthermore, we refine HOI concept representations through language model-guided calibration, which helps distinguish diverse HOI concepts by investigating visual similarities across categories. A negative sampling strategy is also employed to improve inter-modal similarity modeling, enabling the model to better differentiate visually similar but semantically distinct actions. Extensive experimental results demonstrate that INP-CC significantly outperforms state-of-the-art models on the SWIG-HOI and HICO-DET datasets. Code is available at https://github.com/ltttpku/INP-CC."}
{"paperId": "2885ee24025446678e44ab1eea2f6d59c5329a32", "url": "https://www.semanticscholar.org/paper/2885ee24025446678e44ab1eea2f6d59c5329a32", "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-24", "authors": [{"authorId": "2329525526", "name": "Shufan Shen"}, {"authorId": "2313588221", "name": "Junshu Sun"}, {"authorId": "2256640224", "name": "Qingming Huang"}, {"authorId": "2319173985", "name": "Shuhui Wang"}], "abstract": "The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE."}
{"paperId": "28a907de09db656ff82d1c2b18bbce4e39a2c6bb", "url": "https://www.semanticscholar.org/paper/28a907de09db656ff82d1c2b18bbce4e39a2c6bb", "title": "NEXUS-O: An Omni-Perceptive and -Interactive Model for Language, Audio, and Vision", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2329310647", "name": "Che Liu"}, {"authorId": "2108307966", "name": "Yingji Zhang"}, {"authorId": "2329545183", "name": "Dong Zhang"}, {"authorId": "2348409645", "name": "Weijie Zhang"}, {"authorId": "2348611651", "name": "Chenggong Gong"}, {"authorId": "2329746274", "name": "Haohan Li"}, {"authorId": "2329393780", "name": "Yu Lu"}, {"authorId": "2348694968", "name": "Shilin Zhou"}, {"authorId": "2329393780", "name": "Yu Lu"}, {"authorId": "2329188802", "name": "Ziliang Gan"}, {"authorId": "2348418818", "name": "Ziao Wang"}, {"authorId": "2349230529", "name": "Junwei Liao"}, {"authorId": "2329609481", "name": "Haipang Wu"}, {"authorId": "2329309171", "name": "Ji Liu"}, {"authorId": "2301583069", "name": "Andr'e Freitas"}, {"authorId": "2238904920", "name": "Qifan Wang"}, {"authorId": "2329231939", "name": "Zenglin Xu"}, {"authorId": "2329217550", "name": "Rongjunchen Zhang"}, {"authorId": "2330178903", "name": "Yong Dai"}], "abstract": "Human beings perceive the real world through a spectrum of sensory modalities, encompassing auditory, visual, and linguistic faculties. This work proposes an industry-level omni-modal large language model (LLM) pipeline that integrates auditory, visual, and linguistic modalities to overcome challenges such as limited tri-modal datasets, high computational costs, and complex feature alignments. Our pipeline consists of three main components: First, a modular, end-to-end framework enabling flexible configuration of various encoder-LLM-decoder architectures. Second, a lightweight training strategy that pre-trains audio-language alignment on the state-of-the-art vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of vision-specific modalities. Third, an audio synthesis pipeline that generates high-quality audio-text data from diverse real-world scenarios, supporting applications such as Automatic Speech Recognition and Speech-to-Speech chat. To this end, we introduce an industry-level omni-modal LLM, NEXUS-O. Extensive experiments validate the efficacy of our pipeline, yielding the following key findings: (1) In the visual understanding task, NEXUSO exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, validating the efficiency of our training strategy. (2) Within the English Spoken Question-Answering task, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In our realworld ASR testset, NEXUS-O achieves outstanding performance, indicating its robustness in real scenarios. (4) In the Speech-to-Text Translation task, our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task, based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), NEXUS-O is comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment between vision and language."}
{"paperId": "28ef6c02820b95e3f44602ed9dfa38bd8b94f971", "url": "https://www.semanticscholar.org/paper/28ef6c02820b95e3f44602ed9dfa38bd8b94f971", "title": "Parameter-efficient action planning with large language models for vision-and-language navigation", "venue": "Pattern Recognition", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2025.112462?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2025.112462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "81976406", "name": "Bahram Mohammadi"}, {"authorId": "8088602", "name": "Ehsan Abbasnejad"}, {"authorId": "2290976660", "name": "Yuankai Qi"}, {"authorId": "2261450229", "name": "Qi Wu"}, {"authorId": "133678193", "name": "A. van den Hengel"}, {"authorId": "2286467690", "name": "J. Shi"}], "abstract": null}
{"paperId": "29023501b10d0fcfee00c2cbca32d8a0738bef10", "url": "https://www.semanticscholar.org/paper/29023501b10d0fcfee00c2cbca32d8a0738bef10", "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-27", "authors": [{"authorId": "2361028019", "name": "Hangwei Zhang"}, {"authorId": "2383871110", "name": "Chun Kang"}, {"authorId": "2360882257", "name": "Yan Wang"}, {"authorId": "2382918811", "name": "Difan Zou"}], "abstract": "Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for complex downstream tasks has proven effective in vision and language processing, yet this paradigm remains unexplored in scientific machine learning, where the objective is to model complex physical systems. We conduct the first systematic study of PEFT for pre-trained Large Operator Models (LOMs) obtained by scaling variants of Fourier Neural Operator. First, we observe that the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance on LOMs than Adapter tuning. Then, we further theoretically establish that stacked LoRA incurs a depth-amplified lower bound on approximation error within Fourier layers, whereas adapters retain universal approximation capacity and, by concentrating parameters on energy-dominant low-frequency modes, attain exponentially decaying error with bottleneck width in the Fourier domain. Motivated by the robust empirical gains of adapters and by our theoretical characterization of PDE solutions as spectrally sparse, we introduce Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity based on spectral complexity, assigning higher-dimension modules to low-frequency components and lower-dimension modules to high-frequency components. Our F-Adapters establish state-of-the-art (SOTA) results on multiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both generalization and spectral fidelity over LoRA and other PEFT techniques commonly used in LLMs. To the best of our knowledge, this work is the first to explore PEFT for scientific machine-learning and establishes F-Adapter as an effective paradigm for this domain."}
{"paperId": "291db5ed7791ab86ac92674bbd87bc725bbd8e9c", "url": "https://www.semanticscholar.org/paper/291db5ed7791ab86ac92674bbd87bc725bbd8e9c", "title": "UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-27", "authors": [{"authorId": "2306933375", "name": "Hoang-Bao Le"}, {"authorId": "2351604119", "name": "Allie Tran"}, {"authorId": "2159543846", "name": "Binh T. Nguyen"}, {"authorId": "1829790", "name": "Liting Zhou"}, {"authorId": "1737981", "name": "C. Gurrin"}], "abstract": "Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types."}
{"paperId": "292261a561e190746ec517254de89cbca9825a32", "url": "https://www.semanticscholar.org/paper/292261a561e190746ec517254de89cbca9825a32", "title": "VITRIX-CLIPIN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.02329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-08-04", "authors": [{"authorId": "2374680407", "name": "Ziteng Wang"}, {"authorId": "2353312145", "name": "Siqi Yang"}, {"authorId": "2276613199", "name": "Limeng Qiao"}, {"authorId": "2373862784", "name": "Lin Ma"}], "abstract": "Despite the success of Vision-Language Models (VLMs) like CLIP in aligning vision and language, their proficiency in detailed, fine-grained visual comprehension remains a key challenge. We present CLIP-IN, a novel framework that bolsters CLIP's fine-grained perception through two core innovations. Firstly, we leverage instruction-editing datasets, originally designed for image manipulation, as a unique source of hard negative image-text pairs. Coupled with a symmetric hard negative contrastive loss, this enables the model to effectively distinguish subtle visual-semantic differences. Secondly, CLIP-IN incorporates long descriptive captions, utilizing rotary positional encodings to capture rich semantic context often missed by standard CLIP. Our experiments demonstrate that CLIP-IN achieves substantial gains on the MMVP benchmark and various fine-grained visual recognition tasks, without compromising robust zero-shot performance on broader classification and retrieval tasks. Critically, integrating CLIP-IN's visual representations into Multimodal Large Language Models significantly reduces visual hallucinations and enhances reasoning abilities. This work underscores the considerable potential of synergizing targeted, instruction-based contrastive learning with comprehensive descriptive information to elevate the fine-grained understanding of VLMs."}
{"paperId": "293387c97350a9798bf9fddcffaf903a27662095", "url": "https://www.semanticscholar.org/paper/293387c97350a9798bf9fddcffaf903a27662095", "title": "Conditional Diffusion Models: A Survey of Techniques, Applications, and Challenges", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3625094?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3625094, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2282525810", "name": "T. Panagiotakopoulos"}, {"authorId": "1697867", "name": "S. Kotsiantis"}, {"authorId": "2000694253", "name": "A. Gkillas"}, {"authorId": "46599973", "name": "Aris S. Lalos"}], "abstract": "Conditional diffusion models (CDMs) are an emerging family of generative models that enable controllable, data-driven generation across a wide range of modalities. Through conditioning of the generation process on side information such as labels, signals or physical constraints, CDMs promote structured generation, guided sampling and stable inference in settings where complexity, uncertainty and limited data are prevalent. This article provides a complete overview of recent advancements in CDM architectures and, we explore their applications in several areas like computer vision, natural language processing, mechanics, healthcare. Furthermore, we investigate the significant challenges of CDMs. We lastly identify emerging directions and open research problems, with the goal of providing a reference paper for researchers and practitioners engaged in the theory and application of conditional diffusion models."}
{"paperId": "29a18cc958015e287b957e2aaf1a2b244f905c09", "url": "https://www.semanticscholar.org/paper/29a18cc958015e287b957e2aaf1a2b244f905c09", "title": "DiMPLe - Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.21237, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-26", "authors": [{"authorId": "2310606804", "name": "Umaima Rahman"}, {"authorId": "2293312376", "name": "Mohammad Yaqub"}, {"authorId": "2291135952", "name": "Dwarikanath Mahapatra"}], "abstract": "We introduce DiMPLe (Disentangled Multi-Modal Prompt Learning), a novel approach to disentangle invariant and spurious features across vision and language modalities in multi-modal learning. Spurious correlations in visual data often hinder out-of-distribution (OOD) performance. Unlike prior methods focusing solely on image features, DiMPLe disentangles features within and across modalities while maintaining consistent alignment, enabling better generalization to novel classes and robustness to distribution shifts. Our method combines three key objectives: (1) mutual information minimization between invariant and spurious features, (2) spurious feature regularization, and (3) contrastive learning on invariant features. Extensive experiments demonstrate DiMPLe demonstrates superior performance compared to CoOp-OOD, when averaged across 11 diverse datasets, and achieves absolute gains of 15.27 in base class accuracy and 44.31 in novel class accuracy."}
{"paperId": "29f7d7a16297ca32a2988efaf73e773b4628a3c4", "url": "https://www.semanticscholar.org/paper/29f7d7a16297ca32a2988efaf73e773b4628a3c4", "title": "Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-22", "authors": [{"authorId": "2395971660", "name": "Dachuan Zhao"}, {"authorId": "2394096198", "name": "Weiyue Li"}, {"authorId": "2394986896", "name": "Zhenda Shen"}, {"authorId": null, "name": "Yushu Qiu"}, {"authorId": "2392472445", "name": "Bowen Xu"}, {"authorId": "2395515869", "name": "Haoyu Chen"}, {"authorId": "2394127996", "name": "Yongchao Chen"}], "abstract": "Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\\textbf{S}$ubspace $\\textbf{P}$rojection $\\textbf{D}$ebiasing ($\\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline."}
{"paperId": "2a127b87ab422b85f426fefb75ee4c7bae470f06", "url": "https://www.semanticscholar.org/paper/2a127b87ab422b85f426fefb75ee4c7bae470f06", "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-19", "authors": [{"authorId": "2304629163", "name": "Thuy Phuong Vu"}, {"authorId": "71649792", "name": "Dinh-Cuong Hoang"}, {"authorId": "2319738073", "name": "M. Le"}, {"authorId": "79397918", "name": "Phan Xuan Tan"}], "abstract": "Recent research has made significant progress in localizing and editing image regions based on text. However, most approaches treat these regions in isolation, relying solely on local cues without accounting for how each part contributes to the overall visual and semantic composition. This often results in inconsistent edits, unnatural transitions, or loss of coherence across the image. In this work, we propose Region in Context, a novel framework for text-conditioned image editing that performs multilevel semantic alignment between vision and language, inspired by the human ability to reason about edits in relation to the whole scene. Our method encourages each region to understand its role within the global image context, enabling precise and harmonized changes. At its core, the framework introduces a dual-level guidance mechanism: regions are represented with full-image context and aligned with detailed region-level descriptions, while the entire image is simultaneously matched to a comprehensive scene-level description generated by a large vision-language model. These descriptions serve as explicit verbal references of the intended content, guiding both local modifications and global structure. Experiments show that it produces more coherent and instruction-aligned results. Code is available at: https://github.com/thuyvuphuong/Region-in-Context.git"}
{"paperId": "2b26edd4c46d5a5d839764dc988d045b48913c7a", "url": "https://www.semanticscholar.org/paper/2b26edd4c46d5a5d839764dc988d045b48913c7a", "title": "Seeing speech: Neural mechanisms of cued speech perception in prelingually deaf and hearing users", "venue": "Imaging neuroscience", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12319768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-24", "authors": [{"authorId": "2098392491", "name": "Annahita Sarré"}, {"authorId": "2243421111", "name": "Laurent Cohen"}], "abstract": "Abstract For many deaf people, lip-reading plays a major role in verbal communication. However, lip movements are by nature ambiguous, so that lip-reading does not allow for a full understanding of speech. The resulting language access difficulties may have serious consequences on language, cognitive and social development. Cued speech (CS) was developed to eliminate this ambiguity by complementing lip-reading with hand gestures, giving access to the entire phonological content of speech through the visual modality alone. Despite its proven efficiency for improving linguistic and communicative abilities, the mechanisms of CS perception remain largely unknown. The goal of the present study is to delineate the brain regions involved in CS perception and identify their role in visual and language-related processes. Three matched groups of participants were scanned during the presentation of videos of silent CS sentences, isolated lip movements, isolated gestures, plus CS sentences with speech sounds, and meaningless CS sentences: Prelingually deaf users of CS, hearing users of CS, and naïve hearing controls. We delineated a number of mostly left-hemisphere brain regions involved in CS perception. We first found that language areas were activated in all groups by both silent CS sentences and isolated lip movements, and by gestures in deaf participants only. Despite overlapping activations when perceiving CS, several findings differentiated experts from novices. The Visual Word Form Area, which supports the interface between vision and language during reading, was activated by isolated gestures in deaf CS users. In contrast, the Bayes factor indicated either weak evidence of no activation or negligible evidence of activation in hearing and control groups. Moreover, the integration of lip movements and gestures took place in a temporal language-related region in deaf users, and in movement-related regions in hearing users, reflecting their different profile of expertise in CS comprehension and production. Finally, we observed a strong involvement of the Dorsal Attentional Network in hearing users of CS, and identified the neural correlates of the variability in individual proficiency. Cued speech constitutes a novel pathway for accessing core language processes, halfway between speech perception and reading. The current study provides a delineation of the common and specific brain structures supporting those different modalities of language input, paving the way for further research."}
{"paperId": "2b5491652003e0b4ff56b20573a7944c8c2e0ebc", "url": "https://www.semanticscholar.org/paper/2b5491652003e0b4ff56b20573a7944c8c2e0ebc", "title": "MM-GRADE: A Multi-Modal EDA Tool Documentation QA Framework Leveraging Retrieval Augmented Generation", "venue": "2025 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCAD66269.2025.11240857?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCAD66269.2025.11240857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-26", "authors": [{"authorId": "2220363044", "name": "Yuan Pu"}, {"authorId": "8822971", "name": "Zhuolun He"}, {"authorId": "2393430464", "name": "Shutong Lin"}, {"authorId": null, "name": "Jiajun Qin"}, {"authorId": "2155544084", "name": "Xinyun Zhang"}, {"authorId": "2394028431", "name": "Hairuo Han"}, {"authorId": "67219756", "name": "Haisheng Zheng"}, {"authorId": "2311458517", "name": "Yuqi Jiang"}, {"authorId": "2302591978", "name": "Cheng Zhuo"}, {"authorId": "2306912820", "name": "Qi Sun"}, {"authorId": "2393457250", "name": "David Pan"}, {"authorId": "2269025620", "name": "Bei Yu"}], "abstract": "The complexity of EDA tools necessitates the development of advanced documentation query answering systems to enhance user efficiency and reduce the associated learning curve. Recent innovations in the use of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for EDA tool documentation have demonstrated significant progress; however, these approaches typically lack the multi-modal capabilities required to effectively handle visual data, such as circuit layout and GUI screenshots provided through user input. To address the concern, we introduce a multi-modal RAG system that incorporates two domain-customized modules: a multi-modal retriever model finetuned by the customized bilevel hard negative mining (BHNM) strategy, and a vision large language model (VLLM) finetuned using a tailored extract-score-answer pipeline. Moreover, we have manually curated ORD-MMBench, a multi-modal QA benchmark comprising 120 high-quality question-document-answer triplets based on OpenROAD documentation. Experimental results demonstrate that our customized RAG framework outperforms state-of-the-art multi-modal RAG flows and models on ORD-MMBench."}
{"paperId": "2bf123a3d9606aae3827f52b8261442af669e73a", "url": "https://www.semanticscholar.org/paper/2bf123a3d9606aae3827f52b8261442af669e73a", "title": "Source-Free Elastic Model Adaptation for Vision-and-Language Navigation", "venue": "IEEE transactions on multimedia", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2025.3535356?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2025.3535356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2257488640", "name": "Mingkui Tan"}, {"authorId": "2158502526", "name": "Peihao Chen"}, {"authorId": "2231606704", "name": "Hongyan Zhi"}, {"authorId": "2300659834", "name": "Jiajie Mai"}, {"authorId": "2309173092", "name": "Benjamin Rosman"}, {"authorId": "2187874507", "name": "Dongyu Ji"}, {"authorId": "147992804", "name": "Runhao Zeng"}], "abstract": "Vision-and-Language Navigation (VLN) requires an agent to follow given instructions to navigate. Despite the significant progress, the model trained on seen environments has a performance drop on unseen environments due to distribution shift. To improve the generalization, existing method attempts to apply test-time adaptation to VLN. However, it needs to access the training data and all testing data for updating the model before inference. The setting is not suitable for the real application because it is hard for the agent to access training data and all testing data when the agent is applied in a new environment. In this paper, we consider a more practical setting with source-free and online-inference test-time adaption. In other words, the model can only access one testing sample for test-time adaptation. In this setting, the model may suffer from catastrophic forgetting of the learned knowledge and unstable parameter update issues. To solve these challenges, we propose an elastic adaptation model (EAM) that consists of an auxiliary decision model and a sample replay mechanism. We use the online testing samples to adapt the auxiliary decision model to new environments, which cooperates with the frozen original model to make better action decisions. The sample replay mechanism stores the historical testing samples to make the adaptation process more stable. Our method is model-agnostic and is effortless to be applied to most existing methods. Experimental results show that our method achieves stable performance improvement based on three existing methods on three VLN benchmark datasets."}
{"paperId": "2bf34d5ba1607d119ce64317f1280f98103a273d", "url": "https://www.semanticscholar.org/paper/2bf34d5ba1607d119ce64317f1280f98103a273d", "title": "Possibilities of Using Artificial Intelligence Technologies in Prehospital Trauma Care", "venue": "Russian Sklifosovsky Journal \"Emergency Medical Care\"", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23934/2223-9022-2025-14-3-609-618?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23934/2223-9022-2025-14-3-609-618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-24", "authors": [{"authorId": "51436322", "name": "P. A. Seliverstov"}, {"authorId": "65967576", "name": "Y. Shapkin"}, {"authorId": "82978298", "name": "N. Y. Stekolnikov"}], "abstract": "The use of artificial intelligence in the prehospital stage of trauma care is feasible and has great potential. Artificial intelligence technologies can reduce the time of emergency medical care, make objective decisions on triage, evacuation and treatment of victims, facilitate coordination of actions and optimal distribution of rescue services resources in peacetime, emergency situations and combat operations. Artificial intelligence algorithms using computer vision, natural language processing and mobile wireless sensor systems expand the capabilities of remote search and remote medical triage of victims. Artificial intelligence systems developed on the basis of machine learning algorithms significantly outperform traditional triage tools in the accuracy of identifying victims with severe trauma who require emergency surgery and intensive care. Artificial intelligence can reduce the number of errors, but does not replace the professional experience of a specialist providing prehospital care, and only provides an additional tool to support decision making. Further exploration of the potential for using artificial intelligence technologies in real-world prehospital trauma care settings is needed."}
{"paperId": "2c3e41460bea99433b29aef2de54b9b9d3b71aac", "url": "https://www.semanticscholar.org/paper/2c3e41460bea99433b29aef2de54b9b9d3b71aac", "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 12, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20271, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-26", "authors": [{"authorId": "2239200170", "name": "Haoqin Tu"}, {"authorId": "2354198472", "name": "Weitao Feng"}, {"authorId": "2352187820", "name": "Hardy Chen"}, {"authorId": "2353061362", "name": "Hui Liu"}, {"authorId": "2313288320", "name": "Xianfeng Tang"}, {"authorId": "2239227247", "name": "Cihang Xie"}], "abstract": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data."}
{"paperId": "2d1d73e55c2d92d408258e847f9d470c7d1fc3fa", "url": "https://www.semanticscholar.org/paper/2d1d73e55c2d92d408258e847f9d470c7d1fc3fa", "title": "Vision-and-Language Navigation with Perspective Taking and Knowledge Graph", "venue": "2025 IEEE 26th China Conference on System Simulation Technology and its Applications (CCSSTA)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IEEECONF65522.2025.11137203?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IEEECONF65522.2025.11137203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-11", "authors": [{"authorId": "2145346539", "name": "Xuefei Wu"}, {"authorId": "2323795460", "name": "Zihao Wang"}, {"authorId": "2364078680", "name": "Jiabao Zhao"}, {"authorId": "2128415182", "name": "Bo Xin"}, {"authorId": "2379238641", "name": "Chunlin Chen"}], "abstract": "Vision-and-Language Navigation (VLN) is a significant natural navigation task in human-robot interaction environments, which requires a robot to navigate according to natural language instructions and visual information. In unknown environments, the VLN tasks will be confronted with great challenges due to the uncertainty of the unseen environments and the natural language instructions. Hence, it plays a crucial role for the robot to interact efficiently with humans and to have common sense knowledge for better performance in VLN tasks. In this paper, we propose an effective vision-and-language navigation approach with spatial perspective taking and knowledge graph. First, spatial perspective taking is introduced into the VLN tasks by considering the selection of reference frames to avoid the ambiguity of human natural language instructions, where the selection preference of reference frames is estimated in an actual conversational situation for the design of human-centered robots. Second, we propose and implement two VLN models, i.e., Graph Cross-Modal Reasoning Navigator (GCMRN) and Attention Graph Convolutional Network Cross-Modal Recognition Reasoning Navigator (AGCMR2N), using knowledge graph provided with prior knowledge in language and vision. Finally, several groups of experiments are implemented in a continuous 3D environment, and the results demonstrate the improved performance of the proposed approach."}
{"paperId": "2d528a445d3b15dc350da4d5ae9c620840cbe0d6", "url": "https://www.semanticscholar.org/paper/2d528a445d3b15dc350da4d5ae9c620840cbe0d6", "title": "Seeing Beyond Words: MatVQA for Challenging Visual-Scientific Reasoning in Materials Science", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18319, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-23", "authors": [{"authorId": "2201278124", "name": "Sifan Wu"}, {"authorId": "2257922310", "name": "Huan Zhang"}, {"authorId": "2317031868", "name": "Yizhan Li"}, {"authorId": "97205750", "name": "Farshid Effaty"}, {"authorId": "2363495394", "name": "Amirreza Ataei"}, {"authorId": "2302677139", "name": "Bang Liu"}], "abstract": "The emergence of Multimodal Large Language Models (MLLMs) that integrate vision and language modalities has unlocked new potentials for scientific reasoning, outperforming prior benchmarks in both natural language and coding domains. Current materials science evaluation datasets such as MaScQA and SciQA remain largely text-based and fail to capture the visual and research-level analytic complexity required in materials discovery and design. We introduce MatVQA, a scalable benchmark specifically designed to address this gap. Generated via an automated pipeline, MArxivAgent, from recent materials literature, MatVQA features 1325 questions across four critical structure-property-performance (SPP) reasoning tasks. Uniquely, MatVQA employs an iterative process to eliminate textual shortcuts, compelling MLLMs to perform fine-grained, low-level visual analysis of material imagery (e.g., microscopy, diffraction patterns) integrated with multi-step scientific reasoning. Benchmarking 17 open- and closed-source MLLMs on MatVQA reveals substantial gaps in current multimodal reasoning capabilities. MatVQA benchmark data, along with evaluation code, is publicly available in \\href{https://anonymous.4open.science/r/matvqa-1E01}{https://anonymous.4open.science/r/matvqa-1E01/README.md} to catalyze further research in applying MLLMs to complex materials science problems."}
{"paperId": "2d53a8416cd5ededd3e5ce3cf97452230b5c67d4", "url": "https://www.semanticscholar.org/paper/2d53a8416cd5ededd3e5ce3cf97452230b5c67d4", "title": "ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.14689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-20", "authors": [{"authorId": "2120841264", "name": "Yucong Zhang"}, {"authorId": "2348489363", "name": "Juan Liu"}, {"authorId": "2319942460", "name": "Ming Li"}], "abstract": "Pre-trained foundation models have demonstrated remarkable success in audio, vision and language, yet their potential for general machine signal modeling with arbitrary sampling rates-covering acoustic, vibration, and other industrial sensor data-remains under-explored. In this work, we propose a novel foundation model ECHO that integrates an advanced band-split architecture with frequency positional embeddings, enabling spectral localization across arbitrary sampling configurations. Moreover, the model incorporates sliding patches to support inputs of variable length without padding or cropping, producing a concise embedding that retains both temporal and spectral fidelity and naturally extends to streaming scenarios. We evaluate our method on various kinds of machine signal datasets, including previous DCASE task 2 challenges (2020-2025), and widely-used industrial signal corpora. Experimental results demonstrate consistent state-of-the-art performance in machine signal anomaly detection and fault classification, confirming the effectiveness and generalization capability of the proposed model. We open-sourced ECHO on https://github.com/yucongzh/ECHO."}
{"paperId": "2d7770d269e7068c8832b720068a5c0f0304c99f", "url": "https://www.semanticscholar.org/paper/2d7770d269e7068c8832b720068a5c0f0304c99f", "title": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models", "venue": "International Conference on Software Engineering", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-01-08", "authors": [{"authorId": "2327920639", "name": "Kunpeng Zhang"}, {"authorId": "2339100559", "name": "Shuai Wang"}, {"authorId": "2318128236", "name": "Jitao Han"}, {"authorId": "2048997801", "name": "Xiaogang Zhu"}, {"authorId": "2339228796", "name": "Xian Li"}, {"authorId": "2263670970", "name": "Shaohua Wang"}, {"authorId": "2119605594", "name": "Sheng Wen"}], "abstract": "Deep learning (DL) libraries are widely used to form the basis of various AI applications in computer vision, natural language processing, and software engineering domains. Despite their popularity, DL libraries are known to have vulnerabilities, such as buffer overflows, use-after-free, and integer overflows, that can be exploited to compromise the security or effectiveness of the underlying libraries. While traditional fuzzing techniques have been used to find bugs in software, they are not well-suited for DL libraries. In general, the complexity of DL libraries and the diversity of their APIs make it challenging to test them thoroughly. To date, mainstream DL libraries like TensorFlow and PyTorch have featured over 1,000 APIs, and the number of APIs is still growing. Fuzzing all these APIs is a daunting task, especially when considering the complexity of the input data and the diversity of the API usage patterns. Recent advances in large language models (LLMs) have illustrated the high potential of LLMs in understanding and synthesizing human-like code. Despite their high potential, we find that emerging LLM-based fuzzers are less optimal for DL library API fuzzing, given their lack of in-depth knowledge on API input edge cases and inefficiency in generating test inputs. In this paper, we propose DFuzz, a LLM-driven DL library fuzzing approach. We have two key insights: (1) With high reasoning ability, LLMs can replace human experts to reason edge cases (likely error-triggering inputs) from checks in an API's code, and transfer the extracted knowledge to test other (new or rarely-tested) APIs. (2) With high generation ability, LLMs can synthesize initial test programs with high accuracy that automates API testing. DFuzz provides LLMs with a novel “white-box view” of DL library APIs, and therefore, can leverage LLMs' reasoning and generation abilities to achieve comprehensive fuzzing. Our experimental results on popular DL libraries demonstrate that DFuzz is able to cover more APIs than SOTA (LLM-based) fuzzers on TensorFlow and PyTorch, respectively. Moreover, DFuzz successfully detected 37 bugs, with 8 already fixed and 19 replicated by the developer but still under investigation."}
{"paperId": "2e07bd64973d0a96f1f709ff2936ba7d0cb88e3b", "url": "https://www.semanticscholar.org/paper/2e07bd64973d0a96f1f709ff2936ba7d0cb88e3b", "title": "Know \"No\" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP", "venue": "arXiv.org", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.10913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-19", "authors": [{"authorId": "2258881923", "name": "Junsung Park"}, {"authorId": "143758901", "name": "Jungbeom Lee"}, {"authorId": "10788591", "name": "Jongyoon Song"}, {"authorId": "2148401281", "name": "Sangwon Yu"}, {"authorId": "51152160", "name": "Dahuin Jung"}, {"authorId": "2284734965", "name": "Sungroh Yoon"}], "abstract": "While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation - such as failing to differentiate concepts like\"parking\"from\"no parking\"- poses substantial challenges. By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data. To address this, we introduce data generation pipelines that employ a large language model (LLM) and a multimodal LLM to produce negation-inclusive captions. Fine-tuning CLIP with data generated from our pipelines, we develop NegationCLIP, which enhances negation awareness while preserving the generality. Moreover, to enable a comprehensive evaluation of negation understanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'ability to interpret negation across diverse expressions and positions within a sentence. Experiments on various CLIP architectures validate the effectiveness of our data generation pipelines in enhancing CLIP's ability to perceive negation accurately. Additionally, NegationCLIP's enhanced negation awareness has practical applications across various multimodal tasks, demonstrated by performance gains in text-to-image generation and referring image segmentation."}
{"paperId": "2f44ebb9e787d792ad6a8327528e2167bbcd9ace", "url": "https://www.semanticscholar.org/paper/2f44ebb9e787d792ad6a8327528e2167bbcd9ace", "title": "Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.01548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2378736912", "name": "Zihao Wang"}, {"authorId": "151497321", "name": "Enneng Yang"}, {"authorId": "2254142682", "name": "Lu Yin"}, {"authorId": "2255081098", "name": "Shiwei Liu"}, {"authorId": "2346065531", "name": "Li Shen"}], "abstract": "Model merging leverages multiple finetuned expert models to construct a multi-task model with low cost, and is gaining increasing attention. However, as a growing number of finetuned models become publicly available, concerns about the safety of model merging have emerged. Unauthorized merging may infringe on developers'rights and risk leaking sensitive personal information. Most existing methods focus on detecting whether a merged model originates from a specific source model, but fail to effectively prevent illegal merging. In this paper, we propose MergeLock, an active protection mechanism that disrupts model parameters to render them unmergeable, thereby directly preventing unauthorized model merging. Specifically, leveraging the inherent symmetry of the attention mechanism in Transformer-based models, we randomly sample two pairs of invertible matrices and apply them to the Query-Key (QK) and Value-Output (VO) branches. This transformation keeps the model's output unchanged while pushing it away from the shared parameter space of other finetuned models. Extensive experiments across both vision and language tasks demonstrate that MergeLock can degrade the performance of merged models by over 95% when a protected model is involved in most cases, demonstrating its effectiveness. Moreover, we further demonstrate that merged models protected by MergeLock cannot be effectively recovered using low-cost restoration methods, further enhancing robustness against unauthorized merging. The code is available at https://github.com/hetailang/Merge-Lock."}
{"paperId": "2f472a78d893ba279aede615872bf28ffb59e709", "url": "https://www.semanticscholar.org/paper/2f472a78d893ba279aede615872bf28ffb59e709", "title": "Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-15", "authors": [{"authorId": "1810689778", "name": "Adam Goodge"}, {"authorId": "2312723343", "name": "Wee Siong Ng"}, {"authorId": "2258715976", "name": "Bryan Hooi"}, {"authorId": "2331858192", "name": "S. Ng"}], "abstract": "Foundation models have revolutionized artificial intelligence, setting new benchmarks in performance and enabling transformative capabilities across a wide range of vision and language tasks. However, despite the prevalence of spatio-temporal data in critical domains such as transportation, public health, and environmental monitoring, spatio-temporal foundation models (STFMs) have not yet achieved comparable success. In this paper, we articulate a vision for the future of STFMs, outlining their essential characteristics and the generalization capabilities necessary for broad applicability. We critically assess the current state of research, identifying gaps relative to these ideal traits, and highlight key challenges that impede their progress. Finally, we explore potential opportunities and directions to advance research towards the aim of effective and broadly applicable STFMs."}
{"paperId": "2fd9accfa911482db722a64dd09267a5d56e3775", "url": "https://www.semanticscholar.org/paper/2fd9accfa911482db722a64dd09267a5d56e3775", "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.20077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-26", "authors": [{"authorId": "2267015572", "name": "Abdelrahman Mohamed"}, {"authorId": "2352792233", "name": "Yova Kementchedjhieva"}], "abstract": "Despite significant advances in vision-language models (VLMs), image captioning often suffers from a lack of detail, with base models producing short, generic captions. This limitation persists even though VLMs are equipped with strong vision and language backbones. While supervised data and complex reward functions have been proposed to improve detailed image captioning, we identify a simpler underlying issue: a bias towards the end-of-sequence (EOS) token, which is introduced during cross-entropy training. We propose an unsupervised method to debias the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the generation of longer, more detailed captions without the need for intricate reward functions or supervision. Our approach is straightforward, effective, and easily applicable to any pretrained model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed captioning benchmarks. Our results show a substantial increase in caption length and relevant details, albeit with an expected increase in the rate of hallucinations."}
{"paperId": "32cc2e2ad5e0ff51b25798ba5ba23c20c103487f", "url": "https://www.semanticscholar.org/paper/32cc2e2ad5e0ff51b25798ba5ba23c20c103487f", "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "venue": "arXiv.org", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.23468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2261894061", "name": "Xuan Yao"}, {"authorId": "46930271", "name": "Junyu Gao"}, {"authorId": "2237947504", "name": "Changsheng Xu"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at https://github.com/Feliciaxyao/NavMorph."}
{"paperId": "32e329b4e0a2277ca899e555f119c5f7ff9b1b80", "url": "https://www.semanticscholar.org/paper/32e329b4e0a2277ca899e555f119c5f7ff9b1b80", "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.06205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-06", "authors": [{"authorId": "2366255182", "name": "Sheng Chen"}, {"authorId": "2366069176", "name": "Peiyu He"}, {"authorId": "2284031353", "name": "Jiaxin Hu"}, {"authorId": "2353805094", "name": "Ziyang Liu"}, {"authorId": "2365970109", "name": "Yansheng Wang"}, {"authorId": "2366066669", "name": "Tao Xu"}, {"authorId": "2378839045", "name": "Chi Zhang"}, {"authorId": "2365972207", "name": "Chongchong Zhang"}, {"authorId": "2366019430", "name": "Chao An"}, {"authorId": "2326997477", "name": "Shiyu Cai"}, {"authorId": "2366025611", "name": "Duo Cao"}, {"authorId": "2366059979", "name": "Kangping Chen"}, {"authorId": "2366017691", "name": "Shuai Chu"}, {"authorId": "2366006916", "name": "Tianwei Chu"}, {"authorId": "2366024710", "name": "Mingdi Dan"}, {"authorId": "2054400611", "name": "Min Du"}, {"authorId": "2367096878", "name": "Weiwei Fang"}, {"authorId": "2366007875", "name": "Pengyou Fu"}, {"authorId": "2284030649", "name": "Jun-Pei Hu"}, {"authorId": "2144812504", "name": "Xiaowei Jiang"}, {"authorId": "2366001757", "name": "Zhaodi Jiang"}, {"authorId": "2367565418", "name": "Fuxuan Li"}, {"authorId": "2367485344", "name": "Jun Li"}, {"authorId": "2112148529", "name": "Minghui Li"}, {"authorId": "2364399983", "name": "Ming-Yang Li"}, {"authorId": "2365990979", "name": "Yanchang Li"}, {"authorId": "2287247026", "name": "Zhibin Li"}, {"authorId": "2295872981", "name": "Guangming Liu"}, {"authorId": "2366606829", "name": "Kairui Liu"}, {"authorId": "2365994221", "name": "Lihao Liu"}, {"authorId": "2292683205", "name": "Weizhi Liu"}, {"authorId": "2366052119", "name": "Xiaoshun Liu"}, {"authorId": "2363296540", "name": "Yufei Liu"}, {"authorId": "2340515957", "name": "Yunfei Liu"}, {"authorId": "2117523134", "name": "Qiang Lu"}, {"authorId": "2230256756", "name": "Yu-Xuan Luo"}, {"authorId": "2283819883", "name": "Xianguo Lv"}, {"authorId": "2218358946", "name": "Hong‐Hui Ma"}, {"authorId": "2319386756", "name": "Sai Ma"}, {"authorId": "2278411661", "name": "Li-Li Mi"}, {"authorId": "2366013968", "name": "Sha Sa"}, {"authorId": "2250493585", "name": "H. Shu"}, {"authorId": "2368403470", "name": "Lei Tian"}, {"authorId": "2336101125", "name": "Chengzhi Wang"}, {"authorId": "2303302508", "name": "Jiayu Wang"}, {"authorId": "2393791221", "name": "Kaijie Wang"}, {"authorId": "2366060813", "name": "Qingyi Wang"}, {"authorId": "2108875655", "name": "R. Wang"}, {"authorId": "2298015352", "name": "Tao Wang"}, {"authorId": "2274415378", "name": "Wei Wang"}, {"authorId": "2366044919", "name": "Xirui Wang"}, {"authorId": "2340419946", "name": "Chaoran Wei"}, {"authorId": "2115325391", "name": "Xu Wei"}, {"authorId": "2367558041", "name": "Zijun Xia"}, {"authorId": "2367627513", "name": "Zhaohao Xiao"}, {"authorId": "2366192627", "name": "Tingshuai Yan"}, {"authorId": "2329843892", "name": "Liyan Yang"}, {"authorId": "2306051809", "name": "Yifan Yang"}, {"authorId": "2362507899", "name": "Zhikai Yang"}, {"authorId": "2365982973", "name": "Zhong Yin"}, {"authorId": "2281327489", "name": "Li Yuan"}, {"authorId": "2281327489", "name": "Li Yuan"}, {"authorId": "2336047649", "name": "Jinyang Zhang"}, {"authorId": "2200066731", "name": "Junhui Zhang"}, {"authorId": "2310234178", "name": "Ling-Yu Zhang"}, {"authorId": "2341868556", "name": "Zhen-Yu Zhang"}, {"authorId": "2365995921", "name": "Zheyu Zhang"}, {"authorId": "2349402528", "name": "Dong-Ya Zhu"}, {"authorId": "2290999646", "name": "Hang Li"}, {"authorId": "2316163827", "name": "Y. Zhang"}], "abstract": "Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments."}
{"paperId": "32f82c083b7bed076161a19c3198d175c934fb71", "url": "https://www.semanticscholar.org/paper/32f82c083b7bed076161a19c3198d175c934fb71", "title": "Narrativity-Aware Video Summarization Based on Vision and Language Foundation Models", "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/APSIPAASC65261.2025.11249316?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/APSIPAASC65261.2025.11249316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2394987547", "name": "Shumpei Saito"}, {"authorId": "2054189526", "name": "Hiroyuki Ueda"}, {"authorId": "2237865407", "name": "Yosuke Ito"}, {"authorId": "2348450581", "name": "Kazuyoshi Yoshii"}], "abstract": "This paper presents a novel video summarization approach that prioritizes the narrative quality of the summarized video to enhance its enjoyment and appeal. While most video summarization studies focus on extracting salient scenes using lowlevel visual features, they often neglect the storytelling aspect to optimize numerical performance on standard benchmarks. To address this, we propose a multifaceted video summarization method that leverages vision and language foundation models to assess shot-level importance (e.g., 2-sec intervals) based on both visual salience and textual narrativity. Specifically, our method employs a vision-language model (VLM) to generate objective captions for individual shots. These shot-wise textual descriptions are then fed into a large language model (LLM) with a prompt designed to produce a semantically-coherent text summary with strong narrativity. The narrativity-aware text embeddings obtained by the LLM, combined with visual embeddings from a vision foundation model, are processed by a recurrent neural network (RNN) to predict importance scores. The LLM and RNN are jointly fine-tuned to align with existing benchmarks. Experiments on the SumMe benchmark demonstrated the effectiveness of our multifaceted approach, highlighting significant performance improvements and the potential of text-domain video summarization."}
{"paperId": "33edf5549e968b6a87c4165091df24423caea5fa", "url": "https://www.semanticscholar.org/paper/33edf5549e968b6a87c4165091df24423caea5fa", "title": "Advances on Multimodal Remote Sensing Foundation Models for Earth Observation Downstream Tasks: A Survey", "venue": "Remote Sensing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17213532?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17213532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-24", "authors": [{"authorId": "2243399675", "name": "Guoqing Zhou"}, {"authorId": "2309967642", "name": "Lihuang Qian"}, {"authorId": "2060450214", "name": "P. Gamba"}], "abstract": "Remote sensing foundation models (RSFMs) have demonstrated excellent feature extraction and reasoning capabilities under the self-supervised learning paradigm of “unlabeled datasets—model pre-training—downstream tasks”. These models achieve superior accuracy and performance compared to existing models across numerous open benchmark datasets. However, when confronted with multimodal data, such as optical, LiDAR, SAR, text, video, and audio, the RSFMs exhibit limitations in cross-modal generalization and multi-task learning. Although several reviews have addressed the RSFMs, there is currently no comprehensive survey dedicated to vision–X (vision, language, audio, position) multimodal RSFMs (MM-RSFMs). To tackle this gap, this article provides a systematic review of MM-RSFMs from a novel perspective. Firstly, the key technologies underlying MM-RSFMs are reviewed and analyzed, and the available multimodal RS pre-training datasets are summarized. Then, recent advances in MM-RSFMs are classified according to the development of backbone networks and cross-modal interaction methods of vision–X, such as vision–vision, vision–language, vision–audio, vision–position, and vision–language–audio. Finally, potential challenges are analyzed, and perspectives for MM-RSFMs are outlined. This survey from this paper reveals that current MM-RSFMs face the following key challenges: (1) a scarcity of high-quality multimodal datasets, (2) limited capability for multimodal feature extraction, (3) weak cross-task generalization, (4) absence of unified evaluation criteria, and (5) insufficient security measures."}
{"paperId": "3486d45ef719ae2bf18292f9868ec245ff8c21af", "url": "https://www.semanticscholar.org/paper/3486d45ef719ae2bf18292f9868ec245ff8c21af", "title": "Deep learning and GCC-MA based visual system for mango leaf disease analysis with anthracnose and sooty mold as case study", "venue": "Journal of Applied Horticulture", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.37855/jah.2025.v27i03.100?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.37855/jah.2025.v27i03.100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2329034303", "name": "Vaibhav Srivastava"}, {"authorId": "2329033187", "name": "M. Srivastava"}, {"authorId": "2393285547", "name": "Shailendra Rajan"}], "abstract": "Mango leaf diseases threaten yield and quality, underscoring the need for early diagnosis and actionable monitoring. Addressing the absence of an integrated Visual Management System (Proposed VMS Phase-I) in prevailing methodologies, this work introduces an enhanced Proposed VMS Phase-I for real-time mango leaf disease analysis that unifies computer vision and language understanding. The framework employs Soft Root Sign–VGG16–Fast Dropout–ResNet 50 (SRS VGG16 FD ResNet 50) for visual classification and Generalized Canonical Correlation Multimodal Autoencoders (GCC MA) for cross modal feature fusion. On the visual side, mango leaf images are preprocessed, followed by chlorosis detection, super pixel segmentation, and feature extraction. In parallel, disease related question–answer (Q&A) data are curated and preprocessed; keywords are extracted, entity–relation graphs are constructed, and textual features are derived. LeCun Bayesian BERT (LeCunBay BERT) generates embeddings for answers, questions, and keywords to enhance semantic representation. The visual and textual features are then fused via GCC MA to model cross modal correlations, after which SRS VGG16 FD ResNet 50 performs final disease classification. During testing, authenticated users submit images with queries through the Proposed VMS Phase-I interface, and the system returns both the predicted diagnosis and corresponding natural language answers. The proposed framework outperforms existing methodologies for mango leaf disease detection, achieving higher accuracy (98.45%) while supporting interpretable, real time decision support for orchard management. As a case study, we demonstrate the system’s effectiveness on anthracnose and sooty mould, illustrating end to end detection, interpretation, and user facing guidance."}
{"paperId": "348723ee0c01fc853710ab0f0b50ccf2e7e87292", "url": "https://www.semanticscholar.org/paper/348723ee0c01fc853710ab0f0b50ccf2e7e87292", "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.06452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-08", "authors": [{"authorId": "1802481976", "name": "Mattia Litrico"}, {"authorId": "3393888", "name": "M. Giuffrida"}, {"authorId": "1742452", "name": "S. Battiato"}, {"authorId": "2977931", "name": "D. Tuia"}], "abstract": "Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance."}
{"paperId": "34b29fb1575491e5a5eeb2da08ec723b779ca87a", "url": "https://www.semanticscholar.org/paper/34b29fb1575491e5a5eeb2da08ec723b779ca87a", "title": "Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-11", "authors": [{"authorId": "2366662796", "name": "Lipei Xie"}, {"authorId": "2366330612", "name": "Yingxin Li"}, {"authorId": "2367195460", "name": "Huiping Zhuang"}], "abstract": "Embodied foundation models are crucial for Artificial Intelligence (AI) interacting with the physical world by integrating multi-modal inputs, such as proprioception, vision and language, to understand human intentions and generate actions to control robots. While these models demonstrate strong generalization and few-shot learning capabilities, they face significant challenges in continually acquiring new skills without forgetting previously learned skills, a problem known as catastrophic forgetting. To address this issue, we propose the Analytic Task Scheduler (ATS), a novel framework for continual learning in embodied foundation models. ATS consists of a task-specific model library, where each model is fine-tuned independently on a single task, and an analytic scheduler trained using recursive least squares (RLS) to learn the mapping between language instructions and task-specific models. This architecture enables accurate task recognition and dynamic model selection while fundamentally avoiding parameter interference across tasks. The scheduler updates its parameters incrementally using only statistics (autocorrelation and cross-correlation matrices), enabling forgetting-resistant learning without the need to revisit historical data. We validate ATS on a real-world robot platform (RM65B), demonstrating superior resistance to forgetting and strong adaptability to task variations. The results highlight ATS as an effective, scalable, and deployable solution for continual learning in embodied foundation models operating in complex, dynamic environments. Our code will be available at https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler"}
{"paperId": "34d5af80e0857f2ae4cc677fd4f4ae511ca2edce", "url": "https://www.semanticscholar.org/paper/34d5af80e0857f2ae4cc677fd4f4ae511ca2edce", "title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2336303219", "name": "Xinyu Mao"}, {"authorId": "2390402815", "name": "Junsi Li"}, {"authorId": "2390406457", "name": "Haoji Zhang"}, {"authorId": "2390502176", "name": "Yu Liang"}, {"authorId": "2390559469", "name": "Ming Sun"}], "abstract": "Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at https://github.com/Sweet4tars/seps.git."}
{"paperId": "34ed8a5301b4dcf69f20691d075f1c66efc2f8ab", "url": "https://www.semanticscholar.org/paper/34ed8a5301b4dcf69f20691d075f1c66efc2f8ab", "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.10360, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-11", "authors": [{"authorId": "2142458237", "name": "Liuyi Wang"}, {"authorId": "2149073957", "name": "Zongtao He"}, {"authorId": "2397779384", "name": "Jinlong Li"}, {"authorId": "2398000733", "name": "Xiaoyan Qi"}, {"authorId": null, "name": "Mengxian Hu"}, {"authorId": "2033466148", "name": "Chenpeng Yao"}, {"authorId": "2920326", "name": "Chengju Liu"}, {"authorId": "2288041201", "name": "Qijun Chen"}], "abstract": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios."}
{"paperId": "354c8c968586c91e3437e267aa0e4b1bcc448521", "url": "https://www.semanticscholar.org/paper/354c8c968586c91e3437e267aa0e4b1bcc448521", "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-14", "authors": [{"authorId": "2375954744", "name": "Chenliang Zhang"}, {"authorId": "2376052033", "name": "Lin Wang"}, {"authorId": "2283472028", "name": "Yuanyuan Lu"}, {"authorId": "2376152101", "name": "Yusheng Qi"}, {"authorId": "2376277512", "name": "Kexin Wang"}, {"authorId": "2375904841", "name": "Peixu Hou"}, {"authorId": "2376040209", "name": "Wenshi Chen"}], "abstract": "This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline."}
{"paperId": "3560e8b8dc2cc8d4dcd868cff182965766cbbe50", "url": "https://www.semanticscholar.org/paper/3560e8b8dc2cc8d4dcd868cff182965766cbbe50", "title": "Early childhood education teachers' professional vision of language-stimulating interactions: a mixed-method mobile eye-tracking study", "venue": "Teaching and Teacher Education : An International Journal of Research and Studies", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.tate.2025.105199?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.tate.2025.105199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-01", "authors": [{"authorId": "2172285721", "name": "Thibaut Duthois"}, {"authorId": "2378690404", "name": "Maribel Montero-Perez"}, {"authorId": "1593169341", "name": "Piet van Avermaet"}, {"authorId": "39877741", "name": "R. Vanderlinde"}], "abstract": null}
{"paperId": "356f6c46f6862135197575f5045a125fe042d982", "url": "https://www.semanticscholar.org/paper/356f6c46f6862135197575f5045a125fe042d982", "title": "UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.14738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2278501965", "name": "Pengyu Wang"}, {"authorId": "2330229349", "name": "Shaojun Zhou"}, {"authorId": "2281259541", "name": "Chenkun Tan"}, {"authorId": "2157203756", "name": "Xinghao Wang"}, {"authorId": "2381216634", "name": "Wei Huang"}, {"authorId": "2386089546", "name": "Zhen Ye"}, {"authorId": "2278452197", "name": "Zhaowei Li"}, {"authorId": "2258680333", "name": "Botian Jiang"}, {"authorId": "2109797247", "name": "Dong Zhang"}, {"authorId": "2294872225", "name": "Xipeng Qiu"}], "abstract": "Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at https://github.com/fnlp-vision/UnifiedVisual."}
{"paperId": "3574237644089babdf3e94cd4bc6e7377c959313", "url": "https://www.semanticscholar.org/paper/3574237644089babdf3e94cd4bc6e7377c959313", "title": "Certified Robustness of Antenna Selecting Neural Networks for Massive MIMO Wireless Communications", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3570973?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3570973, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2258290719", "name": "Jaekwon Kim"}, {"authorId": "2110706876", "name": "Hyo-Sang Lim"}, {"authorId": "2110459178", "name": "Kwanghoon Choi"}], "abstract": "Future wireless systems with massive antennas must balance data rates and RF chain costs. Antenna selection activating only a subset of antennas addresses this challenge. Recently, neural network-based approaches have shown promise over traditional symbolic methods, offering fixed complexity in inference and suitability for hardware implementation. However, their closed-box nature raises concerns for safety-critical 6G applications like autonomous driving and drones, where reliable communication is vital. Specifically, it is often unclear how the neural network determines which antennas to select, making it difficult to interpret or trust the decision-making process. This paper investigates the robustness of neural networks for antenna selection in such contexts. While empirical robustness against finite random inputs sampled from a uniform distribution may suffice for general applications, certified robustness ensuring consistent inference under all possible perturbations is essential for safety-critical systems. Although certified robustness is well studied in vision and language tasks, we are the first, to our knowledge, to explore its application in telecommunications. We mathematically define robustness for antenna-selection networks and apply state-of-the-art linear relaxation-based perturbation analysis. Our findings show that pruned networks, beyond being more efficient, also exhibit superior certified robustness compared to their unpruned counterparts. We further compare certified and empirical robustness, identifying a significant gap that suggests the need for improved certification methods. Additionally, in our antenna selection setting, we observe that removing monotonic activations in the final layer improves certified robustness."}
{"paperId": "3582ad147fce6a4a6425da7555163847fcdefffb", "url": "https://www.semanticscholar.org/paper/3582ad147fce6a4a6425da7555163847fcdefffb", "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2301111695", "name": "Xuanwen Ding"}, {"authorId": "2364698022", "name": "Chengjun Pan"}, {"authorId": "2109967493", "name": "Zejun Li"}, {"authorId": "2223697026", "name": "Jiwen Zhang"}, {"authorId": "2363177846", "name": "Siyuan Wang"}, {"authorId": "2290027793", "name": "Zhongyu Wei"}], "abstract": "Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model's real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench."}
{"paperId": "35ae555a5867407fe81dc9d03ab807c49ef6fae3", "url": "https://www.semanticscholar.org/paper/35ae555a5867407fe81dc9d03ab807c49ef6fae3", "title": "HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 27, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-25", "authors": [{"authorId": "2292885632", "name": "Jiaxin Zhao"}, {"authorId": "2110040288", "name": "Qize Yang"}, {"authorId": "2340468951", "name": "Yi-Xing Peng"}, {"authorId": "2340401195", "name": "Detao Bai"}, {"authorId": "2342904384", "name": "Shimin Yao"}, {"authorId": "2342467513", "name": "Boyuan Sun"}, {"authorId": "2339423925", "name": "Xiang Chen"}, {"authorId": "2312009121", "name": "Shenghao Fu"}, {"authorId": "2342456832", "name": "Weixuan chen"}, {"authorId": "2339268195", "name": "Xihan Wei"}, {"authorId": "2342410809", "name": "Liefeng Bo"}], "abstract": "In human-centric scenes, the ability to simultaneously understand visual and auditory information is crucial. While recent omni models can process multiple modalities, they generally lack effectiveness in human-centric scenes due to the absence of large-scale, specialized datasets and non-targeted architectures. In this work, we developed HumanOmni, the industry's first human-centric Omni-multimodal large language model. We constructed a dataset containing over 2.4 million human-centric video clips with detailed captions and more than 14 million instructions, facilitating the understanding of diverse human-centric scenes. HumanOmni includes three specialized branches for understanding different types of scenes. It adaptively fuses features from these branches based on user instructions, significantly enhancing visual understanding in scenes centered around individuals. Moreover, HumanOmni integrates audio features to ensure a comprehensive understanding of environments and individuals. Our experiments validate HumanOmni's advanced capabilities in handling human-centric scenes across a variety of tasks, including emotion recognition, facial expression description, and action understanding. Our model will be open-sourced to facilitate further development and collaboration within both academia and industry."}
{"paperId": "35b7fa69d940800bafd667ede84057b57a5f664f", "url": "https://www.semanticscholar.org/paper/35b7fa69d940800bafd667ede84057b57a5f664f", "title": "Knowledge-Augmented Vision-and-Language Assistant", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2346650029", "name": "Chia-Wen Kuo"}], "abstract": null}
{"paperId": "3610c2c67bb25e57d51429ba1fb0ca92ead715ef", "url": "https://www.semanticscholar.org/paper/3610c2c67bb25e57d51429ba1fb0ca92ead715ef", "title": "Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.08298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-11", "authors": [{"authorId": "2391814474", "name": "Luca Bindini"}, {"authorId": "2338888190", "name": "Simone Giovannini"}, {"authorId": "2350578001", "name": "Simone Marinai"}, {"authorId": "2339000478", "name": "Valeria Nardoni"}, {"authorId": "2378127476", "name": "Kimiya Noor Ali"}], "abstract": "This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models'comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs."}
{"paperId": "366479a888a70706fe75f5d0a7aaf821bd0f5de2", "url": "https://www.semanticscholar.org/paper/366479a888a70706fe75f5d0a7aaf821bd0f5de2", "title": "Demystifying Deep Learning and Neural Networks", "venue": "European journal of computer science and information technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.37745/ejcsit.2013/vol13n464555?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.37745/ejcsit.2013/vol13n464555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-15", "authors": [{"authorId": "2369000437", "name": "Jawahar Ravee Nithianandam"}], "abstract": "Deep learning and neural networks have revolutionized artificial intelligence, transforming industries and daily life with applications ranging from voice assistants to medical diagnostics. Despite their ubiquity, these technologies remain enigmatic to many enthusiasts and practitioners. This article demystifies the fundamental concepts underlying neural networks, exploring their biological inspiration, architectural components, and learning mechanisms. Various deep learning architectures are examined, including convolutional neural networks, recurrent neural networks, transformers, and generative adversarial networks, elucidating their distinctive features and applications. The discussion extends to practical considerations in training neural networks, highlighting data requirements, optimization challenges, and regularization techniques. By exploring applications across computer vision, natural language processing, speech recognition, and recommendation systems, the transformative impact of these technologies is illustrated. The article concludes by addressing limitations and ethical considerations, emphasizing the importance of interpretability, fairness, resource efficiency, and environmental sustainability as the field continues to advance."}
{"paperId": "36a1f96d7d6f668b2b913fe66ac2421b1bc05f42", "url": "https://www.semanticscholar.org/paper/36a1f96d7d6f668b2b913fe66ac2421b1bc05f42", "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-30", "authors": [{"authorId": "2389567857", "name": "Manjunath Prasad"}, {"authorId": "2389609135", "name": "Holenarasipura Rajiv"}, {"authorId": "66673632", "name": "B. M. Vidyavathi"}], "abstract": "In real-world environments, AI systems often face unfamiliar scenarios without labeled data, creating a major challenge for conventional scene understanding models. The inability to generalize across unseen contexts limits the deployment of vision-based applications in dynamic, unstructured settings. This work introduces a Dynamic Context-Aware Scene Reasoning framework that leverages Vision-Language Alignment to address zero-shot real-world scenarios. The goal is to enable intelligent systems to infer and adapt to new environments without prior task-specific training. The proposed approach integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions, enhancing contextual comprehension. A dynamic reasoning module refines predictions by combining global scene cues and object-level interactions guided by linguistic priors. Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and Open Images demonstrate up to 18% improvement in scene understanding accuracy over baseline models in complex and unseen environments. Results also show robust performance in ambiguous or cluttered scenes due to the synergistic fusion of vision and language. This framework offers a scalable and interpretable approach for context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings."}
{"paperId": "36d62a3e2b84f1799d9918600964fc850821038b", "url": "https://www.semanticscholar.org/paper/36d62a3e2b84f1799d9918600964fc850821038b", "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-10", "authors": [{"authorId": "2174869132", "name": "Leqi Shen"}, {"authorId": "2350347193", "name": "Guoqiang Gong"}, {"authorId": "2275199190", "name": "Tianxiang Hao"}, {"authorId": "2350871279", "name": "Tao He"}, {"authorId": "2319431168", "name": "Yifeng Zhang"}, {"authorId": "3340237", "name": "Pengzhang Liu"}, {"authorId": "2243033701", "name": "Sicheng Zhao"}, {"authorId": "2345186205", "name": "Jungong Han"}, {"authorId": "2323657703", "name": "Guiguang Ding"}], "abstract": "The parameter-efficient adaptation of the image-text pre-training model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-To-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 2.2% R@1 and 7.5% R@sum. The code is available at https://github.com/LunarShen/DsicoVLA."}
{"paperId": "378993b250a06aebfc7b327a22ac9cc7da9b5ed2", "url": "https://www.semanticscholar.org/paper/378993b250a06aebfc7b327a22ac9cc7da9b5ed2", "title": "From Pixels to Voice: A Simple and Efficient End-to-End Spoken Image Description Approach via Vision Codec Language Models", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP49660.2025.10890285?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP49660.2025.10890285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-06", "authors": [{"authorId": "2336316055", "name": "Chung Tran"}, {"authorId": "1783949", "name": "S. Sakti"}], "abstract": "Neural audio codecs provide a powerful tool for compressing audio signals into discrete codec representations. This compact discrete representation has made it possible to successfully apply a natural language processing (NLP) model to various audio and speech processing tasks, including text-to-speech (e.g., VALL-E, VALL-E X) and multimodal audio-text generation (e.g., LauraGPT, VioLA). While these models excel at handling sequential data like text and speech, their potential for processing non-sequential data, such as images, remains unexplored. In this paper, we introduce PixVoxLM, a simple and efficient end-to-end framework that combines vision-language models with neural audio codecs to tackle the Image-to-Speech (I2S) problem. Experiments on the Flickr8k dataset demonstrate that PixVoxLM delivers promising results compared to existing I2S methods. Furthermore, this research is the first to explore a new capability: visual-guided speech completion in I2S model, paving the way for new practical applications in everyday communication, such as speech prompt-based instruction."}
{"paperId": "37cb83dc757f483cfd389c8709b4aee389da8dc6", "url": "https://www.semanticscholar.org/paper/37cb83dc757f483cfd389c8709b4aee389da8dc6", "title": "Robot Data Curation with Mutual Information Estimators", "venue": "Robotics", "year": 2025, "citationCount": 16, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-12", "authors": [{"authorId": "2122700519", "name": "Joey Hejna"}, {"authorId": "2247302", "name": "Suvir Mirchandani"}, {"authorId": "3117588", "name": "A. Balakrishna"}, {"authorId": "2345006140", "name": "Annie Xie"}, {"authorId": "88728227", "name": "Ayzaan Wahid"}, {"authorId": "2704494", "name": "Jonathan Tompson"}, {"authorId": "2840758", "name": "Pannag R. Sanketi"}, {"authorId": "2322628540", "name": "Dhruv Shah"}, {"authorId": "144373380", "name": "Coline Devin"}, {"authorId": "1779671", "name": "Dorsa Sadigh"}], "abstract": "The performance of imitation learning policies often hinges on the datasets with which they are trained. Consequently, investment in data collection for robotics has grown across both industrial and academic labs. However, despite the marked increase in the quantity of demonstrations collected, little work has sought to assess the quality of said data despite mounting evidence of its importance in other areas such as vision and language. In this work, we take a critical step towards addressing the data quality in robotics. Given a dataset of demonstrations, we aim to estimate the relative quality of individual demonstrations in terms of both action diversity and predictability. To do so, we estimate the average contribution of a trajectory towards the mutual information between states and actions in the entire dataset, which captures both the entropy of the marginal action distribution and the state-conditioned action entropy. Though commonly used mutual information estimators require vast amounts of data often beyond the scale available in robotics, we introduce a novel technique based on k-nearest neighbor estimates of mutual information on top of simple VAE embeddings of states and actions. Empirically, we demonstrate that our approach is able to partition demonstration datasets by quality according to human expert scores across a diverse set of benchmarks spanning simulation and real world environments. Moreover, training policies based on data filtered by our method leads to a 5-10% improvement in RoboMimic and better performance on real ALOHA and Franka setups."}
{"paperId": "389d58d13b5bf5cd189fb92dc0a95a8fe4667d6c", "url": "https://www.semanticscholar.org/paper/389d58d13b5bf5cd189fb92dc0a95a8fe4667d6c", "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-16", "authors": [{"authorId": "2381991315", "name": "Zekai Zhang"}, {"authorId": "2380855068", "name": "Weiye Zhu"}, {"authorId": "2383950336", "name": "Hewei Pan"}, {"authorId": "2375899094", "name": "Xiangchen Wang"}, {"authorId": "2376110671", "name": "Rongtao Xu"}, {"authorId": "2381920472", "name": "Xing Sun"}, {"authorId": "2381315433", "name": "Feng Zheng"}], "abstract": "The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon."}
{"paperId": "38ddb5654dfacea4f39eda443c308f532ebf1ee2", "url": "https://www.semanticscholar.org/paper/38ddb5654dfacea4f39eda443c308f532ebf1ee2", "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.00033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2303337108", "name": "Diqi He"}, {"authorId": "2390558239", "name": "Xuehao Gao"}, {"authorId": "2267357138", "name": "Hao Li"}, {"authorId": "2348736549", "name": "Junwei Han"}, {"authorId": "2282542948", "name": "Dingwen Zhang"}], "abstract": "The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents'actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE."}
{"paperId": "38e418a2b10a97da1a6dd790bce169b53644338b", "url": "https://www.semanticscholar.org/paper/38e418a2b10a97da1a6dd790bce169b53644338b", "title": "FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-09", "authors": [{"authorId": "2303852479", "name": "Yunbo Li"}, {"authorId": "2303405806", "name": "Jiaping Gui"}, {"authorId": "2384816092", "name": "Zhihang Deng"}, {"authorId": "2363287623", "name": "Fanchao Meng"}, {"authorId": "2303414567", "name": "Yue Wu"}], "abstract": "Federated learning (FL) enables collaborative model training across multiple parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as a balanced approach between synchronous and asynchronous FL. However, SAFL faces significant challenges in optimizing both gradient-based (e.g., FedSGD) and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct trade-offs in accuracy, convergence speed, and stability. While gradient aggregation achieves faster convergence and higher accuracy, it suffers from pronounced fluctuations, whereas model aggregation offers greater stability but slower convergence and suboptimal accuracy. This paper presents FedQS, the first framework to theoretically analyze and address these disparities in SAFL. FedQS introduces a divide-and-conquer strategy to handle client heterogeneity by classifying clients into four distinct types and adaptively optimizing their local training based on data distribution characteristics and available computational resources. Extensive experiments on computer vision, natural language processing, and real-world tasks demonstrate that FedQS achieves the highest accuracy, attains the lowest loss, and ranks among the fastest in convergence speed, outperforming state-of-the-art baselines. Our work bridges the gap between aggregation strategies in SAFL, offering a unified solution for stable, accurate, and efficient federated learning. The code and datasets are available at https://github.com/bkjod/FedQS_."}
{"paperId": "39122d803342b4e5d06e8d6ec0299b1b154aca06", "url": "https://www.semanticscholar.org/paper/39122d803342b4e5d06e8d6ec0299b1b154aca06", "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction", "venue": "arXiv.org", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-05", "authors": [{"authorId": "2359443465", "name": "AI Inclusion"}, {"authorId": "2359148714", "name": "Biao Gong"}, {"authorId": "2359147460", "name": "Cheng Zou"}, {"authorId": "2326457430", "name": "Dandan Zheng"}, {"authorId": "2359165250", "name": "Hu Yu"}, {"authorId": "2359204277", "name": "Jingdong Chen"}, {"authorId": "2323165550", "name": "Jianxin Sun"}, {"authorId": "2359747445", "name": "Junbo Zhao"}, {"authorId": "2347495433", "name": "Jun Zhou"}, {"authorId": "2118930016", "name": "Kaixiang Ji"}, {"authorId": "2275056089", "name": "Lixiang Ru"}, {"authorId": "2359204003", "name": "Libin Wang"}, {"authorId": "2352760420", "name": "Qingpei Guo"}, {"authorId": "2359670780", "name": "Rui Liu"}, {"authorId": "2274107325", "name": "Weilong Chai"}, {"authorId": "2359218157", "name": "Xinyu Xiao"}, {"authorId": "2275102564", "name": "Ziyuan Huang"}], "abstract": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined."}
{"paperId": "393a0a3be2f751b4d53601f9a565236ab4584d62", "url": "https://www.semanticscholar.org/paper/393a0a3be2f751b4d53601f9a565236ab4584d62", "title": "Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-27", "authors": [{"authorId": "2326919293", "name": "Shreya Saha"}, {"authorId": "2325261862", "name": "Shurui Li"}, {"authorId": "1780803442", "name": "Greta Tuckute"}, {"authorId": "2373262590", "name": "Yuanning Li"}, {"authorId": "2373486909", "name": "Ru-Yuan Zhang"}, {"authorId": "2383976544", "name": "Leila Wehbe"}, {"authorId": "144733430", "name": "Evelina Fedorenko"}, {"authorId": "2315916417", "name": "Meenakshi Khosla"}], "abstract": "The human language system represents both linguistic forms and meanings, but the abstractness of the meaning representations remains debated. Here, we searched for abstract representations of meaning in the language cortex by modeling neural responses to sentences using representations from vision and language models. When we generate images corresponding to sentences and extract vision model embeddings, we find that aggregating across multiple generated images yields increasingly accurate predictions of language cortex responses, sometimes rivaling large language models. Similarly, averaging embeddings across multiple paraphrases of a sentence improves prediction accuracy compared to any single paraphrase. Enriching paraphrases with contextual details that may be implicit (e.g., augmenting\"I had a pancake\"to include details like\"maple syrup\") further increases prediction accuracy, even surpassing predictions based on the embedding of the original sentence, suggesting that the language system maintains richer and broader semantic representations than language models. Together, these results demonstrate the existence of highly abstract, form-independent meaning representations within the language cortex."}
{"paperId": "39832ae50dd6f6910d10f5015cade95500c1dd48", "url": "https://www.semanticscholar.org/paper/39832ae50dd6f6910d10f5015cade95500c1dd48", "title": "HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions", "venue": "", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-03-18", "authors": [{"authorId": "2308627775", "name": "Yifei Dong"}, {"authorId": "2351817445", "name": "Fengyi Wu"}, {"authorId": "2309914894", "name": "Qi He"}, {"authorId": "2308904893", "name": "Heng Li"}, {"authorId": "2308557514", "name": "Minghan Li"}, {"authorId": "2261885793", "name": "Zebang Cheng"}, {"authorId": "2308535881", "name": "Yuxuan Zhou"}, {"authorId": "2213832152", "name": "Jingdong Sun"}, {"authorId": "2308468669", "name": "Qi Dai"}, {"authorId": "2283516634", "name": "Zhi-Qi Cheng"}, {"authorId": "2298897790", "name": "Alexander G. Hauptmann"}], "abstract": "Vision-and-Language Navigation (VLN) has been studied mainly in either discrete or continuous settings, with little attention to dynamic, crowded environments. We present HA-VLN 2.0, a unified benchmark introducing explicit social-awareness constraints. Our contributions are: (i) a standardized task and metrics capturing both goal accuracy and personal-space adherence; (ii) HAPS 2.0 dataset and simulators modeling multi-human interactions, outdoor contexts, and finer language-motion alignment; (iii) benchmarks on 16,844 socially grounded instructions, revealing sharp performance drops of leading agents under human dynamics and partial observability; and (iv) real-world robot experiments validating sim-to-real transfer, with an open leaderboard enabling transparent comparison. Results show that explicit social modeling improves navigation robustness and reduces collisions, underscoring the necessity of human-centric approaches. By releasing datasets, simulators, baselines, and protocols, HA-VLN 2.0 provides a strong foundation for safe, socially responsible navigation research."}
{"paperId": "399b9877c80802f3bc942163973b03d8a9c321bf", "url": "https://www.semanticscholar.org/paper/399b9877c80802f3bc942163973b03d8a9c321bf", "title": "Token Reduction Should Go Beyond Efficiency in Generative Models - From Vision, Language to Multimodality", "venue": "arXiv.org", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-23", "authors": [{"authorId": "2350368485", "name": "Zhenglun Kong"}, {"authorId": "2161321379", "name": "Yize Li"}, {"authorId": "2350492053", "name": "Fanhu Zeng"}, {"authorId": "2363501527", "name": "Lei Xin"}, {"authorId": "2221638941", "name": "Shvat Messica"}, {"authorId": "2322988586", "name": "Xue Lin"}, {"authorId": "2363522127", "name": "Pu Zhao"}, {"authorId": "2143693283", "name": "M. Kellis"}, {"authorId": "2290540583", "name": "Hao Tang"}, {"authorId": "2347353861", "name": "M. Zitnik"}], "abstract": "In Transformer architectures, tokens\\textemdash discrete units derived from raw data\\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate\"overthinking\"and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling."}
{"paperId": "3ace2d0ef6efd3d76a6f047df6e3cff931ef1f05", "url": "https://www.semanticscholar.org/paper/3ace2d0ef6efd3d76a6f047df6e3cff931ef1f05", "title": "FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-09-28", "authors": [{"authorId": "2345869389", "name": "Yi Yang"}, {"authorId": "2382923758", "name": "Xiaokun Zhang"}, {"authorId": "2382921214", "name": "Qingchen Fang"}, {"authorId": "2390990515", "name": "Jing Liu"}, {"authorId": "2260680129", "name": "Ziqi Ye"}, {"authorId": "2383257145", "name": "Rui Li"}, {"authorId": "2385814762", "name": "Li Liu"}, {"authorId": "2382931910", "name": "Haipeng Wang"}], "abstract": "Cross-modal artificial intelligence, represented by visual language models, has achieved significant success in general image understanding. However, a fundamental cognitive inconsistency exists between general visual representation and remote sensing image interpretation: remote sensing images couple topography, terrain, and spatial structure, thereby inherently requiring models to possess deep geoscientific understanding. This cognitive difference is further amplified in synthetic aperture radar (SAR) imagery: while SAR possesses irreplaceable all-weather, all-day observation capabilities, it is constrained by coherent imaging mechanisms, exhibiting significant modal heterogeneity with general images. To address this inconsistency, we propose FUSAR-KLIP, the first knowledge-guided general multimodal foundational model for SAR, along with reusable data and evaluation baselines. Specifically: (1) FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection attributes) was constructed, covering multiple satellite platforms, 120,000 images, and 135 cities; (2) Aligned structured text was generated through hierarchical cognitive thought chains, accurately encoding more than 1 million multidimensional semantic information from geomorphological environment and regional attributes to spatial relationships; (3) A self-consistent iterative optimization mechanism was designed to guide cross-modal learning with this knowledge information consistent with human cognition and physical laws in a self-supervised closed loop consisting of contrast, matching, and reconstruction; (4) A unified evaluation benchmark was established in 11 typical downstream tasks in the two major categories of vision and language, and compared with 15 mainstream foundation models."}
{"paperId": "3be8120966e3f270dcd4eff0b5a5abd4b4b83117", "url": "https://www.semanticscholar.org/paper/3be8120966e3f270dcd4eff0b5a5abd4b4b83117", "title": "Strong and Weak Prompt Engineering for Remote Sensing Image-Text Cross-Modal Retrieval", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "https://doi.org/10.1109/jstars.2025.3534474", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2025.3534474?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2025.3534474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2343062261", "name": "Tianci Sun"}, {"authorId": "2113918485", "name": "C. Zheng"}, {"authorId": "2277409496", "name": "Xiu Li"}, {"authorId": "2342634632", "name": "Yanli Gao"}, {"authorId": "2267708392", "name": "Jie Nie"}, {"authorId": "2257931780", "name": "Lei Huang"}, {"authorId": "2172744919", "name": "Zhiqiang Wei"}], "abstract": "Cross-modal retrieval is vital at the intersection of vision and language. Specifically, remote sensing image–text retrieval enhances our understanding of complex remote sensing content by combining multiperspective visual information with concise textual descriptions and has increasingly become a hotspot for research. Existing prompts typically emphasize either global or local information, which fails to excavate or fully leverage the effective information of cross-modal data, resulting in the subpar performance of retrieval models. To address these limitations, we propose a novel method called Strong and Weak Prompt Engineering (SWPE) for remote sensing image–text retrieval. Specifically, SWPE employs the Strong and Weak Prompt Generation module to generate fine-grained and global category semantic prompts via an attention mechanism and a pretrained classification model. The prompt-guided feature fine-tuning module then refines the prompt information using a Transformer architecture, integrating the refined prompts with high-level image, and text features to enhance both fine-grained details and global semantics. Finally, the adaptive hard sample elimination module optimizes the triplet loss function by training the model with negative sample pairs of varying difficulty, assigning higher weights to simpler pairs. Extensive quantitative and qualitative experiments on four remote sensing benchmarks validate the superior effectiveness of SWPE."}
{"paperId": "3bfe81facec550c43cad3da495bf4823a914b80a", "url": "https://www.semanticscholar.org/paper/3bfe81facec550c43cad3da495bf4823a914b80a", "title": "Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.20751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-25", "authors": [{"authorId": "2382026398", "name": "Zoe Wanying He"}, {"authorId": "2382920294", "name": "Sean Trott"}, {"authorId": "2315916417", "name": "Meenakshi Khosla"}], "abstract": "Recent studies show that deep vision-only and language-only models--trained on disjoint modalities--nonetheless project their inputs into a partially aligned representational space. Yet we still lack a clear picture of where in each network this convergence emerges, what visual or linguistic cues support it, whether it captures human preferences in many-to-many image-text scenarios, and how aggregating exemplars of the same concept affects alignment. Here, we systematically investigate these questions. We find that alignment peaks in mid-to-late layers of both model types, reflecting a shift from modality-specific to conceptually shared representations. This alignment is robust to appearance-only changes but collapses when semantics are altered (e.g., object removal or word-order scrambling), highlighting that the shared code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a forced-choice\"Pick-a-Pic\"task shows that human preferences for image-caption matches are mirrored in the embedding spaces across all vision-language model pairs. This pattern holds bidirectionally when multiple captions correspond to a single image, demonstrating that models capture fine-grained semantic distinctions akin to human judgments. Surprisingly, averaging embeddings across exemplars amplifies alignment rather than blurring detail. Together, our results demonstrate that unimodal networks converge on a shared semantic code that aligns with human judgments and strengthens with exemplar aggregation."}
{"paperId": "3c40fa562e053143b26eceb84d8ac825174bd8bc", "url": "https://www.semanticscholar.org/paper/3c40fa562e053143b26eceb84d8ac825174bd8bc", "title": "Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-06", "authors": [{"authorId": "2327290378", "name": "Subeen Park"}, {"authorId": "2373738139", "name": "Joowang Kim"}, {"authorId": "2314517110", "name": "Hakyung Lee"}, {"authorId": "2391816038", "name": "Sunjae Yoo"}, {"authorId": "2353784771", "name": "Kyungwoo Song"}], "abstract": "Deep learning models achieve strong performance across various domains but often rely on spurious correlations, making them vulnerable to distribution shifts. This issue is particularly severe in subpopulation shift scenarios, where models struggle in underrepresented groups. While existing methods have made progress in mitigating this issue, their performance gains are still constrained. They lack a rigorous theoretical framework connecting the embedding space representations with worst-group error. To address this limitation, we propose Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness (SCER), a novel approach that directly regularizes feature representations to suppress spurious cues. We show theoretically that worst-group error is influenced by how strongly the classifier relies on spurious versus core directions, identified from differences in group-wise mean embeddings across domains and classes. By imposing theoretical constraints at the embedding level, SCER encourages models to focus on core features while reducing sensitivity to spurious patterns. Through systematic evaluation on multiple vision and language, we show that SCER outperforms prior state-of-the-art studies in worst-group accuracy. Our code is available at \\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}."}
{"paperId": "3cd52394a5acabad982b61df00afd6e801b32dd4", "url": "https://www.semanticscholar.org/paper/3cd52394a5acabad982b61df00afd6e801b32dd4", "title": "Beyond pixels: The synergy of vision and language in image captioning", "venue": "AIP Conference Proceedings", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0286368?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0286368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "2274018652", "name": "Nirdosh Gandhi"}, {"authorId": "2273995421", "name": "Prashant Giridhar Shambharkar"}], "abstract": null}
{"paperId": "3cf8b2341e084b71964a6f989079a8bf973a4044", "url": "https://www.semanticscholar.org/paper/3cf8b2341e084b71964a6f989079a8bf973a4044", "title": "Dual-Branch Dynamic Perception and Interaction Framework for Aerial Vision-and-Language Navigation", "venue": "2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAID65275.2025.11034446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAID65275.2025.11034446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-04-25", "authors": [{"authorId": "2367428977", "name": "Zongmeng Wang"}], "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation aims to enable drones to autonomously navigate complex environments using natural language instructions. Existing approaches often depend on static visual cues or isolated directional signals, thereby neglecting the dynamic interactions between visual perception and directional guidance, which limits their ability to capture cross-modal correlations. To address this challenge, we propose a Dual-Branch Dynamic Perception and Interaction framework, dubbed DBDP. Specifically, a time-aware visual branch is designed to dynamically capture sequential scene features, while a spatial search directional branch robustly extracts and interacts with navigational cues. These modality-specific features are then fused via a joint embedding module with language representations, yielding enhanced action prediction accuracy. Experimental results and ablation studies demonstrate that the proposed DBDP framework significantly outperforms state-of-the-art methods in terms of navigation performance and interpretability."}
{"paperId": "3d3964a7541dd56620c0d5774527a41e890329e5", "url": "https://www.semanticscholar.org/paper/3d3964a7541dd56620c0d5774527a41e890329e5", "title": "WSCurLe: Weakly Supervised Curriculum Learning for Foundational Vision and Language Architectures in Digital Soil Mapping", "venue": "Proceedings of the Conference on Robots and Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21428/d82e957c.2c17961c?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21428/d82e957c.2c17961c, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2305055904", "name": "Vishvam Porwal"}, {"authorId": "2305063461", "name": "Stacey D. Scott"}, {"authorId": "2305053889", "name": "Neil D. B. Bruce"}, {"authorId": "2259353344", "name": "Asim Biswas"}], "abstract": null}
{"paperId": "3dc62ffb33ed1a0dcc1f11bce82028e7eeef4452", "url": "https://www.semanticscholar.org/paper/3dc62ffb33ed1a0dcc1f11bce82028e7eeef4452", "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-18", "authors": [{"authorId": "2334674032", "name": "Yu Zhong"}, {"authorId": "2303434266", "name": "Zihao Zhang"}, {"authorId": "2118404461", "name": "Rui Zhang"}, {"authorId": "2393364521", "name": "Lingdong Huang"}, {"authorId": "2393040488", "name": "Haihan Gao"}, {"authorId": "2334521492", "name": "Shuo Wang"}, {"authorId": "2393004962", "name": "Da Li"}, {"authorId": "2393163115", "name": "Ruijian Han"}, {"authorId": "47093626", "name": "Jiaming Guo"}, {"authorId": "2072713784", "name": "Shaohui Peng"}, {"authorId": "2110180755", "name": "Di Huang"}, {"authorId": "2261914661", "name": "Yunji Chen"}], "abstract": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs'generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks."}
{"paperId": "3e30b1b2bde8e3551b6bbff82117db9924f48c67", "url": "https://www.semanticscholar.org/paper/3e30b1b2bde8e3551b6bbff82117db9924f48c67", "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-17", "authors": [{"authorId": "2335854089", "name": "Menglan Chen"}, {"authorId": "2259929200", "name": "Xianghe Pang"}, {"authorId": "2191405383", "name": "Jingjing Dong"}, {"authorId": "2280328057", "name": "Wenhao Wang"}, {"authorId": "2273581205", "name": "Yaxin Du"}, {"authorId": "2275022100", "name": "Siheng Chen"}], "abstract": "Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59\\% increase in average safety across five models on the SIUO benchmark."}
{"paperId": "3e358730821c15edcfc28880799081ad8c3e1395", "url": "https://www.semanticscholar.org/paper/3e358730821c15edcfc28880799081ad8c3e1395", "title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 23, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.04959, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-07", "authors": [{"authorId": "2029559325", "name": "Daniel Marczak"}, {"authorId": "2220781605", "name": "Simone Magistri"}, {"authorId": "2310438074", "name": "Sebastian Cygert"}, {"authorId": "2470703", "name": "Bartłomiej Twardowski"}, {"authorId": "1749498", "name": "Andrew D. Bagdanov"}, {"authorId": "1834119810", "name": "J. Weijer"}], "abstract": "Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance on vision and language tasks across various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging ."}
{"paperId": "3ecf47a3f30fe37ff82ce9d809adccfd8accdd00", "url": "https://www.semanticscholar.org/paper/3ecf47a3f30fe37ff82ce9d809adccfd8accdd00", "title": "Exploring the Visual Perspective Taking of Vision-and-Language Model", "venue": "International Conference on Big Data and Smart Computing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigComp64353.2025.00084?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigComp64353.2025.00084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-09", "authors": [{"authorId": "2119566888", "name": "Young-Jun Lee"}, {"authorId": "2296409492", "name": "Ho-Jin Choi"}], "abstract": "Recently, many vision-and-language models have demonstrated remarkable performance in various multimodal tasks using a zero-shot approach. However, they lack spatial reasoning abilities, particularly in understanding visuo-spatial information from another’s perspective, a skill known as visual perspective taking. In this paper, we investigate whether vision-and-language models can develop visual perspective-taking abilities in a zero-shot manner. Through a simple experiment, we demonstrate that these models still struggle to see the world from another’s viewpoint, suggesting that vision-and-language models are hindered by an egocentric bias."}
{"paperId": "3f0f24f7f0e41467c445825540748ddbac8b4a7f", "url": "https://www.semanticscholar.org/paper/3f0f24f7f0e41467c445825540748ddbac8b4a7f", "title": "A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-15", "authors": [{"authorId": "2376156088", "name": "Puzhen Wu"}, {"authorId": "2376101480", "name": "Hexin Dong"}, {"authorId": "2394518993", "name": "Yi Lin"}, {"authorId": "2387425584", "name": "Yihao Ding"}, {"authorId": "2373512474", "name": "Yifan Peng"}], "abstract": "Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists'workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality."}
{"paperId": "3f4e1dba8aa5f53d6426c7514e5c4eed189116ba", "url": "https://www.semanticscholar.org/paper/3f4e1dba8aa5f53d6426c7514e5c4eed189116ba", "title": "Sex-specific and age-related progression of auditory neurophysiological deficits in the Cln3 mouse model of Batten disease", "venue": "bioRxiv", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12590631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-18", "authors": [{"authorId": "2351358547", "name": "Yanya Ding"}, {"authorId": "2352263707", "name": "Jingyu Feng"}, {"authorId": "121119524", "name": "V. Prifti"}, {"authorId": "2351267202", "name": "Grace A. Rico"}, {"authorId": "2351266934", "name": "Alexander G. Solorzano"}, {"authorId": "2351331604", "name": "Hayley E. Chang"}, {"authorId": "3340893", "name": "E. Freedman"}, {"authorId": "2220995095", "name": "John J. Foxe"}, {"authorId": "2299541334", "name": "Kuan Hong Wang"}], "abstract": "CLN3 disease is a prevalent form of Neuronal Ceroid Lipofuscinosis (NCL) caused by inherited mutations in the CLN3 gene, with symptoms such as vision loss, language impairment, and cognitive decline. The early onset of visual deficits complicates neurological assessment of brain pathophysiology underlying cognitive decline, while the small number of CLN3 mutation cases in humans hinders the study of sex differences. Building on our recent progress in assessing auditory neurophysiological changes in CLN3 patients, we developed a parallel approach using electroencephalography arrays in Cln3 knockout (Cln3-/-) mice to investigate the longitudinal progression of auditory processing deficits in both sexes. We employed a duration mismatch negativity (MMN) paradigm, similar to that used in our CLN3 patient studies, to assess the automatic detection of pattern changes in a sequence of stimuli. Wild-type mice of both sexes showed robust duration MMN responses when assessed longitudinally in the same subjects from 3 to 9 months of age. In contrast, female Cln3-/- mice developed consistent MMN deficits throughout this age range, while male Cln3-/- mice exhibited MMN deficits at younger ages that were mitigated at older ages. Analyses of auditory brainstem responses indicate that MMN abnormalities in Cln3-/- mice are not due to peripheral hearing loss. Instead, these deficits originate centrally from sex-specific and age-related changes in auditory evoked potentials elicited by standard and deviant stimuli. Our findings reveal a sex-specific progression of central auditory processing deficits in Cln3-/- mice, supporting auditory duration MMN as a translational neurophysiological biomarker for mechanistic studies and therapeutic development. Significance Statement CLN3 disease is an inherited neurodegenerative disorder with progressive decline in cognitive functioning and verbal abilities. The neuropathophysiological mechanisms underlying this decline remain poorly understood, highlighting the urgent need for objective neurological biomarkers to advance mechanistic insights and therapeutic development. Our identification of central auditory processing and change detection deficits in Cln3-/- mice, mirroring findings from our recent studies in CLN3 patients, validates auditory MMN as a translational neurophysiological biomarker bridging pre-clinical and clinical research. Moreover, our discovery of sex-specific, non-linear progression of MMN deficits emphasizes the necessity of developing disease management strategies tailored to each sex. This finding also provides a foundation for investigating both pathogenic and compensatory neural mechanisms to inform the development of individualized treatments."}
{"paperId": "3fbc92a9849650e2f2e998759b1daeba5f0e1b64", "url": "https://www.semanticscholar.org/paper/3fbc92a9849650e2f2e998759b1daeba5f0e1b64", "title": "Lightweight Deep Learning for Resource-Constrained Devices", "venue": "International Journal for Sciences and Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.71097/ijsat.v16.i2.6224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.71097/ijsat.v16.i2.6224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-06-22", "authors": [{"authorId": "2362131237", "name": "Dheeraj Vaddepally"}], "abstract": "With the increasing usage of deep learning models on mobile and resource-constrained hardware, efficient model design is necessary so that accuracy comes at the same cost as resource utilization. Several techniques for producing lightweight deep learning models for real-time inference on mobile hardware are reviewed in this paper. Three fundamental methods are pruned, quantized, and optimized; each reduces the complexity of the model but does not detract from the performance. The practical issues and trade-offs of applying these methods on devices with limited memory, power, and computational resources are analyzed. By concrete case studies and benchmark results, the ways in which these methods enable deep learning in real-world mobile applications, such as computer vision, natural language processing, and augmented reality, are recorded. The paper concludes with the discussion of emerging trends in lightweight model development and future research directions that may have a relation with these optimized models, driving innovations in mobile AI and edge computing."}
{"paperId": "4001be87e2dfb7f951bd31c6f1bdec8a62938614", "url": "https://www.semanticscholar.org/paper/4001be87e2dfb7f951bd31c6f1bdec8a62938614", "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-15", "authors": [{"authorId": "2353204753", "name": "Rulin Zhou"}, {"authorId": "2353210897", "name": "Wenlong He"}, {"authorId": "2352988372", "name": "An Wang"}, {"authorId": "2392905455", "name": "Jianhang Zhang"}, {"authorId": "2392905035", "name": "Xuanhui Zeng"}, {"authorId": "2333384589", "name": "Xi Zhang"}, {"authorId": "2392904381", "name": "Chaowei Zhu"}, {"authorId": "2352848953", "name": "Haijun Hu"}, {"authorId": "2274200621", "name": "Hongliang Ren"}], "abstract": "Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions."}
{"paperId": "4038ef8b24626a6d0b39d99011b4d8af60ae1e4d", "url": "https://www.semanticscholar.org/paper/4038ef8b24626a6d0b39d99011b4d8af60ae1e4d", "title": "Visual Semantic Contextualization Network for Multi-Query Image Retrieval", "venue": "IEEE transactions on multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2025.3590927?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2025.3590927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2099824277", "name": "Zhong Ji"}, {"authorId": "2387117657", "name": "Zhihao Li"}, {"authorId": "2152819985", "name": "Yan Zhang"}, {"authorId": "2241625814", "name": "Yanwei Pang"}, {"authorId": "2240224099", "name": "Xuelong Li"}], "abstract": null}
{"paperId": "40814f1d10a2b636b7d308e7ae4e9ce45d718793", "url": "https://www.semanticscholar.org/paper/40814f1d10a2b636b7d308e7ae4e9ce45d718793", "title": "A Navigation Framework Utilizing Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.10172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-11", "authors": [{"authorId": "2367109356", "name": "Yicheng Duan"}, {"authorId": "2366562039", "name": "Kaiyu tang"}], "abstract": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration."}
{"paperId": "40f4c3b9f2b79c6c479b525a9977e4ff9117f06b", "url": "https://www.semanticscholar.org/paper/40f4c3b9f2b79c6c479b525a9977e4ff9117f06b", "title": "Latent Space Semantic Alignment Network Based on Momentum Contrast for Image-Text Retrieval", "venue": "IEEE International Joint Conference on Neural Network", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN64981.2025.11227703?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN64981.2025.11227703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2299342333", "name": "B. Lu"}, {"authorId": "2392442925", "name": "Xiaojin Wu"}, {"authorId": "2392937099", "name": "Haibin Zhu"}, {"authorId": "2392427247", "name": "Tianyuan Zhong"}, {"authorId": "2392427411", "name": "Lin Gan"}, {"authorId": "2392449218", "name": "Ying Gao"}], "abstract": "Image-text retrieval is an important task in bridging vision and language, and its key challenge is to achieve an accurate and efficient semantic alignment between the two modalities. Although advancements have been achieved, there are still the following two limitations: (1) While current methods utilize self-attention to capture region-level relationships within samples, cross-sample and cross-modal relationships are often overlooked, making it difficult to align image and text semantics in a shared embedding space. (2)Image-text retrieval typically involves contrastive learning within batches containing positive and negative samples; however, the batch size, constrained by GPU limitations, limits the scale of negative samples and consequently the model’s learning capability. To address these problems, we propose a novel Latent Space Semantic Alignment Network(LSAN) based on momentum contrast for ITR. Specifically, we introduce a Latent Space Attention Alignment Module that replaces the original region-level self-attention with shared latent space attention, effectively identifying subtle semantic differences and semantic consistency across modalities. Concurrently, a cross-modal momentum contrastive framework, composed of a momentum encoder and a dynamic queue, decouples the number of negative samples from the batch size, which expands the quantity of negative samples. Extensive experiments conducted on Flickr30K and MSCOCO demonstrate that our LSAN not only outperforms existing state-of-the-art methods but also maintains efficient inference performance."}
{"paperId": "41d278b854bd318ba559635d7e3579b34c5c03e6", "url": "https://www.semanticscholar.org/paper/41d278b854bd318ba559635d7e3579b34c5c03e6", "title": "DWMGrad: an innovative neural network optimization approach using dynamic window data for adaptive updating of momentum and learning rate", "venue": "Applied intelligence (Boston)", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-025-06868-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-025-06868-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-01", "authors": [{"authorId": "2261258362", "name": "Zhifeng Wang"}, {"authorId": "2254063901", "name": "Longlong Li"}, {"authorId": "2169487804", "name": "Chunyan Zeng"}], "abstract": null}
{"paperId": "41fc1439afa37d0ddeb7f8b85056aa80de9c73ee", "url": "https://www.semanticscholar.org/paper/41fc1439afa37d0ddeb7f8b85056aa80de9c73ee", "title": "Layout-Independent License Plate Recognition via Integrated Vision and Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.10533, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-12", "authors": [{"authorId": "2268568982", "name": "Elham Shabaninia"}, {"authorId": "2268610225", "name": "Fatemeh Asadi-Zeydabadi"}, {"authorId": "2268569303", "name": "Hossein Nezamabadi-Pour"}], "abstract": "This work presents a pattern-aware framework for automatic license plate recognition (ALPR), designed to operate reliably across diverse plate layouts and challenging real-world conditions. The proposed system consists of a modern, high-precision detection network followed by a recognition stage that integrates a transformer-based vision model with an iterative language modelling mechanism. This unified recognition stage performs character identification and post-OCR refinement in a seamless process, learning the structural patterns and formatting rules specific to license plates without relying on explicit heuristic corrections or manual layout classification. Through this design, the system jointly optimizes visual and linguistic cues, enables iterative refinement to improve OCR accuracy under noise, distortion, and unconventional fonts, and achieves layout-independent recognition across multiple international datasets (IR-LPR, UFPR-ALPR, AOLP). Experimental results demonstrate superior accuracy and robustness compared to recent segmentation-free approaches, highlighting how embedding pattern analysis within the recognition stage bridges computer vision and language modelling for enhanced adaptability in intelligent transportation and surveillance applications."}
{"paperId": "4234b1e23468d2cdc7e542e98a573507f0c508d5", "url": "https://www.semanticscholar.org/paper/4234b1e23468d2cdc7e542e98a573507f0c508d5", "title": "Representation, Alignment, and Generation: A Comprehensive Survey of Foundation Models for Non-Invasive Brain Decoding", "venue": "bioRxiv", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.64898/2025.11.30.691403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.64898/2025.11.30.691403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review"], "publicationDate": "2025-12-02", "authors": [{"authorId": "2375281577", "name": "Yifan Wang"}, {"authorId": "2358046153", "name": "Shaonan Wang"}, {"authorId": "2116367925", "name": "Yunhao Zhang"}, {"authorId": "2394099273", "name": "Changde Du"}, {"authorId": "2298729994", "name": "Cunhang Fan"}, {"authorId": "2291117223", "name": "Dongyang Li"}, {"authorId": "2322404567", "name": "Hongpeng Zhou"}, {"authorId": "2395996668", "name": "Hongyu Zhang"}, {"authorId": "2265141238", "name": "Jixing Li"}, {"authorId": "2243578772", "name": "Quanying Liu"}, {"authorId": "2368263364", "name": "Wei Huang"}, {"authorId": "2396562404", "name": "Yizhuo Lu"}, {"authorId": "2283850394", "name": "Zijiao Chen"}, {"authorId": "2287155301", "name": "Jingyuan Sun"}], "abstract": null}
{"paperId": "425f54736c7f4c0fd3eed4f5d2c46e9f1525e863", "url": "https://www.semanticscholar.org/paper/425f54736c7f4c0fd3eed4f5d2c46e9f1525e863", "title": "Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models", "venue": "Proceedings of the First BabyLM Workshop", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "41071249", "name": "Ece Takmaz"}, {"authorId": "3280023", "name": "Lisa Bylinina"}, {"authorId": "2378706404", "name": "Jakub Dotlacil"}], "abstract": "State-of-the-art vision-and-language models consist of many parameters and learn from enormous datasets, surpassing the amounts of linguistic data that children are exposed to as they acquire a language. This paper presents our approach to the multimodal track of the BabyLM challenge addressing this discrepancy. We develop language-only and multimodal models in low-resource settings using developmentally plausible datasets, with our multimodal models outperforming previous BabyLM baselines. One finding in the multimodal language model literature is that these models tend to underperform in \\textit{language-only} tasks. Therefore, we focus on maintaining language-only abilities in multimodal models. To this end, we experiment with \\textit{model merging}, where we fuse the parameters of multimodal models with those of language-only models using weighted linear interpolation. Our results corroborate the findings that multimodal models underperform in language-only benchmarks that focus on grammar, and model merging with text-only models can help alleviate this problem to some extent, while maintaining multimodal performance."}
{"paperId": "428ecf50296a73e9bc95f94dcfb7e5655a1cd900", "url": "https://www.semanticscholar.org/paper/428ecf50296a73e9bc95f94dcfb7e5655a1cd900", "title": "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.01351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-01", "authors": [{"authorId": "2309786847", "name": "Chaoxiang Cai"}, {"authorId": "2309079190", "name": "Longrong Yang"}, {"authorId": "2312240258", "name": "Kaibing Chen"}, {"authorId": "2374459052", "name": "Fan Yang"}, {"authorId": "2309120827", "name": "Xi Li"}], "abstract": "The mixture-of-experts (MoE), which replaces dense models with sparse architectures, has gained attention in large vision-language models (LVLMs) for achieving comparable performance with fewer activated parameters. Existing MoE frameworks for LVLMs focus on token-to-expert routing (TER), encouraging different experts to specialize in processing distinct tokens. However, these frameworks often rely on the load balancing mechanism, overlooking the inherent distributional differences between vision and language. To this end, we propose a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER, tackling two challenges: (1) Distribution-aware router for modality-specific routing. We observe that language TER follows a uniform distribution, whereas vision TER exhibits a long-tailed distribution. This discrepancy necessitates distinct routing strategies tailored to each modality. (2) Enhancing expert activation for vision tail tokens. Recognizing the importance of vision tail tokens, we introduce an oversampling-like strategy by increasing the number of activated experts for these tokens. Experiments on extensive benchmarks validate the effectiveness of our approach."}
{"paperId": "42ecf5a7e6f2652ad638912456aa5f9ced25dabb", "url": "https://www.semanticscholar.org/paper/42ecf5a7e6f2652ad638912456aa5f9ced25dabb", "title": "CADFormer: Fine-Grained Cross-Modal Alignment and Decoding Transformer for Referring Remote Sensing Image Segmentation", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-30", "authors": [{"authorId": "2352989750", "name": "Maofu Liu"}, {"authorId": "2352929771", "name": "Xin Jiang"}, {"authorId": "2352989332", "name": "Xiaokang Zhang"}], "abstract": "Referring remote sensing image segmentation (RRSIS) is a challenging task, aiming to segment specific target objects in remote sensing images based on a given language expression. Existing RRSIS methods typically employ coarse-grained unidirectional alignment approaches to obtain multimodal features, and they often overlook the critical role of language features as contextual information during the decoding process. Consequently, these methods exhibit weak object-level correspondence between visual and language features, leading to incomplete or erroneous predicted masks, especially when handling complex expressions and intricate remote sensing image scenes. To address these challenges, we propose a fine-grained cross-modal alignment and decoding Transformer, CADFormer, for RRSIS. Specifically, we design a semantic mutual guidance alignment module (SMGAM) to achieve both vision-to-language and language-to-vision alignment, enabling comprehensive integration of visual and textual features for fine-grained cross-modal alignment. Furthermore, a textual-enhanced cross-modal decoder (TCMD) is introduced to incorporate language features during decoding, using refined textual information as context to enhance the relationship between cross-modal features. To thoroughly evaluate the performance of CADFormer, especially for inconspicuous targets in complex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which includes larger high-resolution remote sensing image patches and semantically richer language expressions. Extensive experiments on the RRSIS-HR dataset and the popular RRSIS-D dataset demonstrate the effectiveness and superiority of CADFormer."}
{"paperId": "430abf498205f781657e32663e87d4907871f929", "url": "https://www.semanticscholar.org/paper/430abf498205f781657e32663e87d4907871f929", "title": "Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-14", "authors": [{"authorId": "2363742802", "name": "Feifan Wang"}, {"authorId": "2356584861", "name": "Tengfei Song"}, {"authorId": "2319973253", "name": "Minggui He"}, {"authorId": "2363401246", "name": "Chang Su"}, {"authorId": "2109666645", "name": "Zhanglin Wu"}, {"authorId": "2356568174", "name": "Hao Yang"}, {"authorId": "2329138232", "name": "Wenming Zheng"}, {"authorId": "2316862714", "name": "Osamu Yoshie"}], "abstract": "Facial emotion perception in the vision large language model (VLLM) is crucial for achieving natural human-machine interaction. However, creating high-quality annotations for both coarse- and fine-grained facial emotion analysis demands costly expertise. The lack of such high-quality instruction data limits the performance of VLLMs in facial emotion perception. To address this, we propose a self-verification approach with emotion knowledge enhancement (SEKE), which generates high-quality instruction data for multi-grained emotion analysis cost-effectively using closed-source VLLM. This approach integrates prior human knowledge to VLLM inference, guided by the inherent correlations between three grained levels of emotion descriptions, i.e., discrete expression, valence-arousal, and action unit, to reliably generate comprehensive annotations. A self-verification strategy with Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to efficiently extract more accurate VLLM predictions, further improving annotation reliability. Consequently, we construct a facial emotion instruction dataset (FEID) containing three comprehensive descriptions, which provides coarse- and fine-grained emotional information for effective model training. Additionally, we introduce a facial emotion analysis benchmark (FEAB) to measure the VLLM's corresponding ability. Our method significantly outperforms state-of-the-art methods on three downstream facial emotion analysis tasks."}
{"paperId": "434288b7a1f8e6037a7b3721f1e21b4ae910be9b", "url": "https://www.semanticscholar.org/paper/434288b7a1f8e6037a7b3721f1e21b4ae910be9b", "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.14749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-18", "authors": [{"authorId": "2215236206", "name": "Alexander Vedernikov"}, {"authorId": "2291199974", "name": "Puneet Kumar"}, {"authorId": "2296975856", "name": "Haoyu Chen"}, {"authorId": "2276780641", "name": "Tapio Seppanen"}, {"authorId": "2266423066", "name": "Xiaobai Li"}], "abstract": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06."}
{"paperId": "4389c10e2e6605b91c21ef0f918d22e7879aa0c7", "url": "https://www.semanticscholar.org/paper/4389c10e2e6605b91c21ef0f918d22e7879aa0c7", "title": "A 28nm 0.22μJ/Token Memory-Compute-Intensity-Aware CNN-Transformer Accelerator with Hybrid-Attention-Based Layer-Fusion and Cascaded Pruning for Semantic-Segmentation", "venue": "IEEE International Solid-State Circuits Conference", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISSCC49661.2025.10904499?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISSCC49661.2025.10904499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-16", "authors": [{"authorId": "2261493190", "name": "Pingcheng Dong"}, {"authorId": "2293880867", "name": "Yonghao Tan"}, {"authorId": "2309823091", "name": "Xuejiao Liu"}, {"authorId": "2293722914", "name": "Peng Luo"}, {"authorId": "2327496942", "name": "Yu Liu"}, {"authorId": "2187525192", "name": "Luhong Liang"}, {"authorId": "2365966228", "name": "Yitong Zhou"}, {"authorId": "2348937858", "name": "Di Pang"}, {"authorId": "2309513302", "name": "Manto Yung"}, {"authorId": "2281018802", "name": "Dong Zhang"}, {"authorId": "2261688809", "name": "Xijie Huang"}, {"authorId": "2220637583", "name": "Shih-Yang Liu"}, {"authorId": "2349338115", "name": "Yongkun Wu"}, {"authorId": "2241571670", "name": "Fengshi Tian"}, {"authorId": "2241621494", "name": "Chi-Ying Tsui"}, {"authorId": "2241612487", "name": "Fengbin Tu"}, {"authorId": "2241569932", "name": "Kwang-Ting Cheng"}], "abstract": "Recently, hybrid models integrating a CNN and a Transformer (ConvFormer), shown in Fig. 23.2.1, have achieved significant advancements in semantic segmentation tasks [1]–[4], which are critical for autonomous driving and embodied intelligence. The CNN enhances the multi-scale feature extraction ability of the Transformer to achieve pixel-level classification, but the large token length (TL) demand of semantic segmentation (> 16K TL) incurs significant computation and memory overheads. Prior NN accelerators [5]–[12] demonstrate that sparse computing and pruning can effectively reduce computation and weight storage, but most of them focus on pure CNN or Transformer models in simpler vision or language-processing tasks (1-4K TL). Moreover, the performance bottlenecks of ConvFormers stem from their memory-intensive Backbone and compute-intensive Segmentation Head (Seg. Head), raising three challenges for hardware acceleration: 1) Conventional sparse attention [5]–[9] fails to buffer the attention feature map (Fmap) on-chip when the TL exceeds 16K, even at 90% sparsity, resulting in massive external memory access (EMA). 2) While Layer-Fusion (LF) [13]–[18] is a common technique to reduce Fmap EMA, it is infeasible to buffer key (K), value (V), and convolution weights on-chip simultaneously. Moreover, different fused attention-convolution layers may cover various vanilla attention (VA) tiles, leading to enormous redundant KV and weight EMA. 3) In the Seg. Head, the Fmap sparsity is extremely low, thereby limiting the effectiveness of conventional zero-skipping strategies [11], [12] designed to reduce computational work."}
{"paperId": "43d6546112a4f93de771145c4107a5ce19a5bc86", "url": "https://www.semanticscholar.org/paper/43d6546112a4f93de771145c4107a5ce19a5bc86", "title": "Beyond-Labels: Advancing Open-Vocabulary Segmentation with Vision-Language Models", "venue": "International Conference on Advanced Computational Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.16769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-01-27", "authors": [{"authorId": "2347550570", "name": "Muhammad Atta-ur-Rahman"}], "abstract": "Open-vocabulary semantic segmentation attempts to classify and outline objects in an image using arbitrary text labels, including those unseen during training. Self-supervised learning resolves numerous visual and linguistic processing problems when effectively trained. This study investigates simple yet efficient methods for adapting previously learned foundation models for open-vocabulary semantic segmentation tasks. Our research proposes “Beyond-Labels,” a lightweight transformer-based fusion module that uses a handful of image segmentation data to fuse frozen visual representations with language concepts. This strategy allows the model to successfully actualize enormous knowledge from pre-trained models without requiring extensive retraining, making the model data-efficient and scalable. Furthermore, we efficiently capture positional information in images using Fourier embeddings, thus improving the generalization and resulting in smooth and consistent spatial encoding. We perform thorough ablation studies to investigate the major components of our proposed method in comparison to the standard benchmark PASCAL-5i, the method performs better despite being trained on frozen vision and language characteristics."}
{"paperId": "43f766c034e12bc4fdc25be5c3fb7064e5e54e2b", "url": "https://www.semanticscholar.org/paper/43f766c034e12bc4fdc25be5c3fb7064e5e54e2b", "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.06496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-20", "authors": [{"authorId": "2329564308", "name": "Rakesh Raj Madavan"}, {"authorId": "2329555644", "name": "Akshat Kaimal"}, {"authorId": "2375389040", "name": "Hashim Faisal"}, {"authorId": "2043632514", "name": "Chandrakala Shanmuganathan"}], "abstract": "An ensemble of trained multimodal encoders and vision-language models (VLMs) has become a standard approach for visual question answering (VQA) tasks. However, such models often fail to produce responses with the detailed precision necessary for complex, domain-specific applications such as medical VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding, extends prior multimodal work by refining the joint embedding space through dense, query-token-based encodings inspired by contrastive pretraining techniques. This refined encoder powers Med-GRIM, a model designed for medical VQA tasks that leverages graph-based retrieval and prompt engineering to integrate domain-specific knowledge. Rather than relying on compute-heavy fine-tuning of vision and language models on specific datasets, Med-GRIM applies a low-compute, modular workflow with small language models (SLMs) for efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject relevant knowledge, ensuring both accuracy and robustness in its responses. By assigning distinct roles to each agent within the VQA system, Med-GRIM achieves large language model performance at a fraction of the computational cost. Additionally, to support scalable research in zero-shot multimodal medical applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising diverse dermatological conditions. This dataset facilitates both multimodal and unimodal querying. The code and dataset are available at: https://github.com/Rakesh-123-cryp/Med-GRIM.git"}
{"paperId": "4429adb028869ccdb327f01eace14c0385adc1ee", "url": "https://www.semanticscholar.org/paper/4429adb028869ccdb327f01eace14c0385adc1ee", "title": "Fine-Tuning Vision-Language Models for Visual Navigation Assistance", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.07488, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-09", "authors": [{"authorId": "2380030207", "name": "Xiao Li"}, {"authorId": "2379760406", "name": "Bharat Gandhi"}, {"authorId": "2379760798", "name": "Ming Zhan"}, {"authorId": "2379760377", "name": "Mohit Nehra"}, {"authorId": "2367310636", "name": "Zhicheng Zhang"}, {"authorId": "2379895045", "name": "Yuchen Sun"}, {"authorId": "2383823440", "name": "Meijia Song"}, {"authorId": "2380521059", "name": "Naisheng Zhang"}, {"authorId": "2380421979", "name": "Xi Wang"}], "abstract": "We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model."}
{"paperId": "44401a74f6254cba92bea7eafdaed03714f9da9c", "url": "https://www.semanticscholar.org/paper/44401a74f6254cba92bea7eafdaed03714f9da9c", "title": "BMIP: Bi-directional Modality Interaction Prompt Learning for VLM", "venue": "International Joint Conference on Artificial Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.07769, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-01-14", "authors": [{"authorId": "2340193000", "name": "Song-Lin Lv"}, {"authorId": "2340140803", "name": "Yu-Yang Chen"}, {"authorId": "2340049285", "name": "Zhi Zhou"}, {"authorId": "2304465090", "name": "Ming Yang"}, {"authorId": "2340176599", "name": "Lan-Zhe Guo"}], "abstract": "Vision-language models (VLMs) have exhibited remarkable generalization capabilities, and prompt learning for VLMs has attracted great attention for the ability to adapt pre-trained VLMs to specific downstream tasks. However, existing studies mainly focus on single-modal prompts or uni-directional modality interaction, overlooking the powerful alignment effects resulting from the interaction between the vision and language modalities. To this end, we propose a novel prompt learning method called Bi-directional Modality Interaction Prompt (BMIP), which dynamically weights bi-modal information through learning the information of the attention layer, enhancing trainability and inter-modal consistency compared to simple information aggregation methods. To evaluate the effectiveness of prompt learning methods, we propose a more realistic evaluation paradigm called open-world generalization complementing the widely adopted cross-dataset transfer and domain generalization tasks. Comprehensive experiments on various datasets reveal that BMIP not only outperforms current state-of-the-art methods across all three evaluation paradigms but is also flexible enough to be combined with other prompt-based methods for consistent performance enhancement."}
{"paperId": "44495e33ab2492f7fb2795bcbcfbb1fe096afd39", "url": "https://www.semanticscholar.org/paper/44495e33ab2492f7fb2795bcbcfbb1fe096afd39", "title": "Large Vision-Language Models with PEFT for Generating Descriptive Annotations in Person Re-Identification", "venue": "International Conference on Human System Interaction", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/HSI66212.2025.11142385?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/HSI66212.2025.11142385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-07-16", "authors": [{"authorId": "2065460907", "name": "Ge Cao"}, {"authorId": "2261444800", "name": "Qing Tang"}, {"authorId": "2176891299", "name": "Adri Priadana"}, {"authorId": "2312046839", "name": "Tran Tien Dat"}, {"authorId": "2312049485", "name": "Ashraf Uddin Russo"}, {"authorId": "2261436646", "name": "Kanghyun Jo"}], "abstract": "Generating descriptive annotations for person re-identification (Re-ID) images is essential for bridging vision and language domain, improving both interpretability and cross-modal retrieval performance. However, large vision-language models (LVLMs), which trained on broad web-scale corpora, often struggle to generate accurate, context-relevant descriptions for ReID samples due to inherent challenges such as occlusions, low resolution, varying illumination, and diverse viewpoints. In this paper, we propose to apply Parameter-Efficient Fine-Tuning (PEFT) via Low-Rank Adaptation (LoRA) to tune Qwen2-VL for ReID-specific captioning tasks. Leveraging existing Re-ID datasets with paired image-text annotations, our fine-tuned model generates domain-aligned and discriminative captions. Experiments show significant improvements in caption relevance and identity descriptiveness, highlighting the potential of PEFT-tuned LVLMs for real-world ReID applications."}
{"paperId": "445f48d765f3227d8b82a07fd7f916f8fb19ca8c", "url": "https://www.semanticscholar.org/paper/445f48d765f3227d8b82a07fd7f916f8fb19ca8c", "title": "Analysis of Argumentation in Surah Ad-Duha Based on Olivier Reboul's Argumentative Model", "venue": "Journal of the College of languages", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36586/jcl.2.2025.0.51.00019?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36586/jcl.2.2025.0.51.00019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-02", "authors": [{"authorId": "2338840915", "name": "Alibaqir Taherinia"}, {"authorId": "2338846510", "name": "Masuma Taqi zadeh"}, {"authorId": "2338847764", "name": "Zahra fallahnejad"}], "abstract": "Influencing others and persuasion in human dialogues is an active and prominent element. Thus, argumentation is one of the distinctive aspects of these dialogues. In other words, argumentation is one of the uses of language to persuade the audience. In this context, rhetoric plays a crucial role in the structure of discourses as it is linked to the use of language and styles related to the situation and conditions of sentence formation. From this perspective, rhetoric is not merely a tool for producing texts but rather one of the important tools for creating a horizon of vision for language and various discourse issues. By examining stylistic phenomena and studying their relationship with different levels of perception, the researcher in the field of rhetoric discovers the hidden psychological, cultural, and social layers of the text. One of the approaches to the analysis of argumentation is Olivier Reboul's rhetorical argumentation approach, which considers the rhetorical aspect in any discourse as something that makes that discourse persuasive, and this occurs through the unity of form and content. In this article, we aim to analyze the different levels of Surah Ad-Duha according to Reboul's model. The research findings show that in Surah Ad-Duha, all three pictorial sections of Reboul's model have been employed in line with the argumentative goal of the surah. At the phonetic level, there is a close relationship between sounds and connotations, which can be observed in the rhymed endings of the surah. In the structural images section, the use of elements such as ellipsis, oath, emphasis, and fronting and postponement has successfully contributed to achieving the argumentative goal of the surah.\n\n"}
{"paperId": "4463ade46212ccf40ddc2c8c2927e3864dbd3ceb", "url": "https://www.semanticscholar.org/paper/4463ade46212ccf40ddc2c8c2927e3864dbd3ceb", "title": "VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-16", "authors": [{"authorId": "2348362749", "name": "Ziyang Zhang"}, {"authorId": "2348771719", "name": "Yang Yu"}, {"authorId": "2376264976", "name": "Xulei Yang"}, {"authorId": "2348281539", "name": "Si Yong Yeo"}], "abstract": "Vision-and-language models (VLMs) have been increasingly explored in the medical domain, particularly following the success of CLIP in general domain. However, unlike the relatively straightforward pairing of 2D images and text, curating large-scale paired data in the medical field for volumetric modalities such as CT scans remains a challenging and time-intensive process. This difficulty often limits the performance on downstream tasks. To address these challenges, we propose a novel vision-language pre-training (VLP) framework, termed as \\textbf{VELVET-Med}, specifically designed for limited volumetric data such as 3D CT and associated radiology reports. Instead of relying on large-scale data collection, our method focuses on the development of effective pre-training objectives and model architectures. The key contributions are: 1) We incorporate uni-modal self-supervised learning into VLP framework, which are often underexplored in the existing literature. 2) We propose a novel language encoder, termed as \\textbf{TriBERT}, for learning multi-level textual semantics. 3) We devise the hierarchical contrastive learning to capture multi-level vision-language correspondence. Using only 38,875 scan-report pairs, our approach seeks to uncover rich spatial and semantic relationships embedded in volumetric medical images and corresponding clinical narratives, thereby enhancing the generalization ability of the learned encoders. The resulting encoders exhibit strong transferability, achieving state-of-the-art performance across a wide range of downstream tasks, including 3D segmentation, cross-modal retrieval, visual question answering, and report generation."}
{"paperId": "44c747477c34e561618c9a5b05cf5cc13c54918b", "url": "https://www.semanticscholar.org/paper/44c747477c34e561618c9a5b05cf5cc13c54918b", "title": "Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2098828412", "name": "Alexa R. Tartaglini"}, {"authorId": "2387866712", "name": "Satchel Grant"}, {"authorId": "2220218920", "name": "Daniel Wurgaft"}, {"authorId": "2313685355", "name": "Christopher Potts"}, {"authorId": "2271393528", "name": "J. E. Fan"}], "abstract": "Data visualizations are vital components of many scientific articles and news stories. Current vision-language models (VLMs) still struggle on basic data visualization understanding tasks, but the causes of failure remain unclear. Are VLM failures attributable to limitations in how visual information in the data visualization is encoded, how information is transferred between the vision and language modules, or how information is processed within the language module? We developed FUGU, a suite of data visualization understanding tasks, to precisely characterize potential sources of difficulty (e.g., extracting the position of data points, distances between them, and other summary statistics). We used FUGU to investigate three widely used VLMs. To diagnose the sources of errors produced by these models, we used activation patching and linear probes to trace information flow through models across a variety of prompting strategies. We found that some models fail to generate the coordinates of individual data points correctly, and these initial errors often lead to erroneous final responses. When these models are provided with the correct coordinates, performance improves substantially. Moreover, even when the model generates an incorrect response, the correct coordinates can be successfully read out from the latent representations in the vision encoder, suggesting that the source of these errors lies in the vision-language handoff. We further found that while providing correct coordinates helps with tasks involving one or a small number of data points, it generally worsens performance for tasks that require extracting statistical relationships across many data points. Fine-tuning models on FUGU also fails to yield ceiling performance. These findings point to architectural constraints in current VLMs that might pose significant challenges for reliable data visualization understanding."}
{"paperId": "456769cf79a6c8f4d76da13ef506b8a683b2b32b", "url": "https://www.semanticscholar.org/paper/456769cf79a6c8f4d76da13ef506b8a683b2b32b", "title": "Visual and Textual Prompts in VLLMs for Enhancing Emotion Recognition", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.17224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-23", "authors": [{"authorId": "2282308992", "name": "Zhifeng Wang"}, {"authorId": "2284403091", "name": "Qixuan Zhang"}, {"authorId": "2357115702", "name": "Peter Zhang"}, {"authorId": "2324109008", "name": "Wenjia Niu"}, {"authorId": "2282385332", "name": "Kaihao Zhang"}, {"authorId": "2212747704", "name": "Ramesh S. Sankaranarayana"}, {"authorId": "9004970", "name": "Sabrina Caldwell"}, {"authorId": "2288840433", "name": "Tom Gedeon"}], "abstract": "Vision Large Language Models (VLLMs) exhibit promising potential for multi-modal understanding, yet their application to video-based emotion recognition remains limited by insufficient spatial and contextual awareness. Traditional approaches, which prioritize isolated facial features, often neglect critical non-verbal cues such as body language, environmental context, and social interactions, leading to reduced robustness in real-world scenarios. To address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel framework that enhances zero-shot emotion recognition by integrating spatial annotations (e.g., bounding boxes, facial landmarks), physiological signals (facial action units), and contextual cues (body posture, scene dynamics, others’ emotions) into a unified prompting strategy. SoVTP preserves holistic scene information while enabling fine-grained analysis of facial muscle movements and interpersonal dynamics. Extensive experiments show that SoVTP achieves substantial improvements over existing visual prompting methods, demonstrating its effectiveness in enhancing VLLMs’ video emotion recognition capabilities."}
{"paperId": "45dd2a918af315bae1da28fdeb9e1f5e1081f789", "url": "https://www.semanticscholar.org/paper/45dd2a918af315bae1da28fdeb9e1f5e1081f789", "title": "VASSO: Variance Suppression for Sharpness-Aware Minimization", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.02433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-02", "authors": [{"authorId": "2145726951", "name": "Bingcong Li"}, {"authorId": "1591119505", "name": "Yilang Zhang"}, {"authorId": "2263029833", "name": "Georgios B. Giannakis"}], "abstract": "Sharpness-aware minimization (SAM) has well-documented merits in enhancing generalization of deep neural network models. Accounting for sharpness in the loss function geometry, where neighborhoods of `flat minima'heighten generalization ability, SAM seeks `flat valleys'by minimizing the maximum loss provoked by an adversarial perturbation within the neighborhood. Although critical to account for sharpness of the loss function, in practice SAM suffers from `over-friendly adversaries,'which can curtail the outmost level of generalization. To avoid such `friendliness,'the present contribution fosters stabilization of adversaries through variance suppression (VASSO). VASSO offers a general approach to provably stabilize adversaries. In particular, when integrating VASSO with SAM, improved generalizability is numerically validated on extensive vision and language tasks. Once applied on top of a computationally efficient SAM variant, VASSO offers a desirable generalization-computation tradeoff."}
{"paperId": "45f73f709563914661b9cc6d4977fea76c8ccf21", "url": "https://www.semanticscholar.org/paper/45f73f709563914661b9cc6d4977fea76c8ccf21", "title": "Redefining oral healthcare through artificial intelligence: a review of current applications and a roadmap for the future of dentistry", "venue": "BMC Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s44398-025-00013-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s44398-025-00013-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2295551643", "name": "Bharti Dua"}, {"authorId": "7841741", "name": "Rajiv Kumar Gupta"}, {"authorId": "2295551755", "name": "Akshay Bhargava"}, {"authorId": "2390250816", "name": "Anupam Bhardwaj"}, {"authorId": "2390263665", "name": "Meena Jain"}, {"authorId": "2390251216", "name": "Siddhi Tripathi"}], "abstract": null}
{"paperId": "465281b31a17157960f59b7f7c8c9ede5216f096", "url": "https://www.semanticscholar.org/paper/465281b31a17157960f59b7f7c8c9ede5216f096", "title": "Image Recognition with Vision and Language Embeddings of VLMs", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-11", "authors": [{"authorId": "2333362908", "name": "Illia Volkov"}, {"authorId": "2333361321", "name": "Nikita Kisel"}, {"authorId": "2091960316", "name": "Klára Janousková"}, {"authorId": "2332097439", "name": "Jirí Matas"}], "abstract": "Vision-language models (VLMs) have enabled strong zero-shot classification through image-text alignment. Yet, their purely visual inference capabilities remain under-explored. In this work, we conduct a comprehensive evaluation of both language-guided and vision-only image classification with a diverse set of dual-encoder VLMs, including both well-established and recent models such as SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the ImageNet-1k validation set and its label-corrected variant. The key factors affecting accuracy are analysed, including prompt design, class diversity, the number of neighbours in k-NN, and reference set size. We show that language and vision offer complementary strengths, with some classes favouring textual prompts and others better handled by visual similarity. To exploit this complementarity, we introduce a simple, learning-free fusion method based on per-class precision that improves classification performance. The code is available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition."}
{"paperId": "4762a974a6326279ecc355d294b56b3928642c44", "url": "https://www.semanticscholar.org/paper/4762a974a6326279ecc355d294b56b3928642c44", "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-02", "authors": [{"authorId": "2261324984", "name": "Zongxia Li"}, {"authorId": "2338862615", "name": "Xiyang Wu"}, {"authorId": "2340214930", "name": "Guangyao Shi"}, {"authorId": "2362647556", "name": "Yubin Qin"}, {"authorId": "2339261614", "name": "Hongyang Du"}, {"authorId": "52220309", "name": "Fuxiao Liu"}, {"authorId": "2268389308", "name": "Tianyi Zhou"}, {"authorId": "2256714387", "name": "Dinesh Manocha"}, {"authorId": "2240779865", "name": "J. Boyd-Graber"}], "abstract": "Vision-Language Models (VLMs) have achieved strong results in video understanding, yet a key question remains: do they truly comprehend visual content or only learn shallow correlations between vision and language? Real visual understanding, especially of physics and common sense, is essential for AI systems that interact with the physical world. Current evaluations mostly use real-world videos similar to training data, so high benchmark scores may not reflect real reasoning ability. To address this, we propose negative-control tests using videos that depict physically impossible or logically inconsistent events. We introduce VideoHallu, a synthetic dataset of physics- and commonsense-violating scenes generated with Veo2, Sora, and Kling. It includes expert-annotated question-answer pairs across four categories of violations. Tests of leading VLMs (Qwen-2.5-VL, Video-R1, VideoChat-R1) show that, despite strong results on benchmarks such as MVBench and MMVU, they often miss these violations, exposing gaps in visual reasoning. Reinforcement learning fine-tuning on VideoHallu improves recognition of such violations without reducing standard benchmark performance. Our data is available at https://github.com/zli12321/VideoHallu.git."}
{"paperId": "476bb6beb9a27e61d36654484cca2f451f6ee266", "url": "https://www.semanticscholar.org/paper/476bb6beb9a27e61d36654484cca2f451f6ee266", "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.07406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-10", "authors": [{"authorId": "2349269580", "name": "Xiaobei Zhao"}, {"authorId": "2375388873", "name": "Xingqi Lyu"}, {"authorId": "2349772700", "name": "Xiang Li"}], "abstract": "Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain."}
{"paperId": "478aa3417dc58bf5c5bf2d57b700a0904048908b", "url": "https://www.semanticscholar.org/paper/478aa3417dc58bf5c5bf2d57b700a0904048908b", "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.00288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-08-01", "authors": [{"authorId": "2375133698", "name": "Jianqiang Xiao"}, {"authorId": "2374424985", "name": "Yuexuan Sun"}, {"authorId": "2374431055", "name": "Yixin Shao"}, {"authorId": "2374411806", "name": "Boxi Gan"}, {"authorId": "2374961636", "name": "Rongqiang Liu"}, {"authorId": "2388478184", "name": "Yanjin Wu"}, {"authorId": "2356687233", "name": "Weili Guan"}, {"authorId": "2375179563", "name": "Xiang Deng"}], "abstract": "Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA)-a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments. Our benchmark and code are available at: https://github.com/Kyaren/UAV_ON."}
{"paperId": "47cc4e7affd1556d547465be2a00c786b2cd2be1", "url": "https://www.semanticscholar.org/paper/47cc4e7affd1556d547465be2a00c786b2cd2be1", "title": "Multi-Modal Cooperative Distillation for Zero-Shot Multi-Label Classification", "venue": "International Conference on Image, Vision and Computing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIVC66358.2025.11200368?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIVC66358.2025.11200368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-16", "authors": [{"authorId": "2357220046", "name": "Yiqin Wang"}, {"authorId": "2305809323", "name": "Ying Chen"}], "abstract": "Considering that real-world scenarios often contain multiple objects and unseen labels, the multi-label classification task is highly significant. The current state-of-the-art multilabel classification method is based on open vocabulary, where it utilizes the image encoder of a Vision-and-Language Pretrained(VLP) model as the teacher to train a vision transformer, and employs prompt tuning for the text encoder. This approach of training image and text encoders separately loses knowledge sharing between the two modalities during the training process, resulting in a lack of semantic consistency between image and text in VLP. To tackle the problem, an image-text cooperative framework for zero-shot multi-label classification is proposed, in which the image-text inherence in VLP can be better preserved and conveyed by constructing an image-text interactive space. The proposed approach achieves the state-of-the-art performance on both the multi-label zero-shot learning(ML-ZSL) task and the generalized zero-shot learning(ML-GZSL) task. Experiments show the proposed method improves the mAP over the SOTA baseline by $\\mathbf{2. 3 \\%}$ for the ML-ZSL tasks and $\\mathbf{2. 5 \\%}$ for the ML-GZSL tasks."}
{"paperId": "48282ae6f1e1e389d32da917bcb5169729ee66ca", "url": "https://www.semanticscholar.org/paper/48282ae6f1e1e389d32da917bcb5169729ee66ca", "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.12622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-14", "authors": [{"authorId": "2332527018", "name": "Zihan Wang"}, {"authorId": "2294600666", "name": "Seungjun Lee"}, {"authorId": "2398809245", "name": "Guangzhao Dai"}, {"authorId": "2332479307", "name": "Gim Hee Lee"}], "abstract": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness."}
{"paperId": "4839b19f93388786ca6a5bef0d1bd79a75e9319f", "url": "https://www.semanticscholar.org/paper/4839b19f93388786ca6a5bef0d1bd79a75e9319f", "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.13328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-17", "authors": [{"authorId": "2371113164", "name": "Yulu Qin"}, {"authorId": "2372809593", "name": "Dheeraj Varghese"}, {"authorId": "2215215844", "name": "Adam Dahlgren Lindstrom"}, {"authorId": "2373393769", "name": "Lucia Donatelli"}, {"authorId": "145274478", "name": "Kanishka Misra"}, {"authorId": "8756748", "name": "Najoung Kim"}], "abstract": "Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic."}
{"paperId": "48574c965ba2fe9bb2e8b054b5ebab3a02161fd1", "url": "https://www.semanticscholar.org/paper/48574c965ba2fe9bb2e8b054b5ebab3a02161fd1", "title": "Dynamic Pseudo-Labeling via Large Language Models for Robust Medical Image Segmentation", "venue": "2025 6th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICBASE66587.2025.11181232?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICBASE66587.2025.11181232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-18", "authors": [{"authorId": "2385449585", "name": "Zhiyan Tang"}], "abstract": "In the field of medical image segmentation, the high cost of annotated data and the poor generalization performance of the model have always been problems that need to be solved urgently. In this study, we innovatively propose a cross-modal collaboration framework based on large language model (LLM), MedSegLLM, which aims to overcome the two major difficulties of weakly supervised learning and domain adaptation at the same time, and open up an efficient and interpretable new path for medical image analysis: 1) Cross-modal semantic alignment network: Through a unique mapping strategy, medical image features and accurate text descriptions generated by LLM are projected into the same semantic space, cleverly bridging the understanding gap between vision and language, and achieving deep semantic integration across modalities. 2) Dynamic pseudolabel generator: deeply excavate the powerful logical reasoning potential of LLM, intelligently generate high-credibility segmentation masks, provide an effective way to break the dilemma of a lack of annotated data, and the generation process has adaptive optimization capabilities. 3) Adaptive feature fusion module: According to the image differences generated by different medical devices, it intelligently integrates multi-source feature information, comprehensively improves the generalization and adaptability of models across devices and scenarios, and ensures the stability of clinical applications. MedSegLLM has been experimentally proven to perform well on BraTS (Brain Tumor Segmentation Benchmark), ISIC (Dermoscopy Image Dataset) and proprietary CT datasets. In particular, under the severe conditions of low annotation data rate (only $10 \\%$), the Dice coefficient is still as high as 0.91, which is significantly higher than that of traditional strongly supervised U-Net (0.85) and advanced TransUNet (0.88) ($\\mathbf{p}\\lt \\mathbf{0 . 0 1}$). The cross-device generalization error of the framework is reduced by $32 \\%$ compared with the existing schemes, which fully demonstrates its adaptability and robustness in complex medical environments(The model can maintain stable and high - accuracy segmentation performance in complex conditions without significant drops due to environmental changes or data - quality issues). Even in low-resource hospitals (less than $10 \\%$ of labeled data) and emergency diagnosis scenarios, it can still maintain high-precision output, which is of great clinical practical value. It will provide a strong boost for the inclusive development of global medical imaging technology, and is expected to accelerate the popularization of precision medicine in medical institutions at all levels."}
{"paperId": "4883c4c661e7ddf54ec2c0747be7742d7bc947ff", "url": "https://www.semanticscholar.org/paper/4883c4c661e7ddf54ec2c0747be7742d7bc947ff", "title": "Expand VSR Benchmark for VLLM to Expertize in Spatial Rules", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i8.32945?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i8.32945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2320452024", "name": "Peijin Xie"}, {"authorId": "2346059467", "name": "Lin Sun"}, {"authorId": "2320575584", "name": "Bingquan Liu"}, {"authorId": "2261981209", "name": "Dexin Wang"}, {"authorId": "2261751811", "name": "Xiangzheng Zhang"}, {"authorId": "2261841509", "name": "Chengjie Sun"}, {"authorId": "2337087116", "name": "Jiajia Zhang"}], "abstract": "Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. \nAlthough benchmarks like MME, MMBench and SEED comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR).\nThere is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. \nTo handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set.\nWe found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information.\nBy expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. \nTo our knowledge, we expanded spatially positioned image data controllably using diffusion models for the first time and integrated original visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and DINO).\nAfter conducting combination experiments on scaling data and models, we obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different instructions but also accurately distinguishes differences in visual positional information. \nVSRE achieved over a 27% increase in accuracy on the VSR test set. \nIt becomes a performant VLLM on the position reasoning of both the VSR dataset and relevant subsets of other evaluation benchmarks. \nWe hope it will accelerate advancements in VLLM on VSR learning."}
{"paperId": "48a01cc8023eae500a9d9567fe62483bfe0d9c9e", "url": "https://www.semanticscholar.org/paper/48a01cc8023eae500a9d9567fe62483bfe0d9c9e", "title": "KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.10011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-14", "authors": [{"authorId": "82903771", "name": "Edgar Anarossi"}, {"authorId": "1557600541", "name": "Yuhwan Kwon"}, {"authorId": "67347124", "name": "Hirotaka Tahara"}, {"authorId": "2111425352", "name": "Shohei Tanaka"}, {"authorId": "2355353380", "name": "Keisuke Shirai"}, {"authorId": "2294718338", "name": "Masashi Hamaya"}, {"authorId": "1450768516", "name": "C. C. Beltran-Hernandez"}, {"authorId": "2221120499", "name": "Atsushi Hashimoto"}, {"authorId": "2351603754", "name": "Takamitsu Matsubara"}], "abstract": "Dynamic Movement Primitives (DMPs) provide a flexible framework wherein smooth robotic motions are encoded into modular parameters. However, they face challenges in integrating multimodal inputs commonly used in robotics like vision and language into their framework. To fully maximize DMPs’ potential, enabling them to handle multimodal inputs is essential. In addition, we also aim to extend DMPs’ capability to handle object-focused tasks requiring one-shot complex motion generation, as observation occlusion could easily happen mid-execution in such tasks (e.g., knife occlusion in cake icing, hand occlusion in dough kneading, etc.). A promising approach is to leverage Vision-Language Models (VLMs), which process multimodal data and can grasp high-level concepts. However, they typically lack enough knowledge and capabilities to directly infer low-level motion details and instead only serve as a bridge between high-level instructions and low-level control. To address this limitation, we propose Keyword Labeled Primitive Selection and Keypoint Pairs Generation Guided Movement Primitives (KeyMPs), a framework that combines VLMs with sequencing of DMPs. KeyMPs use VLMs’ high-level reasoning capability to select a reference primitive through keyword labeled primitive selection and VLMs’ spatial awareness to generate spatial scaling parameters used for sequencing DMPs by generalizing the overall motion through keypoint pairs generation, which together enable one-shot vision-language guided motion generation that aligns with the intent expressed in the multimodal input. We validate our approach through experiments on two occlusion-rich tasks: object cutting, conducted in both simulated and real-world environments, and cake icing, performed in simulation. These evaluations demonstrate superior performance over other DMP-based methods that integrate VLM support."}
{"paperId": "495b75537753fa54501efb5d8c438c33dbec10b6", "url": "https://www.semanticscholar.org/paper/495b75537753fa54501efb5d8c438c33dbec10b6", "title": "3D CG Image Quality Assessment in Vision and Language based on Stable Diffusion", "venue": "Electronic imaging", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2352/ei.2025.37.9.iqsp-243?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2352/ei.2025.37.9.iqsp-243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-02", "authors": [{"authorId": "35418755", "name": "Norifumi Kawabata"}], "abstract": null}
{"paperId": "4a3fd1dd19cf4ba25babd1e973eed1bb40a713ff", "url": "https://www.semanticscholar.org/paper/4a3fd1dd19cf4ba25babd1e973eed1bb40a713ff", "title": "Infecting Generative AI With Viruses", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.05542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-09", "authors": [{"authorId": "46787948", "name": "David A. Noever"}, {"authorId": "2197525782", "name": "Forrest McKee"}], "abstract": "This study demonstrates a novel approach to testing the security boundaries of Vision-Large Language Model (VLM/ LLM) using the EICAR test file embedded within JPEG images. We successfully executed four distinct protocols across multiple LLM platforms, including OpenAI GPT-4o, Microsoft Copilot, Google Gemini 1.5 Pro, and Anthropic Claude 3.5 Sonnet. The experiments validated that a modified JPEG containing the EICAR signature could be uploaded, manipulated, and potentially executed within LLM virtual workspaces. Key findings include: 1) consistent ability to mask the EICAR string in image metadata without detection, 2) successful extraction of the test file using Python-based manipulation within LLM environments, and 3) demonstration of multiple obfuscation techniques including base64 encoding and string reversal. This research extends Microsoft Research's\"Penetration Testing Rules of Engagement\"framework to evaluate cloud-based generative AI and LLM security boundaries, particularly focusing on file handling and execution capabilities within containerized environments."}
{"paperId": "4a470a1cfb9a7ae81ec9c01e508fd833b467dc6d", "url": "https://www.semanticscholar.org/paper/4a470a1cfb9a7ae81ec9c01e508fd833b467dc6d", "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.11277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-15", "authors": [{"authorId": "2294573012", "name": "M. L. Olson"}, {"authorId": "2287831755", "name": "Musashi Hinck"}, {"authorId": "2326836935", "name": "Neale Ratzlaff"}, {"authorId": "2363238179", "name": "Changbai Li"}, {"authorId": "2253672147", "name": "Phillip Howard"}, {"authorId": "2309247409", "name": "Vasudev Lal"}, {"authorId": "2304554910", "name": "Shao-Yen Tseng"}], "abstract": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain."}
{"paperId": "4a8c8946a8849e91b3c4f56f2a3c88c27e12d118", "url": "https://www.semanticscholar.org/paper/4a8c8946a8849e91b3c4f56f2a3c88c27e12d118", "title": "Forecasting at Full Spectrum: Holistic Multi-Granular Traffic Modeling under High-Throughput Inference Regimes", "venue": "International Conference on Information and Knowledge Management", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01279, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-05-02", "authors": [{"authorId": "2212204325", "name": "Zhaoyan Wang"}, {"authorId": "2342662094", "name": "Xiangchi Song"}, {"authorId": "2358999743", "name": "In-Young Ko"}], "abstract": "Notably, current intelligent transportation systems rely heavily on accurate traffic forecasting and swift inference provision to make timely decisions. While Graph Convolutional Networks (GCNs) have shown benefits in modeling complex traffic dependencies, the existing GCN-based approaches cannot fully extract and fuse multi-granular spatiotemporal features across various spatial and temporal scales sufficiently in a complete manner, proven to yield less accurate results. As extracting multi-granular features across scales has been a promising strategy across domains such as computer vision, natural language processing, and time-series forecasting, pioneering studies have attempted to leverage a similar mechanism for spatiotemporal traffic data mining. However, additional feature extraction branches introduced in prior studies critically increased model complexity and extended inference time, making it challenging to provide fast forecasts. In this paper, we propose MultiGran-STGCNFog, an efficient fog distributed inference system with a novel traffic forecasting model that employs multi-granular spatiotemporal feature fusion on generated dynamic traffic graphs to fully capture interdependent traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer execution order and layer-device scheduling scheme simultaneously, contributes to considerable inference throughput improvement by coordinating heterogeneous fog devices in a pipelined manner. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over selected GCN baselines."}
{"paperId": "4aaa4fafbadc2dbdefd513f9177b4a2f49eb1367", "url": "https://www.semanticscholar.org/paper/4aaa4fafbadc2dbdefd513f9177b4a2f49eb1367", "title": "DeepMeaning: Estimating and Interpreting Scene Meaning for Attention Using a Vision-Language Transformer", "venue": "Open Mind", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1162/opmi.a.6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1162/opmi.a.6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2506727", "name": "Taylor R. Hayes"}, {"authorId": "2277707938", "name": "John M. Henderson"}], "abstract": "\n Humans rapidly process and understand real-world scenes with ease. Our stored semantic knowledge gained from experience is thought to be central to this ability by organizing perceptual information into meaningful units to efficiently guide our attention. However, the role stored semantic representations play in attentional guidance remains difficult to study and poorly understood. Here, we apply a state-of-the-art vision-language transformer trained on billions of image-text pairs to help advance our understanding of the role local meaning plays in scene guidance. Specifically, we demonstrate that this transformer-based approach can be used to automatically estimate local scene meaning in indoor and outdoor scenes, predict where people look in these scenes, detect changes in local semantic content, and provide multiple avenues to model interpretation through its language capabilities. Taken together, these findings highlight how multimodal transformers can advance our understanding of the role scene semantics play in scene attention by serving as a representational framework that bridges vision and language."}
{"paperId": "4abdc511b19ce953e4e2226bbb5063e5a64cc95e", "url": "https://www.semanticscholar.org/paper/4abdc511b19ce953e4e2226bbb5063e5a64cc95e", "title": "Will multimodal large language models ever achieve deep understanding of the world?", "venue": "Frontiers in Systems Neuroscience", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12679578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-17", "authors": [{"authorId": "2393350152", "name": "Igor Farkaš"}, {"authorId": "2375137705", "name": "Michal Vavrecka"}, {"authorId": "2265433970", "name": "S. Wermter"}], "abstract": "Despite impressive performance in various tasks, large language models (LLMs) are subject to the symbol grounding problem, so from the cognitive science perspective, one can argue that they are merely statistics-driven distributional models without a deeper understanding. Modern multimodal versions of LLMs (MLLMs) are trying to avoid this problem by linking language knowledge with other modalities such as vision (Vision Language Models called VLM) or action (Vision Language Action Models called VLA) when, for instance, a robotic agent, is acting in the world. If eventually successful, MLLMs could be taken as pathway for symbol grounding. In this work, we explore the extent to which MLLMs integrated with embodied agents can achieve such grounded understanding through interaction with the physical world. We argue that closing the gap between symbolic tokens, neural representations, and embodied experience will require deeper developmental integration of continuous sensory data, goal-directed behavior, and adaptive neural learning in real-world environments. We raise a concern that MLLMs do not currently achieve a human-like level of deep understanding, largely because their random learning trajectory deviates significantly from human cognitive development. Humans typically acquire knowledge incrementally, building complex concepts upon simpler ones in a structured developmental progression. In contrast, MLLMs are often trained on vast, randomly ordered datasets. This non-developmental approach, which circumvents a structured simple-to-complex conceptual scaffolding, inhibits the ability to build a deep and meaningful grounded knowledge base, posing a significant challenge to achieving human-like semantic comprehension."}
{"paperId": "4ac72c78800fe816bb10a5ed982a64879def751c", "url": "https://www.semanticscholar.org/paper/4ac72c78800fe816bb10a5ed982a64879def751c", "title": "GSDNet: Revisiting Incomplete Multimodality-Diffusion Emotion Recognition from the Perspective of Graph Spectrum", "venue": "International Joint Conference on Artificial Intelligence", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2025/688?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2025/688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2269467772", "name": "Yuntao Shou"}, {"authorId": "2367177989", "name": "Jun Yao"}, {"authorId": "2149703522", "name": "Tao Meng"}, {"authorId": "2058030542", "name": "Wei Ai"}, {"authorId": "2346271333", "name": "Cen Chen"}, {"authorId": "2269747248", "name": "Keqin Li"}], "abstract": "Multimodal Emotion Recognition (MER) combines technologies from multiple fields (e.g., computer vision, natural language processing, and audio signal processing), aiming to infer an individual's emotional state by analyzing information from different sources (i.e., video, audio, and text). Compared with single modality, by fusing complementary semantic information from different modalities, the model can obtain more robust knowledge representation. However, the modality missing problem limits the performance of MERC in practical scenarios. Recent work has achieved impressive performance on modality completion using graph neural networks and diffusion models, respectively. This inspires us to combine these two dimensions in the completion network to obtain more powerful representation capabilities. However, we argue that directly running a full-rank score-based diffusion model on the entire graph adjacency matrix space may adversely affect the learning process of the diffusion model. This is because the model assumes a direct relationship between each pair of nodes and ignores local structural features and sparse connections between nodes, thereby significantly reducing the quality of the generated data. Based on the above ideas, we propose a novel Graph Spectral Diffusion Network (GSDNet), which utilizes a low-rank score-based diffusion model to map Gaussian noise to the graph spectral distribution space of missing modalities and recover the missing data according to its original distribution. Extensive experiments have demonstrated that GSDNet achieves state-of-the-art emotion recognition performance in various modality loss scenarios."}
{"paperId": "4bf85474939256898cad0bd39f87bb97c68ba4bc", "url": "https://www.semanticscholar.org/paper/4bf85474939256898cad0bd39f87bb97c68ba4bc", "title": "AODPart: Accuracy-Optimal Online Partitioning for Edge Inference with Delay Constraint", "venue": "Proceedings of the Twenty-sixth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3704413.3764468?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3704413.3764468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book"], "publicationDate": "2025-10-23", "authors": [{"authorId": "2378859347", "name": "Shiva Saxena"}, {"authorId": "2257059071", "name": "Ben Liang"}], "abstract": "We consider the partitioning of a deep neural network (DNN) inference job and offloading part of it from a resource-constrained device to a resource-rich server. The inference job is required to finish within a delay constraint, but it is allowed to perform early exit at some intermediate layer of the DNN, at the cost of lower accuracy. Since in practice both the processing delay and the communication delay of offloading usually are unknown ahead of time, this is naturally modelled as an online delay-constrained accuracy maximization problem. We propose Accuracy-Optimal Delay Constrained Online Partitioning (AODPart), a lightweight online algorithm that uses an adaptive thresholding strategy to solve the offloading problem. We derive the competitive ratio for AODPart and show that it is optimal in the sense that no other online algorithm can achieve a lower deterministic competitive ratio. Furthermore, we show that AODPart is robust and provides worst-case performance guarantee even with parameter estimation error. Through experimenting with common vision and language learning models, we demonstrate that AODPart substantially outperforms state-of-the-art alternatives and returns near optimal accuracy in practice."}
{"paperId": "4c07dac8d701425e4ab8f8bb357e6f7b3c417e8f", "url": "https://www.semanticscholar.org/paper/4c07dac8d701425e4ab8f8bb357e6f7b3c417e8f", "title": "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation", "venue": "Neural Networks", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09938, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2349543135", "name": "Sen Wang"}, {"authorId": "2340676116", "name": "Dongliang Zhou"}, {"authorId": "2152787198", "name": "Liang Xie"}, {"authorId": "2349631400", "name": "Chao Xu"}, {"authorId": "2112212982", "name": "Ye Yan"}, {"authorId": "2289209619", "name": "Erwei Yin"}], "abstract": null}
{"paperId": "4c62d2445018e5bca0216b2273cdc87a5c8ac4f5", "url": "https://www.semanticscholar.org/paper/4c62d2445018e5bca0216b2273cdc87a5c8ac4f5", "title": "MOON: Multimodal Omniscient Operational Network", "venue": "International Scientific Journal of Engineering and Management", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/isjem03324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/isjem03324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-05", "authors": [{"authorId": "2359316321", "name": "Dr. P. Sumalatha"}], "abstract": "Abstract—The advancement of artificial intelligence (AI) has significantly accelerated the development of multimodal virtual assistants that integrate diverse sensory modalities to enrich human-computer interaction. This paper introduces MOON (Multimodal Omniscient Operational Network), an AI assistant designed to seamlessly combine voice recognition, computer vision, gesture control, and environmental analysis within an adaptive and intuitive interface. Built upon frameworks such as MediaPipe for gesture recognition, YOLOv3 for real-time object detection, and spaCy for natural language processing, MOON performs a wide range of tasks, including application control, sentiment analysis, and facial recognition-based user identification. The system incorporates a dynamic memory model to facilitate context-aware responses and personalization.\n\nExperimental evaluations examining accuracy, latency, and user satisfaction indicate that MOON significantly outperforms unimodal assistants. However, its use of facial recognition tech- nology raises ethical concerns related to privacy and surveillance. This research proposes a scalable and modular multimodal AI framework with implications for smart environments, ambient intelligence, and accessibility technologies.\n\nKeywords— Multimodal AI, Virtual Assistant, Computer Vision, Natural Language Processing, Human-Computer Inter- action."}
{"paperId": "4c64c1786d5c0bfdfe151abdfb45bc45f74c72a9", "url": "https://www.semanticscholar.org/paper/4c64c1786d5c0bfdfe151abdfb45bc45f74c72a9", "title": "Generative and Multimodal Learning for Vision and Language", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2369626317", "name": "Emanuele Aiello"}], "abstract": null}
{"paperId": "4d09aa7af26e33e2618a4dc99a9abc8053274bd7", "url": "https://www.semanticscholar.org/paper/4d09aa7af26e33e2618a4dc99a9abc8053274bd7", "title": "Foundation Models: From Current Developments, Challenges, and Risks to Future Opportunities", "venue": "International Conference on Advanced Communication Technology", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23919/ICACT63878.2025.10936649?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23919/ICACT63878.2025.10936649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2025-02-16", "authors": [{"authorId": "2082387868", "name": "Ali Hussain"}, {"authorId": "2130536107", "name": "Sikandar Ali"}, {"authorId": "2353663386", "name": "Umm E. Farwa"}, {"authorId": "2158510130", "name": "Md Ariful Islam Mozumder"}, {"authorId": "2294152458", "name": "Hee-Cheol Kim"}], "abstract": "Foundation models are a revolutionary technology in the field of artificial intelligence that could usher in an era of transformation in every field with its vision and language understating capabilities while maintaining remarkable performance. Here, we extensively review state-of-the-art foundation models in various fields such as LLMs, GPT, BERT, CLIP, etc. We also demonstrate some famous areas where foundation models are gaining much popularity, such as general foundation models, foundation models in the medical domain, education, law and finance, mathematics, autonomous driving, etc. These foundation models are trained on large-scale data by leveraging the capabilities of self-supervised learning approaches that ensure outperforming accuracy for relevant downstream tasks. They are successful due to pioneering architectural innovations, especially synergistically interleaving transformers and convolutional neural networks. These models have exhibited adaptability and resilience in various data patterns and conditions through sophisticated training paradigms such as self-supervised and supervised learning methods. While they hold transformative potential, they also have many challenges. The scale and quality of training data are still important predictors of model performance, and the need for interpretable and explainable AI systems becomes critically important. We highlight important research opportunities, high-performing computationally efficient and scalable architectures, and approaches to enable multimodal learning capacities. This review sheds light on the current state of foundation models and proposes a roadmap for their successful transitions."}
{"paperId": "4d5664abb55fc24b4e5f3706be26155f5ee405c7", "url": "https://www.semanticscholar.org/paper/4d5664abb55fc24b4e5f3706be26155f5ee405c7", "title": "PERAN ARTIFICIAL INTELIGENCE PADA PERKEMBANGAN PENERAPAN MULTIMEDIA DALAM BERBAGAI BIDANG: Systematic Literature Review", "venue": "Jurnal Rekayasa Sistem Informasi dan Teknologi", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.70248/jrsit.v3i1.2899?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.70248/jrsit.v3i1.2899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-08", "authors": [{"authorId": "2314174040", "name": "Nailis Sa’adah"}, {"authorId": "119541291", "name": "Dewi Mulyati"}, {"authorId": "65773924", "name": "Geubrina Maghfirah"}, {"authorId": "151237192", "name": "Titien Sara"}], "abstract": "Penelitian ini bertujuan untuk mengkaji peran Artificial Intelligence (AI) dalam perkembangan dan penerapan teknologi multimedia di berbagai bidang melalui metode systematic review. Ada 30 artikel ilmiah dari jurnal bereputasi internasional (peringkat Q1 dan Q2) yang diterbitkan antara tahun 2020 hingga 2025 dianalisis secara menyeluruh. Hasil kajian menunjukkan bahwa AI memiliki kontribusi yang luas dalam mempercepat inovasi multimedia, mulai dari bidang pendidikan, kesehatan, industri kreatif, sosial, hukum, hingga properti. Sistem multimedia interaktif dan otomatis ini didukung oleh Teknologi AI seperti Deep Learning (DL), Computer Vision, Natural Language Processing (NLP), dan Generative Adversarial Networks (GAN). Teknolgi AI yang paling banya dipakai adalah Teknologi Deep Learning. Di bidang pendidikan, AI digunakan untuk pengembangan konten pembelajaran adaptif dan visualisasi interaktif. Di sektor kesehatan, AI diaplikasikan dalam sistem diagnosis berbasis citra dan media edukatif pasien. Sementara itu, bidang sosial dan hukum memanfaatkan AI dalam analisis visual, pendeteksian konten berbahaya, serta pemantauan media. Penelitian ini menyimpulkan bahwa integrasi AI dan multimedia berperan penting dalam meningkatkan efisiensi, akurasi, serta kualitas informasi visual dan audio yang digunakan secara luas lintas sektor. Temuan ini dapat menjadi dasar bagi pengembangan sistem multimedia berbasis AI di masa depan yang lebih adaptif, aman, dan efisien."}
{"paperId": "4d808178b1a61adeb25368223adf92728e46cd5c", "url": "https://www.semanticscholar.org/paper/4d808178b1a61adeb25368223adf92728e46cd5c", "title": "Research on lip synthesis of virtual digital humans based on the Spark large model", "venue": "Conference on Electronic Information Engineering and Data Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3067065?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3067065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-05-09", "authors": [{"authorId": "2360145931", "name": "Yuqi Huang"}], "abstract": "In recent years, with the support of key technologies such as deep learning algorithms, computer vision, natural language processing, and graphics rendering, the production process of virtual digital humans has achieved revolutionary breakthroughs. Lip synthesis technology, as a crucial component, has also garnered widespread attention. Most existing lip synthesis models are trained on English datasets, resulting in poor synthesis effects in Chinese contexts. This paper, based on the study of the Wav2Lip lip synthesis model, constructs a Chinese lip-sync dataset and conducts a series of training fine-tunings to achieve better synthesis effects. By leveraging the multimodal capabilities of the Spark large model, it realizes efficient speech-driven facial animation generation. Furthermore, this paper explores the application potential of the Wav2Lip model, providing a solid theoretical foundation and technical support for future commercial applications."}
{"paperId": "4d8eadaef62120c999b63431f3721e9c37e915a2", "url": "https://www.semanticscholar.org/paper/4d8eadaef62120c999b63431f3721e9c37e915a2", "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-19", "authors": [{"authorId": "2351344776", "name": "Ying Liu"}, {"authorId": "2350846136", "name": "Yijing Hua"}, {"authorId": "2350857717", "name": "Haojiang Chai"}, {"authorId": "2350859897", "name": "Yanbo Wang"}, {"authorId": "2350859360", "name": "TengQi Ye"}], "abstract": "Open-vocabulary detectors are proposed to locate and recognize objects in novel classes. However, variations in vision-aware language vocabulary data used for open-vocabulary learning can lead to unfair and unreliable evaluations. Recent evaluation methods have attempted to address this issue by incorporating object properties or adding locations and characteristics to the captions. Nevertheless, since these properties and locations depend on the specific details of the images instead of classes, detectors can not make accurate predictions without precise descriptions provided through human annotation. This paper introduces 3F-OVD, a novel task that extends supervised fine-grained object detection to the open-vocabulary setting. Our task is intuitive and challenging, requiring a deep understanding of Fine-grained captions and careful attention to Fine-grained details in images in order to accurately detect Fine-grained objects. Additionally, due to the scarcity of qualified fine-grained object detection datasets, we have created a new dataset, NEU-171K, tailored for both supervised and open-vocabulary settings. We benchmark state-of-the-art object detectors on our dataset for both settings. Furthermore, we propose a simple yet effective post-processing technique. Our data, annotations and codes are available at https://github.com/tengerye/3FOVD."}
{"paperId": "4de33c4a991fdd426ad86d3a0b80b23b4d844df6", "url": "https://www.semanticscholar.org/paper/4de33c4a991fdd426ad86d3a0b80b23b4d844df6", "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-03", "authors": [{"authorId": "2238815102", "name": "Davide Caffagni"}, {"authorId": "2179325162", "name": "Sara Sarto"}, {"authorId": "3468983", "name": "Marcella Cornia"}, {"authorId": "1843795", "name": "L. Baraldi"}, {"authorId": "2303850502", "name": "Rita Cucchiara"}], "abstract": "Cross-modal retrieval is gaining increasing efficacy and interest from the research community, thanks to large-scale training, novel architectural and learning designs, and its application in LLMs and multimodal LLMs. In this paper, we move a step forward and design an approach that allows for multimodal queries – composed of both an image and a text – and can search within collections of multi-modal documents, where images and text are interleaved. Our model, ReT, employs multi-level representations extracted from different layers of both visual and textual backbones, both at the query and document side. To allow for multi-level and cross-modal understanding and feature extraction, ReT employs a novel Transformer-based recurrent cell that integrates both textual and visual features at different layers, and leverages sigmoidal gates inspired by the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR benchmarks show that ReT achieves state-of-the-art performance across diverse settings. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT."}
{"paperId": "4df81705e32935671146a61f843df7f4e74a7d33", "url": "https://www.semanticscholar.org/paper/4df81705e32935671146a61f843df7f4e74a7d33", "title": "VISTAv2: World Imagination for Indoor Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-14", "authors": [{"authorId": "2351000265", "name": "Yanjia Huang"}, {"authorId": "2396422782", "name": "Xianshun Jiang"}, {"authorId": "2336090443", "name": "Xiangbo Gao"}, {"authorId": "2360939852", "name": "Mingyang Wu"}, {"authorId": "2362300531", "name": "Zhengzhong Tu"}], "abstract": "Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN."}
{"paperId": "4e0c1396c5c476688a084702318e059371b67b69", "url": "https://www.semanticscholar.org/paper/4e0c1396c5c476688a084702318e059371b67b69", "title": "Overcoming Feature Contamination by Unidirectional Information Modeling for Vision-Language Tracking", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICME59968.2025.11209477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICME59968.2025.11209477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2333249502", "name": "Jingchao Wang"}, {"authorId": "2340161420", "name": "Zhijian Wu"}, {"authorId": "2357230401", "name": "Wenlong Zhang"}, {"authorId": "2143557558", "name": "Wenhui Liu"}, {"authorId": "2108091550", "name": "Jianwei Zhang"}, {"authorId": "2262087632", "name": "Dingjiang Huang"}], "abstract": "Benefiting from the advantages of multi-modal learning, Vision-Language Tracking shows greater potential than Visual Tracking. Existing work utilizes one-stream structures to fuse vision and language features, resulting in noise propagation from the search region into the language features. This contamination weakens the guidance of language information, consequently limiting the robustness of the tracking model. To solve this problem, we propose a Unidirectional Information modeling (UITracker) to explicitly fuse the language and visual features for Vision-Language Tracking. Specifically, we introduce a plug-and-play lightweight modal adapter to unidirectionally inject language guidance into the visual template and search region across all layers. This allows the tracker to make full use of rich semantic information while overcoming language feature contamination in the feature interaction process. Extensive ablation studies demonstrate the superiority and effectiveness of our UITracker. Code and raw results are available at https://github.com/jcwang0602/UITrack."}
{"paperId": "4eafe649e704f307907ae0ec73307861c3336118", "url": "https://www.semanticscholar.org/paper/4eafe649e704f307907ae0ec73307861c3336118", "title": "Representation Learning for Tabular Data: A Comprehensive Survey", "venue": "arXiv.org", "year": 2025, "citationCount": 15, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-04-17", "authors": [{"authorId": "2304704645", "name": "Jun-Peng Jiang"}, {"authorId": "2309426002", "name": "Si-Yang Liu"}, {"authorId": "2309792840", "name": "Hao-Run Cai"}, {"authorId": "2261452291", "name": "Qi-Le Zhou"}, {"authorId": "2311116010", "name": "Han-Jia Ye"}], "abstract": "Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data -- features, samples, and objectives -- and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey."}
{"paperId": "4eda6bf4a7a89215ee5c03dea3e67616d3b4d16b", "url": "https://www.semanticscholar.org/paper/4eda6bf4a7a89215ee5c03dea3e67616d3b4d16b", "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.16796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-20", "authors": [{"authorId": "2295731839", "name": "Junbo Qiao"}, {"authorId": "2347028116", "name": "Miaomiao Cai"}, {"authorId": "2256598086", "name": "Wei Li"}, {"authorId": "2367744657", "name": "Yutong Liu"}, {"authorId": "2215810208", "name": "Xu Huang"}, {"authorId": "2294580024", "name": "Gaoqi He"}, {"authorId": "2212036699", "name": "Jiao Xie"}, {"authorId": "2112348564", "name": "Jie Hu"}, {"authorId": "2367896549", "name": "Xinghao Chen"}, {"authorId": "2294808365", "name": "Shaohui Lin"}], "abstract": "Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation."}
{"paperId": "4ee1bdd11bfd481330e274dbf03b4113bafdddfd", "url": "https://www.semanticscholar.org/paper/4ee1bdd11bfd481330e274dbf03b4113bafdddfd", "title": "Gen-n-Val: Agentic Image Data Generation and Validation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.04676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-05", "authors": [{"authorId": "2366419760", "name": "Jing-En Huang"}, {"authorId": "2333609356", "name": "I-Sheng Fang"}, {"authorId": "2279866468", "name": "Tzuhsuan Huang"}, {"authorId": "2155887094", "name": "Chih-Yu Wang"}, {"authorId": "2355359878", "name": "Jun-Cheng Chen"}], "abstract": "Recently, Large Language Models (LLMs) and Vision Large Language Models (VLLMs) have demonstrated impressive performance as agents across various tasks while data scarcity and label noise remain significant challenges in computer vision tasks, such as object detection and instance segmentation. A common solution for resolving these issues is to generate synthetic data. However, current synthetic data generation methods struggle with issues, such as multiple objects per mask, inaccurate segmentation, and incorrect category labels, limiting their effectiveness. To address these issues, we introduce Gen-n-Val, a novel agentic data generation framework that leverages Layer Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt agent, an LLM, optimizes prompts for LD to generate high-quality foreground instance images and segmentation masks. These optimized prompts ensure the generation of single-object synthetic data with precise instance masks and clean backgrounds. (2) The data validation agent, a VLLM, which filters out low-quality synthetic instance images. The system prompts for both agents are refined through TextGrad. Additionally, we use image harmonization to combine multiple instances within scenes. Compared to state-of-the-art synthetic data approaches like MosaicFusion, our approach reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance of YOLOv9 and YOLO11 families in instance segmentation and object detection."}
{"paperId": "4f4ad9d1c31d8bad6d4d242b96c73b2d827565c8", "url": "https://www.semanticscholar.org/paper/4f4ad9d1c31d8bad6d4d242b96c73b2d827565c8", "title": "Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-23", "authors": [{"authorId": "2228651380", "name": "Lixiong Qin"}, {"authorId": "2387457738", "name": "Yang Zhang"}, {"authorId": "2338352396", "name": "Mei Wang"}, {"authorId": "2220586656", "name": "Jiani Hu"}, {"authorId": "2338887345", "name": "Weihong Deng"}, {"authorId": "2338500965", "name": "Weiran Xu"}], "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at https://github.com/lxq1000/Fake-in-Facext."}
{"paperId": "4ff842cb78546fad5a070210cbdc019caf881c1c", "url": "https://www.semanticscholar.org/paper/4ff842cb78546fad5a070210cbdc019caf881c1c", "title": "BERT-VQA: Visual Question Answering on Plots", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-14", "authors": [{"authorId": "2056253202", "name": "Tai Vu"}, {"authorId": "2268029897", "name": "Robert Yang"}], "abstract": "Visual question answering has been an exciting challenge in the field of natural language understanding, as it requires deep learning models to exchange information from both vision and language domains. In this project, we aim to tackle a subtask of this problem, namely visual question answering on plots. To achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with a pretrained ResNet 101 image encoder, along with a potential addition of joint fusion. We trained and evaluated this model against a baseline that consisted of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our core hypothesis that the cross-modality module in VisualBERT is essential in aligning plot components with question phrases. Therefore, our work provided valuable insights into the difficulty of the plot question answering challenge as well as the appropriateness of different model architectures in solving this problem."}
{"paperId": "503aa3aee3dc0370f90c8d991cf8a3cfa27c3ee8", "url": "https://www.semanticscholar.org/paper/503aa3aee3dc0370f90c8d991cf8a3cfa27c3ee8", "title": "Style Craft: AI-Driven Fashion Platform", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem47571?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem47571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-12", "authors": [{"authorId": "2357143970", "name": "Abhinav Sinha"}], "abstract": "1. ABSTRACT\n\nAs artificial intelligence continues to reshape industries, personalized and intelligent systems are becoming essential for enriching digital experiences. Style Craft: AI-Driven Fashion Platform introduces a next-generation fashion assistant designed to redefine how users discover, interact with, and personalize their style choices. The platform delivers curated fashion recommendations, enables virtual outfit trials, and helps users stay updated with current trends through an intuitive and immersive interface.\n\nBuilt with Python and enhanced by cutting-edge AI methodologies, the system leverages computer vision, natural language understanding, and recommendation engines to offer dynamic suggestions tailored to individual preferences, body profiles, and browsing behavior. Core components include an AI-powered virtual try-on system, style compatibility analysis, and trend forecasting modules, all accessible through a responsive web interface.\n\nThis paper details the system's architecture and the technologies that power it, emphasizing how AI elevates personalization, visual recognition, and interaction design in the fashion domain. It also addresses implementation challenges, including optimizing garment recognition, adapting to user variability, and maintaining fluid performance.\n\nLooking forward, the platform envisions broader capabilities such as conversational AI for voice-guided fashion navigation, AR/VR support for immersive try-ons, and integration with real-time retail inventories for seamless shopping. Style Craft underscores the innovative potential of AI in crafting tailored, engaging, and futuristic fashion experiences for modern users.\n\nACM Reference Format: Mitansh Sehgal, Nikhil Maurya, Abhinav Sinha. 2025. Style Craft: AI-Driven Fashion Platform.\n\nKeywords – Artificial Intelligence, Personalized Recommendations, Fashion Technology, , Trend Forecasting, Recommendation Systems, User Personalization, Human-Computer Interaction, Conversational AI, Style Analysis, Intelligent Fashion Assistant, E-commerce Innovation"}
{"paperId": "50ec50b6eb4c3cf2b5d8f8810516ef1846d36aca", "url": "https://www.semanticscholar.org/paper/50ec50b6eb4c3cf2b5d8f8810516ef1846d36aca", "title": "Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-14", "authors": [{"authorId": "2310283757", "name": "Yifan Xie"}, {"authorId": "2280950263", "name": "Binkai Ou"}, {"authorId": "2331682968", "name": "Fei Ma"}, {"authorId": "2350675966", "name": "Yaohua Liu"}], "abstract": "Vision and Language Navigation (VLN) requires an agent to navigate through environments following natural language instructions. However, existing methods often struggle with effectively integrating visual observations and instruction details during navigation, leading to suboptimal path planning and limited success rates. In this paper, we propose OIKG (Observation-graph Interaction and Key-detail Guidance), a novel framework that addresses these limitations through two key components: (1) an observation-graph interaction module that decouples angular and visual information while strengthening edge representations in the navigation space, and (2) a key-detail guidance module that dynamically extracts and utilizes fine-grained location and object information from instructions. By enabling more precise cross-modal alignment and dynamic instruction interpretation, our approach significantly improves the agent’s ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR datasets demonstrate that OIKG achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of our method in enhancing navigation precision through better observation-instruction alignment."}
{"paperId": "519e62f38819037908f4d72066df9d4125352473", "url": "https://www.semanticscholar.org/paper/519e62f38819037908f4d72066df9d4125352473", "title": "EPIC: Efficient Prompt Interaction for Text-Image Classification", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.07415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2267397071", "name": "Xinyao Yu"}, {"authorId": "2267687346", "name": "Hao Sun"}, {"authorId": "2372556677", "name": "Zeyu Ling"}, {"authorId": "2213475590", "name": "Ziwei Niu"}, {"authorId": "2261748559", "name": "Zhenjia Bai"}, {"authorId": "2261751726", "name": "Rui Qin"}, {"authorId": "2260947348", "name": "Yen-wei Chen"}, {"authorId": "2289870137", "name": "Lanfen Lin"}], "abstract": "In recent years, large-scale pre-trained multimodal models (LMMs) generally emerge to integrate the vision and language modalities, achieving considerable success in multimodal tasks, such as text-image classification. The growing size of LMMs, however, results in a significant computational cost for fine-tuning these models for downstream tasks. Hence, prompt-based interaction strategy is studied to align modalities more efficiently. In this context, we propose a novel efficient prompt-based multimodal interaction strategy, namely Efficient Prompt Interaction for text-image Classification (EPIC). Specifically, we utilize temporal prompts on intermediate layers, and integrate different modalities with similarity-based prompt interaction, to leverage sufficient information exchange between modalities. Utilizing this approach, our method achieves reduced computational resource consumption and fewer trainable parameters (about 1% of the foundation model) compared to other fine-tuning strategies. Furthermore, it demonstrates superior performance on the UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance on the MM-IMDB dataset."}
{"paperId": "5242802d20037907da74f40d3a80093d556d7556", "url": "https://www.semanticscholar.org/paper/5242802d20037907da74f40d3a80093d556d7556", "title": "Understanding and Enhancing Mask-Based Pretraining towards Universal Representations", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.21650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-25", "authors": [{"authorId": "2180411117", "name": "Mingze Dong"}, {"authorId": "2382818906", "name": "Leda Wang"}, {"authorId": "40021520", "name": "Y. Kluger"}], "abstract": "Mask-based pretraining has become a cornerstone of modern large-scale models across language, vision, and recently biology. Despite its empirical success, its role and limits in learning data representations have been unclear. In this work, we show that the behavior of mask-based pretraining can be directly characterized by test risk in high-dimensional minimum-norm (\"ridge-less\") linear regression, without relying on further model specifications. Further analysis of linear models uncovers several novel aspects of mask-based pretraining. The theoretical framework and its implications have been validated across diverse neural architectures (including MLPs, CNNs, and Transformers) applied to both vision and language tasks. Guided by our theory, we propose an embarrassingly simple yet overlooked pretraining scheme named Randomly Random Mask AutoEncoding (R$^2$MAE), which enforces capturing multi-scale features from data and is able to outperform optimal fixed mask ratio settings in our linear model framework. We implement R$^2$MAE in vision, language, DNA sequence, and single-cell models, where it consistently outperforms standard and more complicated masking schemes, leading to improvements for state-of-the-art models. Our code is available at: https://github.com/MingzeDong/r2mae"}
{"paperId": "5252fc9c44d42587da74fc166e18038fe8ee0263", "url": "https://www.semanticscholar.org/paper/5252fc9c44d42587da74fc166e18038fe8ee0263", "title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24452, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-30", "authors": [{"authorId": "2364619200", "name": "Anda Tang"}, {"authorId": "2330370261", "name": "Yiming Dong"}, {"authorId": "122290781", "name": "Yutao Zeng"}, {"authorId": "2364682591", "name": "zhou Xun"}, {"authorId": "2282430471", "name": "Zhouchen Lin"}], "abstract": "The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by a single hyper-parameter \\varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between \\varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of \\varphi. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets."}
{"paperId": "52744b8ca8cebb25f05622fa519ec868b1aa4719", "url": "https://www.semanticscholar.org/paper/52744b8ca8cebb25f05622fa519ec868b1aa4719", "title": "A Comprehensive System for Real-Time Sign Language Interpretation Using Text and Speech Synthesis", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem48525?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem48525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-23", "authors": [{"authorId": "2363254625", "name": "Mr. Shashidhara H.V"}], "abstract": "Abstract:\n\nEffective communication remains a challenge for individuals who rely on sign language as their primary mode of expression, especially in interactions with non-sign language users. This research explores an innovative system that converts sign language gestures into text and subsequently into synthesized speech, enabling seamless and inclusive communication. Leveraging advancements in computer vision, natural language processing (NLP), and speech synthesis, the proposed model captures real-time sign gestures, translates them into structured textual data, and outputs audible speech with high accuracy.\n\nThe study delves into key technologies, including machine learning algorithms for gesture recognition, dynamic language modelling for text interpretation, and scalable speech synthesis techniques for voice output. This paper aims to provide a comprehensive framework addressing the linguistic and technical complexities of sign-to-text-to-speech conversion, emphasizing its potential impact on accessibility and societal integration for the deaf and hard-of-hearing communities."}
{"paperId": "52c7a761677f2acb53aa006f3e9800f9f74c731e", "url": "https://www.semanticscholar.org/paper/52c7a761677f2acb53aa006f3e9800f9f74c731e", "title": "EasyARC: Evaluating Vision Language Models on True Visual Reasoning", "venue": "arXiv.org", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.11595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-13", "authors": [{"authorId": "2264211265", "name": "Mert Unsal"}, {"authorId": "2367038366", "name": "Aylin Akkus"}], "abstract": "Building on recent advances in language-based reasoning models, we explore multimodal reasoning that integrates vision and text. Existing multimodal benchmarks primarily test visual extraction combined with text-based reasoning, lacking true visual reasoning with more complex interactions between vision and language. Inspired by the ARC challenge, we introduce EasyARC, a vision-language benchmark requiring multi-image, multi-step reasoning, and self-correction. EasyARC is procedurally generated, fully verifiable, and scalable, making it ideal for reinforcement learning (RL) pipelines. The generators incorporate progressive difficulty levels, enabling structured evaluation across task types and complexities. We benchmark state-of-the-art vision-language models and analyze their failure modes. We argue that EasyARC sets a new standard for evaluating true reasoning and test-time scaling capabilities in vision-language models. We open-source our benchmark dataset and evaluation code."}
{"paperId": "536ab69dd05234c2bcbd7625c93186165fb954dc", "url": "https://www.semanticscholar.org/paper/536ab69dd05234c2bcbd7625c93186165fb954dc", "title": "BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-18", "authors": [{"authorId": "2323371268", "name": "Wenqi Lyu"}, {"authorId": "2323374275", "name": "Zerui Li"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2292302699", "name": "Qi Wu"}], "abstract": "Multimodal large language models (MLLMs) have recently gained attention for their generalization and reasoning capabilities in Vision-and-Language Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However, MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety mechanisms and trigger undesired outputs. In embodied scenarios, such vulnerabilities pose greater risks: unlike plain text models that generate toxic content, embodied agents may interpret malicious instructions as executable commands, potentially leading to real-world harm. In this paper, we present the first systematic jailbreak attack paradigm targeting MLLM-driven navigator. We propose a three-tiered attack framework and construct malicious queries across four intent categories, concatenated with standard navigation instructions. In the Matterport3D simulator, we evaluate navigation agents powered by five MLLMs and report an average attack success rate over 90%. To test real-world feasibility, we replicate the attack on a physical robot. Our results show that even well-crafted prompts can induce harmful actions and intents in MLLMs, posing risks beyond toxic output and potentially leading to physical harm."}
{"paperId": "536c74216a29ee824ac1b376c250dc63de4d4bd0", "url": "https://www.semanticscholar.org/paper/536c74216a29ee824ac1b376c250dc63de4d4bd0", "title": "Reflections on Addressing Educational Inequalities Through the Co-Creation of a Rubric for Assessing Children’s Plurilingual and Intercultural Competence", "venue": "Education sciences", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/educsci15060762?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/educsci15060762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-16", "authors": [{"authorId": "2265123279", "name": "Janine Knight"}, {"authorId": "2091874121", "name": "Marta Segura"}], "abstract": "Recognising linguistic diversity as a person’s characteristic is arguably central to their multilingual identity and is important as an equity issue. Different indicators suggest that students with migrant backgrounds, whose linguistic diversity is often not reflected in European education systems, tend to underperform compared to their peers without migrant backgrounds. There is a dire need, therefore, to alleviate the educational inequalities that negatively affect some of the most plurilingual students in European school systems. This can be carried out by revisiting assessment tools. Developing assessments to make children’s full linguistic and cultural repertoire visible, and what they can do with it, is one way that potential inequalities in school systems and assessment practices can be addressed so that cultural and linguistic responsiveness of assessments and practices can be improved. This paper explores the concept of discontinuities or mismatches between the assessment of plurilingual children’s linguistic practices in one primary school in Catalonia and their actual linguistic realities, including heritage languages. It asks: (1) What are the children’s linguistic profiles? (2) What mismatches and/or educational inequalities do they experience? and (3) How does the co-creation and use of a rubric assessing plurilingual and intercultural competence attempt to mitigate these mismatches and inequalities? Mismatches are identified using a context- and participant-relevant reflection tool, based on 18 reflective questions related to aspects of social justice. Results highlight that mismatches exist between children’s plurilingual and intercultural knowledge and skills compared to the school, education system, curriculum, and wider regional and European policy. These mismatches highlight two plurilingual visions for language education. The paper highlights how language assessment tools and practices can be made more culturally and linguistically fair for plurilingual children with migration backgrounds."}
{"paperId": "53957cdd948da3d9dff026a4d7480606000073e4", "url": "https://www.semanticscholar.org/paper/53957cdd948da3d9dff026a4d7480606000073e4", "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-17", "authors": [{"authorId": "2244167279", "name": "Shu Yuan"}, {"authorId": "145249315", "name": "Guowen Xu"}, {"authorId": "2271396643", "name": "Hongwei Li"}, {"authorId": "2284217975", "name": "Rui Zhang"}, {"authorId": "2067735266", "name": "Xi Qian"}, {"authorId": "2116485977", "name": "Wenbo Jiang"}, {"authorId": "2321255113", "name": "Hangcheng Cao"}, {"authorId": "2359462624", "name": "Qingchuan Zhao"}], "abstract": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but are vulnerable to backdoor attacks. Existing physical backdoor attacks either lack stealth, provide inflexible attack control, or ignore emerging Vision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the first physical-world backdoor attack leveraging fluorescent ink as triggers. Fluorescent triggers are invisible under normal conditions and activated stealthily by ultraviolet light, providing superior stealthiness, flexibility, and untraceability. Inspired by real-world graffiti, we derive realistic trigger shapes and enhance their robustness via an interpolation-based fluorescence simulation algorithm. Furthermore, we develop an automated backdoor sample generation method to support three attack objectives. Extensive evaluations in the physical world demonstrate FIGhost's effectiveness against state-of-the-art detectors and VLMs, maintaining robustness under environmental variations and effectively evading existing defenses."}
{"paperId": "53b73f33d33a3e6d3beab6895f3b83d30581dfcb", "url": "https://www.semanticscholar.org/paper/53b73f33d33a3e6d3beab6895f3b83d30581dfcb", "title": "Mitigating Object Hallucination via Robust Local Perception Search", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.06729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-07", "authors": [{"authorId": "2321683538", "name": "Zixian Gao"}, {"authorId": "2337690102", "name": "Chao Yang"}, {"authorId": "2254279326", "name": "Zhanhui Zhou"}, {"authorId": "2367362278", "name": "Xing Xu"}, {"authorId": "2336246821", "name": "Chaochao Lu"}], "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled them to effectively integrate vision and language, addressing a variety of downstream tasks. However, despite their significant success, these models still exhibit hallucination phenomena, where the outputs appear plausible but do not align with the content of the images. To mitigate this issue, we introduce Local Perception Search (LPS), a decoding method during inference that is both simple and training-free, yet effectively suppresses hallucinations. This method leverages local visual prior information as a value function to correct the decoding process. Additionally, we observe that the impact of the local visual prior on model performance is more pronounced in scenarios with high levels of image noise. Notably, LPS is a plug-and-play approach that is compatible with various models. Extensive experiments on widely used hallucination benchmarks and noisy data demonstrate that LPS significantly reduces the incidence of hallucinations compared to the baseline, showing exceptional performance, particularly in noisy settings."}
{"paperId": "53f02801b29bf5caf6c05c2b48fd4faa2919932b", "url": "https://www.semanticscholar.org/paper/53f02801b29bf5caf6c05c2b48fd4faa2919932b", "title": "Deep learning for change detection in remote sensing： A review and new outlooks", "venue": "National Remote Sensing Bulletin", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.11834/jrs.20254441?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.11834/jrs.20254441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2356763332", "name": "Gong Ceng"}, {"authorId": "2152581721", "name": "Guangxing Wang"}, {"authorId": "2146356257", "name": "Junwei Han"}], "abstract": "： Bi-temporal remote sensing image change detection stands as a prevalent direction in the realm of intellectual interpretation and applied research of remote sensing imagery. It aims to acquire information regarding changes in land cover types or geophysical attributes within a monitored area over a specified time span, according to practical application requirements. Over the past few years, remote sensing image change detection techniques have undergone a rapid evolution and upgrading, fueled by the synergetic forces of the ever-growing remote sensing big data (especially the proliferation of very-high-resolution remote sensing images) and the revolutionary advancements in deep learning. In this paper, we delve into and analyze the existing popular algorithms for common change detection tasks using bi-temporal very-high-resolution remote sensing images, encompassing binary change detection that aims to identify the presence or absence of changes, semantic change detection that delves deeper into the semantic categories of the changed areas, building damage assessment that applies to natural disasters, and change captioning that focuses on generating meaningful descriptions of the detected changes using natural language. Finally, we present an outlook on the pivotal research trends in remote sensing image change detection and highlight lingering open issues under the current developmental trajectory, with the intention of offering some valuable insights and perspectives for future research endeavors in this field. Through this review, we raise two observations as follows. 1) Deep learning solutions for the common change detection tasks in remote sensing have been continuously evolving, with many innovative algorithms proposed; 2) The advanced capabilities of current vision and language foundation models have subtly permeated into this field. In fact, the remote sensing domain itself has witnessed a highly encouraging development trend of foundation models. On this basis, remote sensing change detection tends to embark on new development opportunities, and would face a reshaping of its technological landscape in the future. Though, researchers have to spend efforts in developing specialized solutions to (1) enhance the reliability of change detection in complex scenes. At present, even for the relatively mature fully supervised binary change detection, the detection results often show a complete loss of changed entities when faced with some complex scenarios. In semantic change detection, there may even be a phenomenon where the segmented changed semantics are not consistent with the binary change detection results. (2) reduce the dependence on bi-temporal image registration. This requires new algorithms to be developed to adaptively handle small misalignments and deformations in the spatial positions of corresponding objects in bi-temporal images and complete change detection in this case. (3) improve the practicality of multi-modal change detection. This may ask for a comprehensive framework that integrates multi-modal information extraction, feature registration and fusion, and change detection. Meanwhile, semi-supervised or weakly supervised learning methods are expected to become a research focus due to the annotation difficulty of heterogeneous remote sensing data. Besides, it is also believed that the synergy between image modality and language modality has broad prospects for future research in remote sensing change detection, owing to the increasing advancement of large language models as well as vision and remote sensing foundation models."}
{"paperId": "544040e48ca0461262af20a1b6ee17f56162ae38", "url": "https://www.semanticscholar.org/paper/544040e48ca0461262af20a1b6ee17f56162ae38", "title": "Detection of APTs by Machine Learning: A Performance Comparison", "venue": "Expert systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/exsy.70181?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/exsy.70181, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-09", "authors": [{"authorId": "2338459010", "name": "Marcos Luengo Viñuela"}, {"authorId": "2136493309", "name": "Jesús-Ángel Román-Gallego"}, {"authorId": "1403181188", "name": "M. Pérez-Delgado"}, {"authorId": "2058263293", "name": "Miguel Á. Conde"}, {"authorId": "1422220444", "name": "M. Vega-Hernández"}, {"authorId": "2398371317", "name": "Hernando Silva Varela"}], "abstract": "Recent advances in machine learning and deep learning have significantly impacted multiple domains, including computer vision, natural language processing and cybersecurity. In the context of increasingly sophisticated Advanced Persistent Threats (APTs), deep learning models have shown strong potential for network intrusion detection by addressing the limitations of traditional methods. This study presents a comparative evaluation of classical and deep learning models for APT detection, highlighting the ability of deep architectures, such as Convolutional Neural Networks and Long Short‐Term Memory networks, to automatically extract complex temporal and spatial patterns from network traffic data. A key objective is to maximise detection accuracy while minimising false positives and false negatives. Experimental results show that Convolutional Neural Networks applied to the SCVIC‐APT‐2021 dataset achieved outstanding performance, with 99.24% accuracy, 99.39% precision, 99.24% recall and a 99.24% F1‐score. These results confirm the robustness of deep learning techniques for APT detection and underscore their effectiveness in identifying malicious activity in modern network environments."}
{"paperId": "5516f0690143a121987010f8e1a964d01ba6ce18", "url": "https://www.semanticscholar.org/paper/5516f0690143a121987010f8e1a964d01ba6ce18", "title": "A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing Image Captioning", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-06", "authors": [{"authorId": "2273820877", "name": "Qing Zhou"}, {"authorId": "2348741902", "name": "Tao Yang"}, {"authorId": "2302531336", "name": "Junyu Gao"}, {"authorId": "2063979", "name": "W. Ni"}, {"authorId": "46366033", "name": "Junzheng Wu"}, {"authorId": "2302724276", "name": "Qi Wang"}], "abstract": "Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision and language, aimed at automatically generating natural language descriptions of features and scenes in remote sensing imagery. Despite significant advances in developing sophisticated methods and large-scale datasets for training vision-language models (VLMs), two critical challenges persist: the scarcity of non-English descriptive datasets and the lack of multilingual capability evaluation for models. These limitations fundamentally impede the progress and practical deployment of RSIC, particularly in the era of large VLMs. To address these challenges, this paper presents several significant contributions to the field. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image Captioning), a comprehensive bilingual dataset that enriches three established English RSIC datasets with Chinese descriptions, encompassing 13,634 images paired with 68,170 bilingual captions. Building upon this foundation, we develop a systematic evaluation framework that addresses the prevalent inconsistency in evaluation protocols, enabling rigorous assessment of model performance through standardized retraining procedures on BRSIC. Furthermore, we present an extensive empirical study of eight state-of-the-art large vision-language models (LVLMs), examining their capabilities across multiple paradigms including zero-shot inference, supervised fine-tuning, and multi-lingual training. This comprehensive evaluation provides crucial insights into the strengths and limitations of current LVLMs in handling multilingual remote sensing tasks. Additionally, our cross-dataset transfer experiments reveal interesting findings. The code and data will be available at https://github.com/mrazhou/BRSIC."}
{"paperId": "55500cc90c21dbb0062ff4d1973ba910b5afb46b", "url": "https://www.semanticscholar.org/paper/55500cc90c21dbb0062ff4d1973ba910b5afb46b", "title": "Scaling Laws for Uncertainty in Deep Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-11", "authors": [{"authorId": "2366427632", "name": "Mattia Rosso"}, {"authorId": "47628623", "name": "Simone Rossi"}, {"authorId": "2258552880", "name": "Giulio Franzese"}, {"authorId": "2303463736", "name": "Markus Heinonen"}, {"authorId": "2266840528", "name": "M. Filippone"}], "abstract": "Deep learning has recently revealed the existence of scaling laws, demonstrating that model performance follows predictable trends based on dataset and model sizes. Inspired by these findings and fascinating phenomena emerging in the over-parameterized regime, we examine a parallel direction: do similar scaling laws govern predictive uncertainties in deep learning? In identifiable parametric models, such scaling laws can be derived in a straightforward manner by treating model parameters in a Bayesian way. In this case, for example, we obtain $O(1/N)$ contraction rates for epistemic uncertainty with respect to the number of data $N$. However, in over-parameterized models, these guarantees do not hold, leading to largely unexplored behaviors. In this work, we empirically show the existence of scaling laws associated with various measures of predictive uncertainty with respect to dataset and model sizes. Through experiments on vision and language tasks, we observe such scaling laws for in- and out-of-distribution predictive uncertainty estimated through popular approximate Bayesian inference and ensemble methods. Besides the elegance of scaling laws and the practical utility of extrapolating uncertainties to larger data or models, this work provides strong evidence to dispel recurring skepticism against Bayesian approaches:\"In many applications of deep learning we have so much data available: what do we need Bayes for?\". Our findings show that\"so much data\"is typically not enough to make epistemic uncertainty negligible."}
{"paperId": "555841ee0ef49adac035a5c33bbf7a2795bfbc64", "url": "https://www.semanticscholar.org/paper/555841ee0ef49adac035a5c33bbf7a2795bfbc64", "title": "Visual Question Answering on the Indian Heritage in Digital Space Dataset Using the BLIP Model", "venue": "Proceedings of the 3rd International Conference on Futuristic Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0013585800004664?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0013585800004664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "2382564770", "name": "Aryan Phadnis"}, {"authorId": "2382568802", "name": "Vibhu Revadi"}, {"authorId": "2382560903", "name": "Abhishek B R"}, {"authorId": "2382568713", "name": "Vinayak Neginhal"}, {"authorId": "2313112774", "name": "Uday Kulkarni"}, {"authorId": "2186822705", "name": "Shashank Hegde"}], "abstract": ": Visual Question Answering is a rapidly evolving domain in the field of artificial intelligence, which combines computer vision and natural language processing to understand and answer textual questions based on image content. Our approach involves the fine-tuning of the Bootstrapping Language-Image Pre-training model, a multimodal framework to address the problems between vision and language modalities. By using a pre-trained architecture, we can optimize the model for real-world applications through some task-specific adaptations. Our work highlights how such a model can address the practical challenges in Visual Question Answering tasks thus improving the alignment between the visual and textual modalities. Experimental re-sults on the test dataset created using unseen images and questions from the IHDS dataset show an accuracy of 86.42% and a weighted F1 score of 0.89, showing the effectiveness of our approach in enhancing VQA systems for any diverse and complex dataset. The integration of domain-specific datasets highlights the versatility of using fine-tuned models for addressing distinct challenges while also maintaining robust performance. Our proposed methodology demonstrates adaptability to a domain and also establishes a foundation for applying multimodal frameworks to culturally rich datasets."}
{"paperId": "5593cf2078adb5fb7a8fba7747b92c8daf13696e", "url": "https://www.semanticscholar.org/paper/5593cf2078adb5fb7a8fba7747b92c8daf13696e", "title": "Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16136, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-04-21", "authors": [{"authorId": "2356855861", "name": "Chiung-Yi Tseng"}, {"authorId": "2312846636", "name": "Jun-Jie Song"}, {"authorId": "2319608188", "name": "Ziqian Bi"}, {"authorId": "2323695882", "name": "Tianyang Wang"}, {"authorId": "2330158457", "name": "C. Liang"}, {"authorId": "2322495607", "name": "Ming Liu"}], "abstract": "In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning."}
{"paperId": "55a291abd10c22f02590de9b27869f5e3d872999", "url": "https://www.semanticscholar.org/paper/55a291abd10c22f02590de9b27869f5e3d872999", "title": "ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-09-19", "authors": [{"authorId": "2381746304", "name": "Zhaoyang Li"}, {"authorId": "49706114", "name": "Z. Ling"}, {"authorId": "2271278766", "name": "Yuchen Zhou"}, {"authorId": "2382452592", "name": "Hao Su"}], "abstract": "Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs. The code is available at https://github.com/ZhaoyangLi-1/ORIC."}
{"paperId": "55be063045eeb8953bd6319dcf778a943a789f33", "url": "https://www.semanticscholar.org/paper/55be063045eeb8953bd6319dcf778a943a789f33", "title": "Adam Simplified: Bias Correction Debunked", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.20516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-25", "authors": [{"authorId": "2394165256", "name": "Sam Laing"}, {"authorId": "2326112647", "name": "Antonio Orvieto"}], "abstract": "The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $\\beta_1, \\beta_2 \\in [0,1)$. Our findings challenge the universal inclusion of this component."}
{"paperId": "562dfd11039a06404b32988f957f6113a3e05186", "url": "https://www.semanticscholar.org/paper/562dfd11039a06404b32988f957f6113a3e05186", "title": "FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-13", "authors": [{"authorId": "2272561548", "name": "Sabrina McCallum"}, {"authorId": "2130361903", "name": "Amit Parekh"}, {"authorId": "3444866", "name": "Alessandro Suglia"}], "abstract": "Current approaches to embodied AI tend to learn policies from expert demonstrations. However, without a mechanism to evaluate the quality of demonstrated actions, they are limited to learning from optimal behaviour, or they risk replicating errors and inefficiencies. While reinforcement learning offers one alternative, the associated exploration typically results in sacrificing data efficiency. This work explores how agents trained with imitation learning can learn robust representations from both optimal and suboptimal demonstrations when given access to constructive language feedback as a means to contextualise different modes of behaviour. We directly provide language feedback embeddings as part of the input sequence into a Transformer-based policy, and optionally complement the traditional next action prediction objective with auxiliary self-supervised learning objectives for feedback prediction. We test our approach on a range of embodied Vision-and-Language tasks in our custom BabyAI-XGen environment and show significant improvements in agents'compositional generalisation abilities and robustness, suggesting that our data-efficient method allows models to successfully convert suboptimal behaviour into learning opportunities. Overall, our results suggest that language feedback is a competitive and intuitive alternative to intermediate scalar rewards for language-specified embodied tasks."}
{"paperId": "568b3c0f16409d5ba77eef091e0f87fb56852a04", "url": "https://www.semanticscholar.org/paper/568b3c0f16409d5ba77eef091e0f87fb56852a04", "title": "OSClip: Domain-Adaptive Prompt Tuning of Vision-Language Models for Open-Set Remote Sensing Image Classification", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2025.3617915?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2025.3617915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2384537379", "name": "Dingkang Peng"}, {"authorId": "2297329981", "name": "Xiaokang Zhang"}, {"authorId": "2325455431", "name": "Wanjing Wu"}, {"authorId": "2149145512", "name": "Xianping Ma"}, {"authorId": "2118684874", "name": "Weikang Yu"}], "abstract": "Remote sensing image classification models face significant challenges when adapting to new domains due to variations in image acquisition conditions, sensor types, and scene categories. Conventional domain adaptation methods rely on multistage adaptation pipelines with limited semantic understanding, and even recently developed vision-language models (VLMs) still exhibit limited discriminative capability when encountering unseen images. To tackle these challenges, we propose OSClip, a novel open-set domain adaptation framework based on the VLM model, contrastive language-image pre-training (CLIP). Specifically, OSClip harnesses the powerful generalization capabilities of CLIP by employing domain-adaptive prompt tuning, which inserts lightweight, learnable prompts into both the vision and language encoders. This design enables efficient adaptation to new, unlabeled target domains while retaining knowledge acquired during pretraining. Furthermore, a robust open-set recognition mechanism is incorporated by combining confidence-weighted pseudolabel supervision and energy-based regularization, further strengthened by a teacher–student self-distillation scheme to enhance pseudolabel reliability under unsupervised conditions. To support adaptation across multiple target domains while mitigating catastrophic forgetting, OSClip adopts a continual adaptation paradigm for the blended test set. It dynamically aggregates prompts based on the distribution of domain-specific features to ensure stable knowledge transfer. Extensive experiments on public remote sensing datasets demonstrate that OSClip consistently outperforms state-of-the-art methods, delivering superior accuracy in distinguishing known and unknown classes across various adaptation scenarios. The results also confirm the effectiveness of OSClip in achieving robust cross-modal and cross-domain semantic alignment."}
{"paperId": "5695f55c4359125bee7d8b885608619f0d9f3a69", "url": "https://www.semanticscholar.org/paper/5695f55c4359125bee7d8b885608619f0d9f3a69", "title": "Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model", "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.04715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-05", "authors": [{"authorId": "2147439322", "name": "Zelu Qi"}, {"authorId": "2292185003", "name": "Ping Shi"}, {"authorId": "2349633160", "name": "Chaoyang Zhang"}, {"authorId": "2308366474", "name": "Shuqi Wang"}, {"authorId": "2310299869", "name": "Fei Zhao"}, {"authorId": "2308283219", "name": "Da Pan"}, {"authorId": "29444563", "name": "Zefeng Ying"}], "abstract": "The development of AI-Generated Video (AIGV) technology has been remarkable in recent years, significantly transforming the paradigm of video content production. However, AIGVs still suffer from noticeable visual quality defects, such as noise, blurriness, frame jitter and low dynamic degree, which severely impact the user's viewing experience. Therefore, an effective automatic visual quality assessment is of great importance for AIGV content regulation and generative model improvement. In this work, we decompose the visual quality of AIGVs into three dimensions: technical quality, motion quality, and video semantics. For each dimension, we design corresponding encoder to achieve effective feature representation. Moreover, considering the outstanding performance of large language models (LLMs) in various vision and language tasks, we introduce a LLM as the quality regression module. To better enable the LLM to establish reasoning associations between multi-dimensional features and visual quality, we propose a specially designed multi-modal prompt engineering framework. Additionally, we incorporate LoRA fine-tuning technology during the training phase, allowing the LLM to better adapt to specific tasks. Our proposed method achieved second place in the NTIRE 2025 Quality Assessment of AIGenerated Content Challenge: Track 2 AI Generated video, demonstrating its effectiveness. Codes can be obtained at AIGVEval."}
{"paperId": "5698029ec5ae6e0142fcfe6cbdff5801c3520cf0", "url": "https://www.semanticscholar.org/paper/5698029ec5ae6e0142fcfe6cbdff5801c3520cf0", "title": "VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.21557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-26", "authors": [{"authorId": "2356550345", "name": "Hui Zhou"}, {"authorId": "2394525401", "name": "Siyuan Huang"}, {"authorId": "2305744009", "name": "Minxing Li"}, {"authorId": "2394564631", "name": "Hao Zhang"}, {"authorId": "2397439462", "name": "Lue Fan"}, {"authorId": "2396745787", "name": "Shaoshuai Shi"}], "abstract": "Vision Language Action models have significantly advanced general purpose robotic manipulation by harnessing large scale pretrained vision and language representations. Among existing approaches, a majority of current VLA systems employ parallel two finger grippers as their default end effectors. However, such grippers face inherent limitations in handling certain real world tasks such as wiping glass surfaces or opening drawers without handles due to insufficient contact area or lack of adhesion. To overcome these challenges, we present a low cost, integrated hardware design that combines a mechanical two finger gripper with a vacuum suction unit, enabling dual mode manipulation within a single end effector. Our system supports flexible switching or synergistic use of both modalities, expanding the range of feasible tasks. We validate the efficiency and practicality of our design within two state of the art VLA frameworks: DexVLA and Pi0. Experimental results demonstrate that with the proposed hybrid end effector, robots can successfully perform multiple complex tasks that are infeasible for conventional two finger grippers alone. All hardware designs and controlling systems will be released."}
{"paperId": "57208d96e5433d38413f5e0dba9e9b4bb3ac0207", "url": "https://www.semanticscholar.org/paper/57208d96e5433d38413f5e0dba9e9b4bb3ac0207", "title": "PENEX: AdaBoost-Inspired Neural Network Regularization", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02107, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2162194709", "name": "Klaus-Rudolf Kladny"}, {"authorId": "2303256458", "name": "Bernhard Scholkopf"}, {"authorId": "46888248", "name": "Michael Muehlebach"}], "abstract": "AdaBoost sequentially fits so-called weak learners to minimize an exponential loss, which penalizes mislabeled data points more severely than other loss functions like cross-entropy. Paradoxically, AdaBoost generalizes well in practice as the number of weak learners grows. In the present work, we introduce Penalized Exponential Loss (PENEX), a new formulation of the multi-class exponential loss that is theoretically grounded and, in contrast to the existing formulation, amenable to optimization via first-order methods. We demonstrate both empirically and theoretically that PENEX implicitly maximizes margins of data points. Also, we show that gradient increments on PENEX implicitly parameterize weak learners in the boosting framework. Across computer vision and language tasks, we show that PENEX exhibits a regularizing effect often better than established methods with similar computational cost. Our results highlight PENEX's potential as an AdaBoost-inspired alternative for effective training and fine-tuning of deep neural networks."}
{"paperId": "5797568a3c3352b89773c7069c892da490f90d70", "url": "https://www.semanticscholar.org/paper/5797568a3c3352b89773c7069c892da490f90d70", "title": "Ground-Level Viewpoint Vision-and-Language Navigation in Continuous Environments", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.19024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2323374275", "name": "Zerui Li"}, {"authorId": "2218747386", "name": "Gengze Zhou"}, {"authorId": "2305123085", "name": "Haodong Hong"}, {"authorId": "2349766762", "name": "Yanyan Shao"}, {"authorId": "2323371268", "name": "Wenqi Lyu"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2304606995", "name": "Qi Wu"}], "abstract": "Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment is still challenging. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a lowheight field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Groundlevel Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots."}
{"paperId": "57bc38df9d33dbd4c1106f58b4bdf3e206a5a0d5", "url": "https://www.semanticscholar.org/paper/57bc38df9d33dbd4c1106f58b4bdf3e206a5a0d5", "title": "A Modular Vision-Language-Action Framework for Autonomous Pressure Ulcer Care", "venue": "IEEE International Conference on Cyborg and Bionic Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CBS65871.2025.11267604?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CBS65871.2025.11267604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-17", "authors": [{"authorId": "2397827972", "name": "Tianze Wang"}, {"authorId": "2152824745", "name": "An-Chi Wang"}, {"authorId": "2307480327", "name": "Yan Ma"}, {"authorId": null, "name": "Jiewen Lai"}, {"authorId": "2352886234", "name": "Jiankun Wang"}, {"authorId": null, "name": "Hongliang Ren"}], "abstract": "Pressure ulcers represent a significant healthcare challenge with high prevalence rates and substantial caregiver burden. This paper introduces an autonomous robotic system for pressure ulcer care that operates through a novel hierarchical Vision-Language-Action (VLA) architecture. Unlike end-to-end approaches, our modular three-component design enhances interpretability and adaptability while reducing dependence on large domain-specific datasets. The system integrates an RGB-D camera for spatial perception, Qwen3 language model for instruction interpretation, and a ROS-controlled DENSO Cobotta robotic arm for precise interventions. Through in-context learning techniques, our approach effectively overcomes domain knowledge limitations in both vision and language components. Laboratory experiments with medical mannequins demonstrate the system achieves an 87.51% success rate across diverse tasks, including joint movement, object manipulation, and simulated wound irrigation. Comparative analysis reveals clear performance improvements through in-context learning, with success rates increasing from near-zero to over 85% for complex tasks. As the first implementation of a VLA-driven robotic system for wound care, this research addresses the critical gap between surgical robotics and nursing assistance, potentially enhancing treatment consistency while reducing healthcare worker workload in clinical settings."}
{"paperId": "58bc2a3427b42bc4f6b5401b29eefac8cb0f1e47", "url": "https://www.semanticscholar.org/paper/58bc2a3427b42bc4f6b5401b29eefac8cb0f1e47", "title": "IntelliRMS: A Robotic Manipulation System for Domain-Specific Tasks Using Vision and Language Foundational Models", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA55743.2025.11127298?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA55743.2025.11127298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2354524840", "name": "Chandan Kumar Singh"}, {"authorId": "2354536494", "name": "Devesh Kumar"}, {"authorId": "2147433316", "name": "Vipul Sanap"}, {"authorId": "2378879883", "name": "Mayank Khandelwal"}, {"authorId": "2354334329", "name": "Rajesh Sinha"}], "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced machines' ability to understand and follow human instructions. In many tasks, LLMs have demonstrated performance that rivals human-level common sense. However, directly applying LLMs to domain-specific use cases, such as robotic pick-and-place, remains a challenge. Tasks that are intuitive for humans, who rely on prior knowledge and skills, become complex for robots. Industrial robotic applications like pick-and-place require a high degree of accuracy, often exceeding 90 %. In response to these challenges in domain-specific applications, we propose IntelliRMS, a novel system-oriented architecture for instruction-following robotic manipulation. The IntelliRMS synergizes the linguistic and open-vocabulary visual capabilities of foundational models to arrive at an accurate, robust and scalable system. Further, we demonstrate the effectiveness of IntelliRMS in a real-world industrial Bin-picking scenario within the retail sector, validating its performance with a comprehensive dataset."}
{"paperId": "58ea5921b909a43350896937bf2d9fec4963b7c2", "url": "https://www.semanticscholar.org/paper/58ea5921b909a43350896937bf2d9fec4963b7c2", "title": "Subspace-Boosted Model Merging", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.16506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-19", "authors": [{"authorId": "2367741138", "name": "Ronald Skorobogat"}, {"authorId": "2258553172", "name": "Karsten Roth"}, {"authorId": "41021255", "name": "Mariana-Iuliana Georgescu"}, {"authorId": "1854487018", "name": "Zeynep Akata"}], "abstract": "Model merging enables the combination of multiple specialized expert models into a single model capable of performing multiple tasks. However, the benefits of merging an increasing amount of specialized experts generally lead to diminishing returns and reduced overall performance gains. In this work, we offer an explanation and analysis from a task arithmetic perspective; revealing that as the merging process (across numerous existing merging methods) continues for more and more experts, the associated task vector space experiences rank collapse. To mitigate this issue, we introduce Subspace Boosting, which operates on the singular value decomposed task vector space and maintains task vector ranks. Subspace Boosting raises merging efficacy for up to 20 expert models by large margins of more than 10% when evaluated on both vision and language benchmarks. Moreover, we propose employing Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging."}
{"paperId": "58ef0b0d3a7203f3f6620470609cfdc3c200b7de", "url": "https://www.semanticscholar.org/paper/58ef0b0d3a7203f3f6620470609cfdc3c200b7de", "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "venue": "IEEE Transactions on Medical Imaging", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08990, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-02", "authors": [{"authorId": "2362273135", "name": "Chenyu Lian"}, {"authorId": "2325660953", "name": "Hong-Yu Zhou"}, {"authorId": "2365334315", "name": "Dongyun Liang"}, {"authorId": "2114046891", "name": "Jing Qin"}, {"authorId": "2254332860", "name": "Liansheng Wang"}], "abstract": "Medical vision-language alignment through cross-modal contrastive learning shows promising performance in image-text matching tasks, such as retrieval and zero-shot classification. However, conventional cross-modal contrastive learning (CLIP-based) methods suffer from suboptimal visual representation capabilities, which also limits their effectiveness in vision-language alignment. In contrast, although the models pretrained via multimodal masked modeling struggle with direct cross-modal matching, they excel in visual representation. To address this contradiction, we propose ALTA (ALign Through Adapting), an efficient medical vision-language alignment method that utilizes only about 8% of the trainable parameters and less than 1/5 of the computational consumption required for masked record modeling. ALTA achieves superior performance in vision-language matching tasks like retrieval and zero-shot classification by adapting the pretrained vision model from masked record modeling. Additionally, we integrate temporal-multiview radiograph inputs to enhance the information consistency between radiographs and their corresponding descriptions in reports, further improving the vision-language alignment. Experimental evaluations show that ALTA outperforms the best-performing counterpart by over 4% absolute points in text-to-image accuracy and approximately 6% absolute points in image-to-text retrieval accuracy. The adaptation of vision-language models during efficient alignment also promotes better vision and language understanding. Code is publicly available at https://github.com/DopamineLcy/ALTA."}
{"paperId": "594f0b4ed017a064e7b63285d12d886bebe247fb", "url": "https://www.semanticscholar.org/paper/594f0b4ed017a064e7b63285d12d886bebe247fb", "title": "Vision and language representations in multimodal AI models and human social brain regions during natural movie viewing", "venue": "UniReps", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2323206132", "name": "Hannah Small"}, {"authorId": "40206566", "name": "Haemy Lee Masson"}, {"authorId": "2376335154", "name": "Stewart Mostofsky"}, {"authorId": "2254394810", "name": "Leyla Isik"}], "abstract": null}
{"paperId": "59aca96a6a64bbf9f1a247faa80977771222c117", "url": "https://www.semanticscholar.org/paper/59aca96a6a64bbf9f1a247faa80977771222c117", "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-11", "authors": [{"authorId": "2315308131", "name": "Runxi Cheng"}, {"authorId": "2349461907", "name": "Feng Xiong"}, {"authorId": "2268462471", "name": "Yongxian Wei"}, {"authorId": "2315456208", "name": "Wanyun Zhu"}, {"authorId": "2333235284", "name": "Chun Yuan"}], "abstract": "Model merging seeks to integrate task-specific expert models into a unified architecture while preserving multi-task generalization capabilities, yet parameter interference between constituent models frequently induces performance degradation. Although prior work has explored many merging strategies, resolving interference without additional data for retraining or test-time computation remains challenging. In this paper, we theoretically demonstrate that the task vectors of the linear layer constitute an approximate linear subspace for its corresponding input. Therefore, we can minimize interference under the guidance of task vectors. Based on this insight, we propose \\textbf{WUDI-Merging} (\\textbf{W}hoever started the interference sho\\textbf{U}ld en\\textbf{D} \\textbf{I}t), a simple yet effective model merging method that eliminates interference without any additional data or rescaling coefficients. Comprehensive empirical evaluations across vision and language benchmarks demonstrate our method's superiority, achieving state-of-the-art performance in data-free model merging scenarios (average 10.9\\% improvement versus baseline methods) while even outperforming mainstream test-time adaptation approaches by 3.3\\%, and only very few computing resources are required. The code will be publicly available soon."}
{"paperId": "59c5df7c848c8339cd7f9c18c5e6402a868237fe", "url": "https://www.semanticscholar.org/paper/59c5df7c848c8339cd7f9c18c5e6402a868237fe", "title": "Research on Autonomous Vehicle Lane-Keeping and Navigation System Based on Deep Reinforcement Learning: From Simulation to Real-World Application", "venue": "Electronics", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics14132738?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics14132738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-07", "authors": [{"authorId": "2347587293", "name": "Chia-Hsin Cheng"}, {"authorId": "2373557073", "name": "Hsiang-Hao Lin"}, {"authorId": "2346890604", "name": "Yu-Yong Luo"}], "abstract": "In recent years, with the rapid development of science and technology and the substantial improvement of computing power, various deep learning research topics have been promoted. However, existing autonomous driving technologies still face significant challenges in achieving robust lane-keeping and navigation performance, especially when transferring learned models from simulation to real-world environments due to environmental complexity and domain gaps. Many fields such as computer vision, natural language processing, and medical imaging have also accelerated their development due to the emergence of this wave, and the field of self-driving cars is no exception. The trend of self-driving cars is unstoppable. Many technology companies and automobile manufacturers have invested a lot of resources in the research and development of self-driving technology. With the emergence of different levels of self-driving cars, most car manufacturers have already reached the L2 level of self-driving classification standards and are moving towards L3 and L4 levels. This study applies deep reinforcement learning (DRL) to train autonomous vehicles with lane-keeping and navigation capabilities. Through simulation training and Sim2Real strategies, including domain randomization and CycleGAN, the trained models are evaluated in real-world environments to validate performance. The results demonstrate the feasibility of DRL-based autonomous driving and highlight the challenges in transferring models from simulation to reality."}
{"paperId": "59c633eaf4a3cec9353cf8fae02b3754724d7ec5", "url": "https://www.semanticscholar.org/paper/59c633eaf4a3cec9353cf8fae02b3754724d7ec5", "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-16", "authors": [{"authorId": "2328798215", "name": "Hangyu Zhou"}, {"authorId": "2261493215", "name": "Aaron Gokaslan"}, {"authorId": "2257271109", "name": "Volodymyr Kuleshov"}, {"authorId": "73710317", "name": "B. Hariharan"}], "abstract": "From a multi-model compression perspective, model merging enables memory-efficient serving of multiple models fine-tuned from the same base, but suffers from degraded performance due to interference among their task-specific parameter adjustments (i.e., deltas). In this paper, we reformulate model merging as a compress-and-retrieve scheme, revealing that the task interference arises from the summation of irrelevant deltas during model retrieval. To address this issue, we use random orthogonal transformations to decorrelate these vectors into self-cancellation. We show that this approach drastically reduces interference, improving performance across both vision and language tasks. Since these transformations are fully defined by random seeds, adding new models requires no extra memory. Further, their data- and model-agnostic nature enables easy addition or removal of models with minimal compute overhead, supporting efficient and flexible multi-model serving."}
{"paperId": "59cbf25fbc3f4a3c1b30d3af8af5b17148e2dd3f", "url": "https://www.semanticscholar.org/paper/59cbf25fbc3f4a3c1b30d3af8af5b17148e2dd3f", "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22653, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-26", "authors": [{"authorId": "2305019052", "name": "Chih-Yao Hu"}, {"authorId": "2383184778", "name": "Yang-Sen Lin"}, {"authorId": "2382784675", "name": "Yuna Lee"}, {"authorId": "2304899166", "name": "Chih-Hai Su"}, {"authorId": "2311274155", "name": "Jie-Ying Lee"}, {"authorId": "2323199463", "name": "Shr-Ruei Tsai"}, {"authorId": "2309205916", "name": "Chin-Yang Lin"}, {"authorId": "2382802097", "name": "Kuan-Wen Chen"}, {"authorId": "2382767788", "name": "Tsung-Wei Ke"}, {"authorId": "2309657159", "name": "Yu-Lun Liu"}], "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev"}
{"paperId": "59d6bebd55d2250fa2cff497c47329b6c997d1cf", "url": "https://www.semanticscholar.org/paper/59d6bebd55d2250fa2cff497c47329b6c997d1cf", "title": "Memory-Efficient Fine-Tuning via Low-Rank Activation Compression", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-27", "authors": [{"authorId": "2153118899", "name": "Jiang-Xin Shi"}, {"authorId": "2319387953", "name": "Wen-Da Wei"}, {"authorId": "2384025605", "name": "Jin-Fei Qi"}, {"authorId": "2382945046", "name": "Xuanyu Chen"}, {"authorId": "2068206353", "name": "Tong Wei"}, {"authorId": "2327825016", "name": "Yu-Feng Li"}], "abstract": "The parameter-efficient fine-tuning paradigm has garnered significant attention with the advancement of foundation models. Although numerous methods have been proposed to reduce the number of trainable parameters, their substantial memory overhead remains a critical bottleneck that hinders practical deployment. In this paper, we observe that model activations constitute a major source of memory consumption, especially under large batch sizes and long context lengths; however, the rank of the activations remains consistently low. Motivated by this insight, we propose a memory-efficient fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior work, LoRAct provides a more flexible and versatile compressing strategy that can be applied online during the forward pass without the need for any calibration data. Moreover, LoRAct incorporates a novel sampling-based orthogonal decomposition algorithm specifically designed for low-rank matrices, offering improved computational efficiency and a tighter error bound compared to the widely used RSVD. Experiments on both vision and language tasks demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces activation memory by approximately 80% in comparison with the widely adopted LoRA method, while maintaining competitive performance. The source code is available at https://github.com/shijxcs/meft."}
{"paperId": "59ee9510111c9174c2c2812e1ccfe31f347f1a6f", "url": "https://www.semanticscholar.org/paper/59ee9510111c9174c2c2812e1ccfe31f347f1a6f", "title": "Embodied Navigation Foundation Model", "venue": "arXiv.org", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-15", "authors": [{"authorId": "2107990526", "name": "Jiazhao Zhang"}, {"authorId": "2364873725", "name": "Anqi Li"}, {"authorId": "2281974382", "name": "Yunpeng Qi"}, {"authorId": "2333311294", "name": "Minghan Li"}, {"authorId": "2364072653", "name": "Jiahang Liu"}, {"authorId": "2335081438", "name": "Shaoan Wang"}, {"authorId": "2333402095", "name": "Haoran Liu"}, {"authorId": "2218747386", "name": "Gengze Zhou"}, {"authorId": "2274079622", "name": "Yuze Wu"}, {"authorId": "2380899125", "name": "Xingxing Li"}, {"authorId": "2380634301", "name": "Yuxin Fan"}, {"authorId": "2380698509", "name": "Wenjun Li"}, {"authorId": "2380571542", "name": "Zhibo Chen"}, {"authorId": "2380640598", "name": "Fei Gao"}, {"authorId": "2272649520", "name": "Qi Wu"}, {"authorId": "2287041015", "name": "Zhizheng Zhang"}, {"authorId": "2334325487", "name": "He Wang"}], "abstract": "Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach."}
{"paperId": "5a1300b6774619524ef30eaabc96c95757db029a", "url": "https://www.semanticscholar.org/paper/5a1300b6774619524ef30eaabc96c95757db029a", "title": "Influence of Dementia on Vision-Related Functional Performance Among Patients With Type 2 Diabetes.", "venue": "American Journal of Occupational Therapy", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5014/ajot.2025.050631?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5014/ajot.2025.050631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-23", "authors": [{"authorId": "5887460", "name": "Li-Ting Tsai"}, {"authorId": "153246212", "name": "Chung-Sen Chen"}, {"authorId": "2357299107", "name": "Chia-Wei Hung"}, {"authorId": "2357065390", "name": "I-Mo Fang"}, {"authorId": "2296297980", "name": "Kuo-Meng Liao"}], "abstract": "IMPORTANCE\nComplications of Type 2 diabetes mellitus (T2DM) leading to vision loss may increase the risk of dementia. The relationship between diabetic retinopathy severity and visual acuity (VA) has been explored, but the impact of dementia on vision-related functional performance in patients with T2DM is less understood.\n\n\nOBJECTIVE\nTo investigate the association of diabetes-related eye problems with dementia and the impact of dementia on vision-related quality of life (VRQoL) and activities of daily living (ADLs) in patients with T2DM.\n\n\nDESIGN\nRetrospective cohort and nested case-control study.\n\n\nSETTING\nHealth care institution.\n\n\nPARTICIPANTS\nSubstudy 1 included 4,454 patients with T2DM. In Substudy 2, 33 patients with T2DM and dementia (male, n = 15; M age = 78.7 yr) were compared with 67 matched control participants (male, n = 36; M age = 76.6 yr).\n\n\nOUTCOMES AND MEASURES\nPatients with and without dementia were assessed with the 25-item National Eye Institute Visual Function Questionnaire (NEI-VFQ 25) and the Revised Self-Report Assessment of Functional Visual Performance (R-SRAFVP).\n\n\nRESULTS\nSubstudy 1 showed a borderline significant association between proliferative diabetic retinopathy and dementia. In Substudy 2, functional vision, particularly in the overall scales and three subscales of the R-SRAFVP and four subscales of the NEI-VFQ 25, declined significantly among patients with T2DM and dementia, but no significant differences were found in VA.\n\n\nCONCLUSIONS AND RELEVANCE\nThe findings illustrate the complex relationships among T2DM, dementia, VRQoL, and vision-dependent ADL and suggest that occupational therapists who care for patients with T2DM and dementia should pay close attention to patients' functional vision. Plain-Language Summary: Complications of Type 2 diabetes mellitus (T2DM) that lead to vision loss may increase the risk of dementia. People with T2DM and dementia show a significant decline in functional vision. This study investigated the relationship between diabetes-related eye problems and dementia as well as the impact of dementia on vision-related quality of life and activities of daily living for patients with T2DM. The study demonstrates the complex relationships among dementia, T2DM, eye conditions, and vision-related function. The results highlight the importance of a functional vision assessment for patients with T2DM and dementia. Occupational therapists who care for patients with T2DM and dementia should pay close attention to patients' functional vision, which will guide them in assessment and intervention planning."}
{"paperId": "5a89c1080c95533e9c2dcad6377616f74be07ea2", "url": "https://www.semanticscholar.org/paper/5a89c1080c95533e9c2dcad6377616f74be07ea2", "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.06564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-07-08", "authors": [{"authorId": "2372466780", "name": "Tianshun Li"}, {"authorId": "2373025694", "name": "Tianyi Huai"}, {"authorId": "2374467343", "name": "Zhen Li"}, {"authorId": "2374087322", "name": "Yichun Gao"}, {"authorId": "2384363611", "name": "Haoang Li"}, {"authorId": "2367145350", "name": "Xinhu Zheng"}], "abstract": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments."}
{"paperId": "5ae54c475b41976f4773f2cc6e771ef337d8c2e2", "url": "https://www.semanticscholar.org/paper/5ae54c475b41976f4773f2cc6e771ef337d8c2e2", "title": "Mathematical Foundations of Deep Learning: Theory, Algorithms, and Practical Applications", "venue": "Panamerican Mathematical Journal", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.52783/pmj.v35.i3s.4201?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.52783/pmj.v35.i3s.4201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-20", "authors": [{"authorId": "2349820712", "name": "Dr. T. Rameshkumar"}, {"authorId": "2349832356", "name": "Dr. L. Jesmalar"}, {"authorId": "2161923432", "name": "Venkateswara Rao Cheekati"}, {"authorId": "2349829112", "name": "J. P. Priya"}, {"authorId": "66809927", "name": "Mihirkumar B. Suthar"}, {"authorId": "2349820709", "name": "M. Elumalai"}, {"authorId": "2349829034", "name": "Dr. T. Vengatesh"}, {"authorId": "2349881303", "name": "Dr. B. Anbuselvan"}], "abstract": "Deep learning has revolutionized the field of artificial intelligence, enabling breakthroughs in computer vision, natural language processing, and reinforcement learning. This paper explores the mathematical foundations of deep learning, focusing on the theoretical underpinnings, algorithmic frameworks, and practical applications. We provide a rigorous treatment of key concepts, including optimization, generalization, and neural network architectures, supported by mathematical derivations and illustrative examples. The paper concludes with a discussion of open challenges and future directions in the field."}
{"paperId": "5b19317e0c991c8486ed435a7111e8cfc986a40b", "url": "https://www.semanticscholar.org/paper/5b19317e0c991c8486ed435a7111e8cfc986a40b", "title": "Usability Benchmarking of Data Analytic Tools with Market Research for Decision-Making", "venue": "Proceedings of the 3rd International Conference on Futuristic Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0013582400004664?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0013582400004664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "2382530441", "name": "Preethi Bitra"}, {"authorId": "2384302670", "name": "Mohan Devarakonda"}, {"authorId": "2384307320", "name": "Poodi Venkata"}, {"authorId": "52459992", "name": "Vijaya Durga"}, {"authorId": "2384304586", "name": "Jistnasai Upendra"}, {"authorId": "2384277968", "name": "L. N. P. Kumar"}], "abstract": ": Data is the key for any Data Analytics application. To be a master in various fields like Data Science, Machine Learning, Deep Learning, Computer Vision, Natural Language Processing, and Predictive analysis, everything depends on perfect data maintenance. Also, the overfitting and underfitting analyses of various popular models depend on the data that has been provided to them. To meet the requirements, several tools emerged in the form of software, web versions, and command-line applications to make it easier to absorb and analyze the data. Some of the latest data analysis tools include Tableau, Power BI, Alteryx, etc. Choosing the right tool to perform the data analysis will give you a bit more success with the result. This paper attempts to define the functionalities, advantages, and disadvantages of various data analysis tools on the market and also attempts to produce a choice chart to help you select the right tool for your data."}
{"paperId": "5b5abac76bb619de9fe22819b3fd02f14602c607", "url": "https://www.semanticscholar.org/paper/5b5abac76bb619de9fe22819b3fd02f14602c607", "title": "Task Matrices: Linear Maps for Cross-Model Finetuning Transfer", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.14880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-16", "authors": [{"authorId": "2399036829", "name": "Darrin O'Brien"}, {"authorId": "2399035210", "name": "Dhikshith Gajulapalli"}, {"authorId": "2399035795", "name": "Eric Xia"}], "abstract": "Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available."}
{"paperId": "5bbe977fd40c05116ab74d22e80af42588b09c58", "url": "https://www.semanticscholar.org/paper/5bbe977fd40c05116ab74d22e80af42588b09c58", "title": "FAR: Function-preserving Attention Replacement for IMC-friendly Inference", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-05-24", "authors": [{"authorId": "2365451549", "name": "Yuxin Ren"}, {"authorId": "2372224131", "name": "Maxwell D Collins"}, {"authorId": "2364560815", "name": "Miao Hu"}, {"authorId": "2334663584", "name": "Huanrui Yang"}], "abstract": "While transformers dominate modern vision and language models, their attention mechanism remains poorly suited for in-memory computing (IMC) devices due to intensive activation-to-activation multiplications and non-local memory access, leading to substantial latency and bandwidth overhead on ReRAM-based accelerators. To address this mismatch, we propose FAR, a Function-preserving Attention Replacement framework that substitutes all attention in pretrained DeiTs with sequential modules inherently compatible with IMC dataflows. Specifically, FAR replaces self-attention with a multi-head bidirectional LSTM architecture via block-wise distillation to retain functional equivalence while enabling linear-time computation and localized weight reuse. We further incorporate structured pruning on FAR models, enabling flexible adaptation to resource-constrained IMC arrays while maintaining functional fidelity. Evaluations on the DeiT family demonstrate that FAR maintains comparable accuracy to the original attention-based models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships learned by attention while improving computational efficiency, highlighting its potential for energy-efficient transformer inference on IMC-based edge accelerators."}
{"paperId": "5bde7c9f523fe31da647d9f028354c9196f91633", "url": "https://www.semanticscholar.org/paper/5bde7c9f523fe31da647d9f028354c9196f91633", "title": "The multimodal nature of development makes language multimodal: A commentary on Karadöller, Sümer and Özyürek", "venue": "First language", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/01427237251325631?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/01427237251325631, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-12", "authors": [{"authorId": "4367712", "name": "O. Capirci"}, {"authorId": "2283226675", "name": "Jana M Iverson"}], "abstract": "Multimodal visions of language acquisition have been present in the literature for many years, highlighting developmental continuity between infants’ early actions on objects, gestures, and words or signs. This framework has recently been expanded to include consideration of early object exploration and motor development and how the multimodal experiences they create for infants and infuse into social interactions support language development. This vision of multimodality in communication and language – firmly grounded in development and the fundamentally multimodal nature of infants’ early experiences and actions – can broaden our view of language and its emergence in infancy."}
{"paperId": "5bf42053e895754cdcebb55668c7ce262a16c321", "url": "https://www.semanticscholar.org/paper/5bf42053e895754cdcebb55668c7ce262a16c321", "title": "Deep Learning Techniques and Applications", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.po25828?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.po25828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-30", "authors": [{"authorId": "2375517847", "name": "Tangyi Huang"}], "abstract": "Deep learning has brought about a significant transformation in artificial intelligence, facilitating progress in various domains, including computer vision, natural language processing, and biomedical imaging. This paper undertakes a review of the fundamental breakthroughs, approaches, and uses of deep learning. The emphasis is placed on key architectures, namely convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformers, and generative models. By conducting an analysis of the existing body of literature and real - world case studies, this research delves into the advantages and drawbacks of these models. It underscores the far - reaching and revolutionary influence of these models while also tackling issues such as scalability, interpretability, and the high computational requirements. Key findings include the potential of emerging techniques like diffusion models and hybrid generative approaches to overcome current limitations. However, the study acknowledges limitations in its reliance on secondary sources and the scope of reviewed literature. Subsequent research stands to gain significantly from the incorporation of empirical investigations and more extensive data sets. This approach would offer a more holistic comprehension of the prospects and effects of deep learning. The objective of this paper is to offer guidance to researchers and professionals in the progress of both the theoretical and practical uses of deep learning."}
{"paperId": "5c0ed9f9a51eead755a96a896e8ec78521d2341e", "url": "https://www.semanticscholar.org/paper/5c0ed9f9a51eead755a96a896e8ec78521d2341e", "title": "ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.08417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-13", "authors": [{"authorId": "2360694581", "name": "Avihai Giuili"}, {"authorId": "2330245907", "name": "Rotem Atari"}, {"authorId": "38111870", "name": "A. Sintov"}], "abstract": "Grasping unknown objects in unstructured environments remains a fundamental challenge in robotics, requiring both semantic understanding and spatial reasoning. Existing methods often rely on dense training datasets or explicit geometric modeling, limiting their scalability to real-world tasks. Recent advances in Large Multimodal Models (LMMs) offer new possibilities for integrating vision and language understanding, but their application to autonomous robotic grasping remains largely unexplored. We present ORACLE-Grasp, a zero-shot framework that leverages LMMs as semantic oracles to guide grasp selection without requiring additional training or human input. The system formulates grasp prediction as a structured, iterative decision process, using dual-prompt tool calling to first extract high-level object context and then select task-relevant grasp regions. By discretizing the image space and reasoning over candidate areas, ORACLE-Grasp mitigates the spatial imprecision common in LMMs and produces human-like, task-driven grasp suggestions. Early stopping and depth-based refinement steps further enhance efficiency and physical grasp reliability. Experiments demonstrate that the predicted grasps achieve low positional and orientation errors relative to human-annotated ground truth and lead to high success rates in real-world pick up tasks. These results highlight the potential of combining language-driven reasoning with lightweight vision techniques to enable robust, autonomous grasping without task-specific datasets or retraining."}
{"paperId": "5c4388f12d74795c8c07a8063ba7a03da5be4713", "url": "https://www.semanticscholar.org/paper/5c4388f12d74795c8c07a8063ba7a03da5be4713", "title": "A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions", "venue": "arXiv.org", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-03-19", "authors": [{"authorId": "2267349343", "name": "Saddam Hussain Khan"}, {"authorId": "2267339416", "name": "Rashid Iqbal"}], "abstract": "Deep Convolutional Neural Networks (CNNs) have significantly advanced deep learning, driving breakthroughs in computer vision, natural language processing, medical diagnosis, object detection, and speech recognition. Architectural innovations including 1D, 2D, and 3D convolutional models, dilated and grouped convolutions, depthwise separable convolutions, and attention mechanisms address domain-specific challenges and enhance feature representation and computational efficiency. Structural refinements such as spatial-channel exploitation, multi-path design, and feature-map enhancement contribute to robust hierarchical feature extraction and improved generalization, particularly through transfer learning. Efficient preprocessing strategies, including Fourier transforms, structured transforms, low-precision computation, and weight compression, optimize inference speed and facilitate deployment in resource-constrained environments. This survey presents a unified taxonomy that classifies CNN architectures based on spatial exploitation, multi-path structures, depth, width, dimensionality expansion, channel boosting, and attention mechanisms. It systematically reviews CNN applications in face recognition, pose estimation, action recognition, text classification, statistical language modeling, disease diagnosis, radiological analysis, cryptocurrency sentiment prediction, 1D data processing, video analysis, and speech recognition. In addition to consolidating architectural advancements, the review highlights emerging learning paradigms such as few-shot, zero-shot, weakly supervised, federated learning frameworks and future research directions include hybrid CNN-transformer models, vision-language integration, generative learning, etc. This review provides a comprehensive perspective on CNN's evolution from 2015 to 2025, outlining key innovations, challenges, and opportunities."}
{"paperId": "5cc896e66d09f4add9708037b7b320d6754f4eee", "url": "https://www.semanticscholar.org/paper/5cc896e66d09f4add9708037b7b320d6754f4eee", "title": "Pre-contextualized Augmented Language Instructions for Autonomous Vision-and-Language Navigation", "venue": "International Conference on Database Theory", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDT63985.2025.10986598?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDT63985.2025.10986598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-03-07", "authors": [{"authorId": "2361125035", "name": "Olivia Piazza"}, {"authorId": "2347770007", "name": "Sudhir Shrestha"}], "abstract": "Advances in natural language has allowed for the automation of robotic tasks with minimal supervision, even in ambiguous situations. In the field of robotic navigation, human instruction can be utilized to allow a robotic agent to traverse unknown environments using instructional commands that mimic human-to-human interaction. This paper demonstrates the success of one such machine learning algorithms, delineating each machine learning architecture, and processes alternate fields where learning techniques can be applied for robotic automation."}
{"paperId": "5ce333bd200ba2067a24019f878423a8bf3b16a6", "url": "https://www.semanticscholar.org/paper/5ce333bd200ba2067a24019f878423a8bf3b16a6", "title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-10", "authors": [{"authorId": "2258602946", "name": "Jiazheng Liu"}, {"authorId": "2258682220", "name": "Sipeng Zheng"}, {"authorId": "2047947436", "name": "Börje F. Karlsson"}, {"authorId": "2258676670", "name": "Zongqing Lu"}], "abstract": "Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs."}
{"paperId": "5d2e8d47e6a63681ddcbe38cc108f82a346ea3d2", "url": "https://www.semanticscholar.org/paper/5d2e8d47e6a63681ddcbe38cc108f82a346ea3d2", "title": "Representations in vision and language converge in a shared, multidimensional space of perceived similarities", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.21871, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-29", "authors": [{"authorId": "2181730109", "name": "Katerina Marie Simkova"}, {"authorId": "2238278699", "name": "Adrien Doerig"}, {"authorId": "2323298475", "name": "Clayton Hickey"}, {"authorId": "2238376794", "name": "Ian Charest"}], "abstract": "Humans can effortlessly describe what they see, yet establishing a shared representational format between vision and language remains a significant challenge. Emerging evidence suggests that human brain representations in both vision and language are well predicted by semantic feature spaces obtained from large language models (LLMs). This raises the possibility that sensory systems converge in their inherent ability to transform their inputs onto shared, embedding-like representational space. However, it remains unclear how such a space manifests in human behaviour. To investigate this, sixty-three participants performed behavioural similarity judgements separately on 100 natural scene images and 100 corresponding sentence captions from the Natural Scenes Dataset. We found that visual and linguistic similarity judgements not only converge at the behavioural level but also predict a remarkably similar network of fMRI brain responses evoked by viewing the natural scene images. Furthermore, computational models trained to map images onto LLM-embeddings outperformed both category-trained and AlexNet controls in explaining the behavioural similarity structure. These findings demonstrate that human visual and linguistic similarity judgements are grounded in a shared, modality-agnostic representational structure that mirrors how the visual system encodes experience. The convergence between sensory and artificial systems suggests a common capacity of how conceptual representations are formed-not as arbitrary products of first order, modality-specific input, but as structured representations that reflect the stable, relational properties of the external world."}
{"paperId": "5d514f32c8d7bd0485e10e68227b1decaf8810a9", "url": "https://www.semanticscholar.org/paper/5d514f32c8d7bd0485e10e68227b1decaf8810a9", "title": "Visual Perturbation for Text-Based Person Search", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i10.33091", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i10.33091?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i10.33091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2239091384", "name": "Pengcheng Zhang"}, {"authorId": "2290919968", "name": "Xiaohan Yu"}, {"authorId": "2290908564", "name": "Xiao Bai"}, {"authorId": "2290970416", "name": "Jin Zheng"}], "abstract": "Text-based person search aims at locating a person described by natural language in uncropped scene images. Recent works for TBPS mainly focus on aligning multi-granularity vision and language representations, neglecting a key discrepancy between training and inference where the former learns to unify vision and language features where the visual side covers all clues described by language, yet the latter matches image-text pairs where the images may capture only part of the described clues due to perturbations such as occlusions, background clutters and misaligned boundaries. To alleviate this issue, we present ViPer: a Visual Perturbation network that learns to match language descriptions with perturbed visual clues. On top of a CLIP-driven baseline, we design three visual perturbation modules: (1) Spatial ViPer that varies person proposals and produces visual features with misaligned boundaries, (2) Attentive ViPer that estimates visual attention on the fly and manipulates attentive visual tokens within a proposal to produce global features under visual perturbations, and (3) Fine-grained ViPer that learns to recover masked visual clues from detailed language descriptions to encourage matching language features with perturbed visual features at the fine granularity. This overall framework thus simulates real-world scenarios at the training stage to minimize the discrepancy and improve the generalization ability of the model. Experimental results demonstrate that the proposed method clearly surpasses previous TBPS methods on the PRW-TBPS and CUHK-SYSU-TBPS datasets."}
{"paperId": "5de49cb60714272e70b453bfbe0b15b2b7a586ab", "url": "https://www.semanticscholar.org/paper/5de49cb60714272e70b453bfbe0b15b2b7a586ab", "title": "A Prompt That Can Stimulate the Self-Correction Ability of the VLM, Improving the Performance on VQA Task", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.kl21691?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.kl21691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2353166987", "name": "Tiange Lyu"}], "abstract": "Visual language Models (VLMS) are revolutionizing multimodal understanding by bridging the gap between vision and language, with great potential in diverse applications. How to improve its performance, so that it can better complete a variety of tasks has become the goal of researchers. To improve the performance of VLM on Visual Question answering (VQA) tasks, this paper proposes an innovative method, Self-correction Prompt, which integrates self-correction into prompt engineering by looping questions to improve accuracy while avoiding additional model training. Experiments on three typical VLMs show that Self-correction Prompt is effective, and the accuracy can be improved by 1.7% at most. It can also stimulate the self-detection ability of the model to find the previously made errors, and its efficiency can reach an average of 72%. The paper also discusses the types of tasks VLMs are not good at, which often prevent models from further improving their accuracy. The proposed method is simple and can be seamlessly integrated into a variety of VLMs, which provides a new idea for the following research on prompt engineering."}
{"paperId": "5de8798a63a89c4ad4beb5e0f39a2e8790deab44", "url": "https://www.semanticscholar.org/paper/5de8798a63a89c4ad4beb5e0f39a2e8790deab44", "title": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset", "venue": "Asian Conference on Intelligent Information and Database Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2313306294", "name": "Hoang-Loc La"}, {"authorId": "2313306064", "name": "Phuong Hoai Ha"}], "abstract": "Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS."}
{"paperId": "5e90173dd064e595458ac4c0f7cc2deb60d29955", "url": "https://www.semanticscholar.org/paper/5e90173dd064e595458ac4c0f7cc2deb60d29955", "title": "A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI", "venue": "arXiv.org", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-08", "authors": [{"authorId": "2159692145", "name": "Kazusato Oko"}, {"authorId": "2257668289", "name": "Licong Lin"}, {"authorId": "2306831128", "name": "Yuhang Cai"}, {"authorId": "2306258864", "name": "Song Mei"}], "abstract": "Multi-modal generative AI systems, such as those combining vision and language, rely on contrastive pre-training to learn representations across different modalities. While their practical benefits are widely acknowledged, a rigorous theoretical understanding of the contrastive pre-training framework remains limited. This paper develops a theoretical framework to explain the success of contrastive pre-training in downstream tasks, such as zero-shot classification, conditional diffusion models, and vision-language models. We introduce the concept of approximate sufficient statistics, a generalization of the classical sufficient statistics, and show that near-minimizers of the contrastive pre-training loss are approximately sufficient, making them adaptable to diverse downstream tasks. We further propose the Joint Generative Hierarchical Model for the joint distribution of images and text, showing that transformers can efficiently approximate relevant functions within this model via belief propagation. Building on this framework, we derive sample complexity guarantees for multi-modal learning based on contrastive pre-trained representations. Numerical simulations validate these theoretical findings, demonstrating the strong generalization performance of contrastively pre-trained transformers in various multi-modal tasks."}
{"paperId": "5eb2b025c3c021482fdb92b4d29f9de4afbddc0f", "url": "https://www.semanticscholar.org/paper/5eb2b025c3c021482fdb92b4d29f9de4afbddc0f", "title": "Vision meets language: a novel residual learning framework with LLM blocks for robust representations", "venue": "International Conference on Advances in Computer Science, Information Technology and Communications", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3093014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3093014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-11-21", "authors": [{"authorId": "2397442662", "name": "Xuena Ren"}, {"authorId": "2257404645", "name": "Haohui Sun"}, {"authorId": "2257480741", "name": "Wushen Li"}], "abstract": "Object representation learning is fundamental in computer vision but remains limited in capturing high-level semantics and fine-grained inter-class differences with purely visual models. To address this, we propose a novel framework that integrates pretrained Large Language Models (LLMs) into a Vision Transformer (ViT) backbone via a Gated Fusion Mechanism, serving as a semantic enhancer to adaptively inject language-derived knowledge while avoiding redundancy. A joint optimization with classification and metric learning losses further encourages compact intra-class and separable inter-class representations. Extensive experiments on Celeb-reID and DeepChange demonstrate state-of-the-art performance, highlighting the effectiveness of incorporating LLM semantics for robust and discriminative object representation, especially under significant appearance variations."}
{"paperId": "5ee1e65d2b69298e5beb059623589d076ab0dec5", "url": "https://www.semanticscholar.org/paper/5ee1e65d2b69298e5beb059623589d076ab0dec5", "title": "[CRAKUT:integrating contrastive regional attention and clinical prior knowledge in U-transformer for radiology report generation].", "venue": "Nan fang yi ke da xue xue bao = Journal of Southern Medical University", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.12122/j.issn.1673-4254.2025.06.24?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.12122/j.issn.1673-4254.2025.06.24, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-20", "authors": [{"authorId": "2368718511", "name": "Yedong Liang"}, {"authorId": "2285408386", "name": "Xiongfeng Zhu"}, {"authorId": "1718122", "name": "Meiyan Huang"}, {"authorId": "2312268389", "name": "Wencong Zhang"}, {"authorId": "2368935334", "name": "Hanyu Guo"}, {"authorId": "2285403219", "name": "Qianjin Feng"}], "abstract": null}
{"paperId": "5fd4dd2c9b03acce7d51795402837a8089d97f10", "url": "https://www.semanticscholar.org/paper/5fd4dd2c9b03acce7d51795402837a8089d97f10", "title": "Towards Robust Visual Question Answering via Prompt-Driven Geometric Harmonization", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i6.32610?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i6.32610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2144406642", "name": "Yishu Liu"}, {"authorId": "2292586111", "name": "Jiawei Zhu"}, {"authorId": "2329165106", "name": "Congcong Wen"}, {"authorId": "2256470712", "name": "Guangming Lu"}, {"authorId": "2350299227", "name": "Hui Lin"}, {"authorId": "152176293", "name": "Bingzhi Chen"}], "abstract": "Visual Question Answering (VQA) has garnered significant attention as a crucial link between vision and language, aimed at generating accurate responses to visual queries. However, current VQA models still struggle with the challenges of minority class collapse and spurious semantic correlations posed by language bias and imbalanced distributions. To address these challenges, this paper proposes a novel Prompt-Driven Geometric Harmonization (PDGH) paradigm, which integrates both geometric structure and information entropy principles to enhance the ability of VQA models to generalize effectively across diverse scenarios. Specifically, our PDGH approach is meticulously designed to generate image-generated prompts that are guided by specific question cues, facilitating a more accurate and context-aware understanding of the visual content. Moreover, we project the prompt-visual-question and visual-question joint representations into a unified hypersphere space, applying feature weight self-orthogonality and prompt-information entropy correction constraints to optimize the margin, further alleviating minority class collapse and correcting language bias. To maintain the geometric integrity of the representation space, we introduce multi-space geometric contrast constraints to minimize the impact of spurious priors introduced during training. Finally, a semantic matrix is constructed for the coordinated joint representation to ensure that the learned instances are semantically consistent and improve reasoning ability. Extensive experiments on various general and medical VQA datasets demonstrate the consistent superiority of our PDGH approach over existing state-of-the-art baselines."}
{"paperId": "5ff3b3ccc60c2297bddf4145fc4d4e6b47a7b463", "url": "https://www.semanticscholar.org/paper/5ff3b3ccc60c2297bddf4145fc4d4e6b47a7b463", "title": "Riemannian-Geometric Fingerprints of Generative Models", "venue": "arXiv.org", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.22802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-28", "authors": [{"authorId": "2371444464", "name": "Hae Jin Song"}, {"authorId": "2364751296", "name": "Laurent Itti"}], "abstract": "Recent breakthroughs and rapid integration of generative models (GMs) have sparked interest in the problem of model attribution and their fingerprints. For instance, service providers need reliable methods of authenticating their models to protect their IP, while users and law enforcement seek to verify the source of generated content for accountability and trust. In addition, a growing threat of model collapse is arising, as more model-generated data are being fed back into sources (e.g., YouTube) that are often harvested for training (\"regurgitative training\"), heightening the need to differentiate synthetic from human data. Yet, a gap still exists in understanding generative models'fingerprints, we believe, stemming from the lack of a formal framework that can define, represent, and analyze the fingerprints in a principled way. To address this gap, we take a geometric approach and propose a new definition of artifact and fingerprint of GMs using Riemannian geometry, which allows us to leverage the rich theory of differential geometry. Our new definition generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by learning Riemannian metrics from data and replacing the Euclidean distances and nearest-neighbor search with geodesic distances and kNN-based Riemannian center of mass. We apply our theory to a new gradient-based algorithm for computing the fingerprints in practice. Results show that it is more effective in distinguishing a large array of GMs, spanning across 4 different datasets in 2 different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2 modalities (Vision, Vision-Language). Using our proposed definition significantly improves the performance on model attribution, as well as a generalization to unseen datasets, model types, and modalities, suggesting its practical efficacy."}
{"paperId": "6066e17fbef533fb912cd0d8474c4e6352850fb2", "url": "https://www.semanticscholar.org/paper/6066e17fbef533fb912cd0d8474c4e6352850fb2", "title": "Flexible Job Scheduling With Spatial-Temporal Compatibility for In-Network Aggregation", "venue": "IEEE transactions on computers", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TC.2024.3523420?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TC.2024.3523420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2292464772", "name": "Yulong Li"}, {"authorId": "2292660514", "name": "Wenxin Li"}, {"authorId": "2316412214", "name": "Yuxuan Du"}, {"authorId": "2316092565", "name": "Yinan Yao"}, {"authorId": "2108964546", "name": "Song Zhang"}, {"authorId": "2338885268", "name": "Linxuan Zhong"}, {"authorId": "2254317763", "name": "Keqiu Li"}], "abstract": "In-Network Aggregation (INA) solutions represent the forefront in advancing All-Reduce, utilizing limited switch memory for efficient gradient aggregation. However, existing INA solutions primarily focus on enhancing aggregation efficiency, often overlooking the efficient utilization of memory. Isolation solutions typically pre-allocate resources for each job, leading to memory wastage due to the uncontrolled use of resources. In contrast, the sharing solutions encounter significant memory contention, resulting in performance degradation within a multi-tenant environment. In this paper, we propose DynaINA, a flexible job scheduler to support multi-tenant training. The core idea of DynaINA is to provide spatial and temporal compatibility between jobs. For spatial compatibility, DynaINA utilizes multiple dynamic memory pools to provide job isolation. For temporal compatibility, DynaINA employs contention-aware job scheduling to facilitate memory sharing. Furthermore, DynaINA prioritizes communication-intensive jobs, leveraging the benefits of INA to enhance overall performance in training clusters. Extensive experiments with popular vision and language models demonstrate that DynaINA reduces training time by up to 65.16% and improves switch memory utilization by up to 85.02% compared to state-of-the-art solutions in a 100Gbps network."}
{"paperId": "60ef939bc62b27968513b819aef903f2234f42b5", "url": "https://www.semanticscholar.org/paper/60ef939bc62b27968513b819aef903f2234f42b5", "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-05", "authors": [{"authorId": "2279768995", "name": "Huangyue Yu"}, {"authorId": "26663607", "name": "Baoxiong Jia"}, {"authorId": "2116664370", "name": "Yixin Chen"}, {"authorId": "2296748223", "name": "Yandan Yang"}, {"authorId": "2145015272", "name": "Puhao Li"}, {"authorId": "2338267110", "name": "Rongpeng Su"}, {"authorId": "2359394931", "name": "Jiaxin Li"}, {"authorId": "2117895397", "name": "Qing Li"}, {"authorId": "2150326083", "name": "Wei Liang"}, {"authorId": "2347682849", "name": "Song-Chun Zhu"}, {"authorId": "2110032600", "name": "Tengyu Liu"}, {"authorId": "2239060852", "name": "Siyuan Huang"}], "abstract": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demon strate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce SCAN2SIM, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScenes ’s potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research."}
{"paperId": "612a3176fb5ad740d1b253178bcbc82fba6ee24e", "url": "https://www.semanticscholar.org/paper/612a3176fb5ad740d1b253178bcbc82fba6ee24e", "title": "FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.01194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2048029359", "name": "Divyansh Jhunjhunwala"}, {"authorId": "2303364025", "name": "Arian Raje"}, {"authorId": "144487556", "name": "M. Ganesh"}, {"authorId": "29359383", "name": "Chaithanya Kumar Mummadi"}, {"authorId": "2365332625", "name": "Chaoqun Dong"}, {"authorId": "2364826218", "name": "Jiawei Zhou"}, {"authorId": "2257132255", "name": "Wan-Yi Lin"}, {"authorId": "2292169475", "name": "Gauri Joshi"}, {"authorId": "2257091754", "name": "Zhenzhen Li"}], "abstract": "LoRA has emerged as one of the most promising fine-tuning techniques, especially for federated learning (FL), since it significantly reduces communication and computation costs at resource-constrained clients. However, data heterogeneity remains a significant challenge for LoRA-based FL, and the conventional aggregation strategy based on FedAvg suffers from slow convergence and suboptimal accuracy. Motivated by recent advances in model merging, particularly Task Arithmetic, we explore the idea of aggregating client LoRA parameters using scaled averaging. We first observe that a naive application of Task Arithmetic is ineffective due to the high cosine similarity between client updates, indicating significant common knowledge in the updates across clients. To address this issue, we propose decomposing client LoRA updates via Robust Principal Component Analysis (Robust-PCA) into a common low-rank component and client-specific sparse components. Our proposed algorithm FedRPCA aggregates the low-rank components through averaging, consolidating common knowledge, and applies scaled averaging to the sparse components to amplify client-specific knowledge. We evaluate our approach across a variety of vision and language tasks and demonstrate that it achieves higher final accuracy and faster convergence compared to competing baselines."}
{"paperId": "6151559371d481cd95ea44f08ebad17399413362", "url": "https://www.semanticscholar.org/paper/6151559371d481cd95ea44f08ebad17399413362", "title": "DynamicVLN: Incorporating Dynamics into Vision-and-Language Navigation Scenarios", "venue": "Italian National Conference on Sensors", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "https://doi.org/10.3390/s25020364", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11768887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2221085603", "name": "Yanjun Sun"}, {"authorId": "2316061396", "name": "Yue Qiu"}, {"authorId": "2267612969", "name": "Yoshimitsu Aoki"}], "abstract": "Traditional Vision-and-Language Navigation (VLN) tasks require an agent to navigate static environments using natural language instructions. However, real-world road conditions such as vehicle movements, traffic signal fluctuations, pedestrian activity, and weather variations are dynamic and continually changing. These factors significantly impact an agent’s decision-making ability, underscoring the limitations of current VLN models, which do not accurately reflect the complexities of real-world navigation. To bridge this gap, we propose a novel task called Dynamic Vision-and-Language Navigation (DynamicVLN), incorporating various dynamic scenarios to enhance the agent’s decision-making abilities and adaptability. By redefining the VLN task, we emphasize that a robust and generalizable agent should not rely solely on predefined instructions but must also demonstrate reasoning skills and adaptability to unforeseen events. Specifically, we have designed ten scenarios that simulate the challenges of dynamic navigation and developed a dedicated dataset of 11,261 instances using the CARLA simulator (ver.0.9.13) and large language model to provide realistic training conditions. Additionally, we introduce a baseline model that integrates advanced perception and decision-making modules, enabling effective navigation and interpretation of the complexities of dynamic road conditions. This model showcases the ability to follow natural language instructions while dynamically adapting to environmental cues. Our approach establishes a benchmark for developing agents capable of functioning in real-world, dynamic environments and extending beyond the limitations of static VLN tasks to more practical and versatile applications."}
{"paperId": "6176f20fbb3bf50f81a1a860ebaf0fc9784c0ed0", "url": "https://www.semanticscholar.org/paper/6176f20fbb3bf50f81a1a860ebaf0fc9784c0ed0", "title": "LFSRM: Few-Shot Diagram-Sentence Matching via Local-Feedback Self-Regulating Memory.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tpami.2025.3528723?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tpami.2025.3528723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-14", "authors": [{"authorId": "2145417611", "name": "Lingling Zhang"}, {"authorId": "2267272280", "name": "Wenjun Wu"}, {"authorId": "2157175938", "name": "Jun Liu"}, {"authorId": "2237199647", "name": "Xiaojun Chang"}, {"authorId": "2110048361", "name": "Xin Hu"}, {"authorId": "2267029435", "name": "Yuhui Zheng"}, {"authorId": "2348486157", "name": "Yaqiang Wu"}, {"authorId": "2246509212", "name": "Qinghua Zheng"}], "abstract": "Image-sentence matching that aims to understand the correspondence between vision and language, has achieved significant progress with various deep methods trained under large-scale supervision. Different from natural images taken by camera, diagrams in the textbooks contain more graphic objects, drawings, and natural objects, and the diagram-sentence matching plays an important role in textbook understanding and question answering. However, existing matching models are not suitable for the challenging task between diagrams and sentences, due to the more serious few-shot content and incomplete description problems. In this paper, we propose a novel local-feedback self-regulating memory framework (LFSRM) for diagram-sentence matching. On one hand, LFSRM includes an external memory to store the useful multi-modal information, especially uncommon ones, to overcome the few-shot content problem, where the memory is updated flexibly according to the local-feedback from visual-textual alignment scores. On the other hand, LFSRM designs an attention mechanism on local-level alignment scores and a strengthening factor impacted on sentence-to-diagram matching direction for alleviating the incomplete description problem. Extensive experiments on three datasets show that LFSRM achieves satisfactory results on conventional image-sentence matching, and outperforms SOTA methods on few-shot image/diagram-sentence matching by a large margin. The dataset for diagram-sentence matching called AI2D and the LFSRM code are opened on Github https://github.com/TeamResearchWork/LFSRM."}
{"paperId": "6220a5c93c912b12b95a11bc150cbd28f6bb0ee5", "url": "https://www.semanticscholar.org/paper/6220a5c93c912b12b95a11bc150cbd28f6bb0ee5", "title": "Improving Large Vision and Language Models by Learning from a Panel of Peers", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.01610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2307403846", "name": "Jefferson Hernandez"}, {"authorId": "2379363267", "name": "Jing Shi"}, {"authorId": "2297849207", "name": "Simon Jenni"}, {"authorId": "2307005293", "name": "Vicente Ordonez"}, {"authorId": "33315685", "name": "Kushal Kafle"}], "abstract": "Traditional alignment methods for Large Vision and Language Models (LVLMs) primarily rely on human-curated preference data. Human-generated preference data is costly; machine-generated preference data is limited in quality; and self-supervised preference data often introduces hallucinations. To overcome these limitations, we propose a novel Panel-of-Peers learning framework inspired by collaborative learning among humans. This approach leverages a panel of LVLMs, each evaluating and learning from their collective outputs through an iterative self-improvement process. By simulating a peer review system, our models generate, assess, and refine outputs in response to a curated set of prompts, mimicking a classroom learning environment. We demonstrate that this methodology enhances model performance without requiring extensive human-labeled datasets. Our experiments show significant improvement across multiple benchmarks, demonstrating the potential of peer evaluations as a scalable alternative to self-supervised alignment. Notably, we show that Panel-of-Peers increases the average score on fifteen benchmarks from 48% to 57%"}
{"paperId": "6229363b28da88fd978e4ef658d77fbf01c7dc40", "url": "https://www.semanticscholar.org/paper/6229363b28da88fd978e4ef658d77fbf01c7dc40", "title": "Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey", "venue": "arXiv.org", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-13", "authors": [{"authorId": "2377738360", "name": "Shuanghao Bai"}, {"authorId": "2293142288", "name": "Wenxuan Song"}, {"authorId": "2348394168", "name": "Jiayi Chen"}, {"authorId": "2297947392", "name": "Yuheng Ji"}, {"authorId": "2349315841", "name": "Zhide Zhong"}, {"authorId": "2385504611", "name": "Jin Yang"}, {"authorId": "2266256598", "name": "Han Zhao"}, {"authorId": "2275025483", "name": "Wanqi Zhou"}, {"authorId": "2292894629", "name": "Wei Zhao"}, {"authorId": "2385507969", "name": "Zhe Li"}, {"authorId": "2275186266", "name": "Pengxiang Ding"}, {"authorId": "2384366428", "name": "Cheng Chi"}, {"authorId": "2384363611", "name": "Haoang Li"}, {"authorId": "2385774244", "name": "Chang Xu"}, {"authorId": "2348216441", "name": "Xiaolong Zheng"}, {"authorId": "2275032226", "name": "Donglin Wang"}, {"authorId": "2346116279", "name": "Shanghang Zhang"}, {"authorId": "2275030348", "name": "Badong Chen"}], "abstract": "Embodied intelligence has witnessed remarkable progress in recent years, driven by advances in computer vision, natural language processing, and the rise of large-scale multimodal models. Among its core challenges, robot manipulation stands out as a fundamental yet intricate problem, requiring the seamless integration of perception, planning, and control to enable interaction within diverse and unstructured environments. This survey presents a comprehensive overview of robotic manipulation, encompassing foundational background, task-organized benchmarks and datasets, and a unified taxonomy of existing methods. We extend the classical division between high-level planning and low-level control by broadening high-level planning to include language, code, motion, affordance, and 3D representations, while introducing a new taxonomy of low-level learning-based control grounded in training paradigms such as input modeling, latent learning, and policy learning. Furthermore, we provide the first dedicated taxonomy of key bottlenecks, focusing on data collection, utilization, and generalization, and conclude with an extensive review of real-world applications. Compared with prior surveys, our work offers both a broader scope and deeper insight, serving as an accessible roadmap for newcomers and a structured reference for experienced researchers. All related resources, including research papers, open-source datasets, and projects, are curated for the community at https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation."}
{"paperId": "6242915289b937ea87f278c682eb7d9847797345", "url": "https://www.semanticscholar.org/paper/6242915289b937ea87f278c682eb7d9847797345", "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 18, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-10", "authors": [{"authorId": "2044443762", "name": "Haiwen Diao"}, {"authorId": "2306970622", "name": "Xiaotong Li"}, {"authorId": "2263702215", "name": "Yufeng Cui"}, {"authorId": "2217456303", "name": "Yueze Wang"}, {"authorId": "2269127146", "name": "Haoge Deng"}, {"authorId": "2274107332", "name": "Ting Pan"}, {"authorId": "2273948946", "name": "Wenxuan Wang"}, {"authorId": "2307039319", "name": "Huchuan Lu"}, {"authorId": "2306805587", "name": "Xinlong Wang"}], "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE."}
{"paperId": "6295aa861f9b88e81ecb38787b54abff1e939616", "url": "https://www.semanticscholar.org/paper/6295aa861f9b88e81ecb38787b54abff1e939616", "title": "ConNEF: An Encryption Framework for Deep Neural Network IP Protection", "venue": "The Web Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3701716.3715470?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3701716.3715470, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-05-08", "authors": [{"authorId": "2238386048", "name": "Lanjun Wang"}, {"authorId": "2296381502", "name": "Mingwang Hu"}, {"authorId": "2293207882", "name": "Shixiong Xu"}, {"authorId": "2362975835", "name": "Mengbiao Zhao"}, {"authorId": "2349371331", "name": "Jianlong Chang"}], "abstract": "In the era of Machine Learning as a Service (MLaaS), service providers upload pre-trained deep neural networks (DNNs) to the website of the MLaaS platform, allowing customers to purchase model usage rights. Due to the substantial amount of training data and computational resources consumed, pre-trained DNNs have become significant assets for service providers. To protect the intellectual property (IP) of DNNs, existing active authentication methods encrypt the models to prevent unauthorized usage. However, these methods face challenges such as insufficient generalization, high overhead, and weak robustness. This paper proposes a Confusion Neuron-based Encryption Framework (ConNEF), which encrypts DNN models with a small portion of confusion neurons. Extensive experiments demonstrate that ConNEF is compatible with various networks across vision and language tasks, and outperforms baseline methods in effectiveness, efficiency, and robustness."}
{"paperId": "630dd3b225149d719f172c0842e202b715c78dac", "url": "https://www.semanticscholar.org/paper/630dd3b225149d719f172c0842e202b715c78dac", "title": "Toward Coastal Infrastructure Resiliency: An AI-Enabled Decision Support Framework for Multiscale Comprehension and Stakeholder Empowerment", "venue": "Transactions of the American Philosophical Society", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1353/tap.2025.a957549?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1353/tap.2025.a957549, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2341871230", "name": "Xinyue Ye"}, {"authorId": "2136725156", "name": "Galen Newman"}, {"authorId": "2326331977", "name": "Wei Zhai"}, {"authorId": "2311156071", "name": "David Retchless"}, {"authorId": "2356574311", "name": "Subasish Das"}, {"authorId": "2058262", "name": "Youngjib Ham"}, {"authorId": "2357724242", "name": "Lei Zou"}, {"authorId": "2356328323", "name": "Xiao Huang"}, {"authorId": "2356483201", "name": "Zhe Zhang"}], "abstract": "Abstract:While coastal flooding poses a significant global challenge, there remains a notable dearth of comprehensive research on strategies, technologies, and policies aimed at enhancing community resiliency across geographic scales. To address this gap, this study advocates the utilization of an artificial intelligence (AI)-enabled digital twin platform that capitalizes on recent advancements in computer vision, natural language processing, and network science. Such a platform facilitates a multi-scale comprehension of coastal infrastructure mechanisms. By harnessing the AI capabilities of integrating and interpreting complex datasets related to the built environment, this platform will empower stakeholders to gain invaluable insights. Specifically, it enables them to evaluate the tradeoffs associated with different planning and design efforts and comprehend the collective impacts these endeavors may exert on stakeholders' objectives. Armed with this knowledge, decision-makers might leverage the platform to embark on incremental or place-based planning, ensuring real-time prioritization, policy formulation, and informed recommendations. This decision-support framework not only acts as a catalyst for further research in data-driven decision-making but also serves as an interface connecting disparate datasets. Moreover, it presents stakeholders with training opportunities and encourages collaborative research endeavors."}
{"paperId": "6323a917e53d548acec37fe31f161224e45e317f", "url": "https://www.semanticscholar.org/paper/6323a917e53d548acec37fe31f161224e45e317f", "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-23", "authors": [{"authorId": "2363496598", "name": "Trinity Chung"}, {"authorId": "2365308211", "name": "Yuchen Shen"}, {"authorId": "2363495415", "name": "Nathan C. L. Kong"}, {"authorId": "40637388", "name": "Aran Nayebi"}], "abstract": "Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments."}
{"paperId": "63ab897d5ef19b9050816042805e2eea7c1ddf12", "url": "https://www.semanticscholar.org/paper/63ab897d5ef19b9050816042805e2eea7c1ddf12", "title": "Fair Resource Allocation for Fleet Intelligence", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.03353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-02", "authors": [{"authorId": "2283308339", "name": "Oğuzhan Başer"}, {"authorId": "2301146746", "name": "Kaan Kale"}, {"authorId": "84105530", "name": "Po-han Li"}, {"authorId": "2277751998", "name": "Sandeep P. Chinchali"}], "abstract": "Resource allocation is crucial for the performance optimization of cloud-assisted multi-agent intelligence. Traditional methods often overlook agents'diverse computational capabilities and complex operating environments, leading to inefficient and unfair resource distribution. To address this, we open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave relationship between the agents'accuracy and the system resources to ensure fair resource allocation across fleet intelligence. We extend traditional allocation approaches to encompass a multidimensional machine learning utility landscape defined by model parameters, training data volume, and task complexity. We evaluate Fair-Synergy with advanced vision and language models such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST, CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy outperforms standard benchmarks by up to 25% in multi-agent inference and 11% in multi-agent learning settings. Also, we explore how the level of fairness affects the least advantaged, most advantaged, and average agents, providing insights for equitable fleet intelligence."}
{"paperId": "6578e85cbfedb1052b453feb3acb9b4e76b2c7fe", "url": "https://www.semanticscholar.org/paper/6578e85cbfedb1052b453feb3acb9b4e76b2c7fe", "title": "Fair Domain Generalization: An Information-Theoretic View", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.05823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-08", "authors": [{"authorId": "2148650522", "name": "Tangzheng Lian"}, {"authorId": "2289593782", "name": "Guanyu Hu"}, {"authorId": "2312324639", "name": "Dimitrios Kollias"}, {"authorId": "2312345361", "name": "Xinyu Yang"}, {"authorId": "2353951736", "name": "Oya Çeliktutan"}], "abstract": "Domain generalization (DG) and algorithmic fairness are two critical challenges in machine learning. However, most DG methods focus only on minimizing expected risk in the unseen target domain without considering algorithmic fairness. Conversely, fairness methods typically do not account for domain shifts, so the fairness achieved during training may not generalize to unseen test domains. In this work, we bridge these gaps by studying the problem of Fair Domain Generalization (FairDG), which aims to minimize both expected risk and fairness violations in unseen target domains. We derive novel mutual information-based upper bounds for expected risk and fairness violations in multi-class classification tasks with multi-group sensitive attributes. These bounds provide key insights for algorithm design from an information-theoretic perspective. Guided by these insights, we introduce PAFDG (Pareto-Optimal Fairness for Domain Generalization), a practical framework that solves the FairDG problem and models the utility-fairness trade-off through Pareto optimization. Experiments on real-world vision and language datasets show that PAFDG achieves superior utility-fairness trade-offs compared to existing methods."}
{"paperId": "65b576278297ff0f530d34e002bdc36446eb4a1e", "url": "https://www.semanticscholar.org/paper/65b576278297ff0f530d34e002bdc36446eb4a1e", "title": "The role of IoT and XAI convergence in the prediction, explanation, and decision of customer perceived value (CPV) in SMEs: a theoretical framework and research proposition perspective", "venue": "Discover Internet of Things", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "https://doi.org/10.1007/s43926-025-00092-x", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s43926-025-00092-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s43926-025-00092-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-11", "authors": [{"authorId": "2209845643", "name": "Kwabena Abrokwah-Larbi"}], "abstract": "The goal of this study is to look at how the convergence of IoT and XAI (IoT-XAI) effects the explanation, prediction, and decision-making on customer perceived value (CPV) in SMEs, utilising CPV and IoT-XAI convergence theories. This study also investigates how customer-IoT interaction influences deep learning (DL) model prediction of CPV, as well as XAI explanation and decision making on CPV prediction. The literature on customer-IoT interaction, IoT physical objects, IoT data analysis, deep learning model, XAI, and CPV was reviewed to develop a theoretical framework for investigating the relationships between IoT and XAI convergence, and CPV prediction, explanation, and decision-making towards personalised marketing. The theoretical framework and research propositions are depicted in Fig. 1. Drawing on the theoretical framework used in this study, eight key research propositions were developed on the relationship between customers, IoT, DL, XAI, and CPV explanation and decision. According to the created theoretical framework and research propositions, customer-IoT interaction generates CPV data, which is then converted into structured CPV data by IoT analytics and fed into DL models for prediction. As a result, XAI models produce explanations and decisions based on DL-enabled CPV prediction, which guides personalise marketing. This paper explains how SMEs may leverage the convergence capabilities of IoT and XAI to generate CPV explanations and decisions to modify their personalize marketing methods. The results of the present investigation demonstrate that the combination of Explainable AI and the Internet of Things improves CPV prediction, interpretation, and decision-making, which results in personalised marketing optimisation. This study establishes that deep learning models like computer vision, natural language processing, and reinforcement support the Internet of Things and Explainable AI convergence assessment of CPV by taking in CPV data from the Internet of Things and analysing it to create CPV predictions. Explainable AI then interprets, makes decisions, and suggests CPV strategies based on the predictions. In terms of practical implications, Internet of Things and Explainable AI convergence can significantly boost personalisation effectiveness through CPV recommendations. In order to help business managers enhance their marketing strategy, Internet of Things and Explainable AI convergence highlights the logic behind CPV recommendations for product recommendations and targeted advertising. Business managers can more precisely tailor the personalisation process to each customer's preferences if they have a better understanding of the CPV factors (i.e., social values – acceptance, economic value – affordability and perceived cost, and functional value – performance) influencing the deep learning models' decisions. The results of the present investigation demonstrate that the combination of Explainable AI and the Internet of Things improves CPV prediction, interpretation, and decision-making, which results in personalised marketing optimisation. This study establishes that deep learning models like computer vision, natural language processing, and reinforcement support the Internet of Things and Explainable AI convergence assessment of CPV by taking in CPV data from the Internet of Things and analysing it to create CPV predictions. Explainable AI then interprets, makes decisions, and suggests CPV strategies based on the predictions. In terms of practical implications, Internet of Things and Explainable AI convergence can significantly boost personalisation effectiveness through CPV recommendations. In order to help business managers enhance their marketing strategy, Internet of Things and Explainable AI convergence highlights the logic behind CPV recommendations for product recommendations and targeted advertising. Business managers can more precisely tailor the personalisation process to each customer's preferences if they have a better understanding of the CPV factors (i.e., social values – acceptance, economic value – affordability and perceived cost, and functional value – performance) influencing the deep learning models' decisions."}
{"paperId": "65c1a6459d3eb63167885d15a3106624b44c9c4e", "url": "https://www.semanticscholar.org/paper/65c1a6459d3eb63167885d15a3106624b44c9c4e", "title": "Enhancing Vision-and-Language Navigation in Continuous Environment via Data Synthesis", "venue": "2025 5th International Conference on Neural Networks, Information and Communication Engineering (NNICE)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/NNICE64954.2025.11063767?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/NNICE64954.2025.11063767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-01-10", "authors": [{"authorId": "2282505189", "name": "Zeyu Wang"}, {"authorId": "2383532594", "name": "Xu Yang"}], "abstract": null}
{"paperId": "65f3f9603475f5d54526dd6cc691fe0dc94c7727", "url": "https://www.semanticscholar.org/paper/65f3f9603475f5d54526dd6cc691fe0dc94c7727", "title": "Parameter-Efficient Transfer Learning for Remote Sensing Image Captioning", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2025.3584887?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2025.3584887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2365340076", "name": "Xuezhi Zhao"}, {"authorId": "2164787970", "name": "Zhigang Yang"}, {"authorId": "2239813911", "name": "Qiang Li"}, {"authorId": "2237405133", "name": "Qi Wang"}], "abstract": "Remote sensing image captioning (RSIC) aims to generate accurate and concise textual descriptions for remote sensing (RS) images. It plays a significant role in the analysis of Earth observation data. The success of vision-and-language pretraining (VLP) models provides the foundation for their transfer to the RSIC task. To reduce the cost of transferring VLP models to downstream tasks, numerous parameter-efficient transfer learning (PETL) techniques have been proposed. However, most of them focus on fine-tuning general-purpose foundation models without fully considering the unique characteristics of RS data. In this article, we introduce PE-RSIC, a novel PETL framework tailored for RSIC. Specifically, the framework builds on a pretrained BLIP-2 model while further designing a lightweight cross-modal RS adapter (CRS-Adapter) and a Class Prompt. During training, all parameters of the pretrained model remain frozen, and the newly added CRS-Adapter modules are updated to efficiently transfer vision-and-language knowledge from the natural domain to the RS domain. The Class Prompt is obtained by projecting the vision-encoded [CLS] token into the decoder, guiding the model to generate more accurate captions. This approach enables the model to capture critical RS class features that might be lost during the query decoding process, with only a minimal increase in parameters. Extensive experiments show that our PE-RSIC framework outperforms full fine-tuning while using only 5% of the trainable parameters. The code will be available at https://github.com/Nick-Xzz/PE-RSIC."}
{"paperId": "667ae6f49061c8fc6878341c52e356e8a1c93cd8", "url": "https://www.semanticscholar.org/paper/667ae6f49061c8fc6878341c52e356e8a1c93cd8", "title": "CAT-TPT: Class-Agnostic Text-based Test-time Prompt Tuning for Vision-Language Models", "venue": "International Journal of Computer Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11263-025-02508-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11263-025-02508-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-05", "authors": [{"authorId": "2340917805", "name": "Youjia Zhang"}, {"authorId": "2370516405", "name": "Huiling Liu"}, {"authorId": "2349376200", "name": "Youngeun Kim"}, {"authorId": "2348986210", "name": "Sungeun Hong"}], "abstract": null}
{"paperId": "672484686296b1aaf3a56e8057117a081ec5e9c0", "url": "https://www.semanticscholar.org/paper/672484686296b1aaf3a56e8057117a081ec5e9c0", "title": "CMIRNet: Cross-Modal Interactive Reasoning Network for Referring Image Segmentation", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2024.3508752?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2024.3508752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2153557026", "name": "Mingzhu Xu"}, {"authorId": "2315181320", "name": "Tianxiang Xiao"}, {"authorId": "2315112966", "name": "Yutong Liu"}, {"authorId": "2221575636", "name": "Haoyu Tang"}, {"authorId": "2283010363", "name": "Yupeng Hu"}, {"authorId": "2237197209", "name": "Liqiang Nie"}], "abstract": "Referring Image Segmentation (RIS) aims to semantically segment the target object (referent) in alignment with the provided natural language query. Existing works still suffer from that the non-referent was segmented mistakenly, which can be attributed to the insufficient comprehension of vision and language. To tackle this problem, we propose a Cross-Modal Interactive Reasoning Network (CMIRNet) to explore semantic information that consistently existed between vision and language. Specifically, we first devise a novel Text-Guided Multi-Modality Joint Encoder (TGMM-JE), where the key expression can be extracted and the important visual features will be encoded under the continuous guidance of language expression. Then, we design a Cross-Graph Interactive Positioning (CGIP) module to locate the key pixels of the referent object in deepest layer. The multi-modality graph data is constructed between visual and linguistic features, and the important pixels can be positioned from cross-graph interaction and intra-graph reasoning. Finally, a novel Cross-Modal Attention Enhanced DEcoder (CMAE-DE) is dedicated to refine the referent object mask from coarse to fine progressively, where hybrid cross modal attentions are explored to enhance the representation of referent object. Extensive ablation studies validate the efficacy of our key modules and comprehensive experimental results show the superiority of our proposed model over 22 state-of-the-art (SOTA) models."}
{"paperId": "672851ee6178b0af05978c465a616029755585a7", "url": "https://www.semanticscholar.org/paper/672851ee6178b0af05978c465a616029755585a7", "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.06644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-08", "authors": [{"authorId": "2349269580", "name": "Xiaobei Zhao"}, {"authorId": "2375388873", "name": "Xingqi Lyu"}, {"authorId": "2349772700", "name": "Xiang Li"}], "abstract": "Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, however, still heavily rely on manual operation or fixed railways for movement. To address this limitation, the AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents to navigate to the target positions following the natural language instructions. AgriVLN effectively understands the simple instructions, but often misunderstands the complex ones. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be more refined and precise. When evaluated on the A2A benchmark, our T-araVLN effectively improves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/T-araVLN."}
{"paperId": "67582f44a70a1cf88b977ce4bfabe090038cb8eb", "url": "https://www.semanticscholar.org/paper/67582f44a70a1cf88b977ce4bfabe090038cb8eb", "title": "MSV-Mamba: A Multiscale Vision Mamba Network for Echocardiography Segmentation", "venue": "IEEE Transactions on Computational Social Systems", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.07120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-13", "authors": [{"authorId": "2294514717", "name": "Xiaoxian Yang"}, {"authorId": "2294642970", "name": "Qi Wang"}, {"authorId": "2294490368", "name": "Kaiqi Zhang"}, {"authorId": "2294378532", "name": "Ke Wei"}, {"authorId": "2296415907", "name": "Jun Lyu"}, {"authorId": "2297372102", "name": "Lingchao Chen"}], "abstract": "Ultrasound imaging frequently encounters challenges, such as those related to elevated noise levels, diminished spatiotemporal resolution, and the complexity of anatomical structures. These factors significantly hinder the model's ability to accurately capture and analyze structural relationships and dynamic patterns across various regions of the heart. Mamba, an emerging model, is one of the most cutting-edge approaches that is widely applied to diverse vision and language tasks. To this end, this paper introduces a U-shaped deep learning model incorporating a large-window Mamba scale (LMS) module and a hierarchical feature fusion approach for echocardiographic segmentation. First, a cascaded residual block serves as an encoder and is employed to incrementally extract multiscale detailed features. Second, a large-window multiscale mamba module is integrated into the decoder to capture global dependencies across regions and enhance the segmentation capability for complex anatomical structures. Furthermore, our model introduces auxiliary losses at each decoder layer and employs a dual attention mechanism to fuse multilayer features both spatially and across channels. This approach enhances segmentation performance and accuracy in delineating complex anatomical structures. Finally, the experimental results using the EchoNet-Dynamic and CAMUS datasets demonstrate that the model outperforms other methods in terms of both accuracy and robustness. For the segmentation of the left ventricular endocardium (${LV}_{endo}$), the model achieved optimal values of 95.01 and 93.36, respectively, while for the left ventricular epicardium (${LV}_{epi}$), values of 87.35 and 87.80, respectively, were achieved. This represents an improvement ranging between 0.54 and 1.11 compared with the best-performing model."}
{"paperId": "67967d95ad790941a477291cc07d94ca15ddf5ac", "url": "https://www.semanticscholar.org/paper/67967d95ad790941a477291cc07d94ca15ddf5ac", "title": "Prompt-Based Invertible Mapping Alignment for Unsupervised Domain Adaptation", "venue": "ACM Trans. Multim. Comput. Commun. Appl.", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3725735?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3725735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-21", "authors": [{"authorId": "2350336323", "name": "Chao Wen"}, {"authorId": "2351821726", "name": "Chen Wei"}, {"authorId": "2187326606", "name": "Yuhua Qian"}, {"authorId": "2304749284", "name": "Xiaodan Song"}, {"authorId": "2298342498", "name": "Xuemei Xie"}], "abstract": "Large pre-trained vision-language models (VLMs) like CLIP have shown great potential for solving the unsupervised domain adaptation (UDA) problem. Existing prompt learning for UDA based on the unsupervised-trained VLMs requires distribution alignment between source and target domains in the common space for both vision and language branches. However, it is difficult for rough cross-domain alignment to maintain the discriminative semantic structure of both domains. Besides, the coarse features with non-informative noises due to ignoring the pseudo-label noises may cause failures to concentrate on precise semantics alignment. In this work, we propose a Prompt-Based Invertible Mapping Alignment (PIMA) method to incorporate discriminative domain knowledge into prompt learning, which is featured with refined cross-domain alignment in two separate space with a well-kept structure. Specifically, we design an invertible neural network-based homeomorphism mapping, and then achieve distribution alignment through such invertible mapping for connecting source and target visual feature space, which can preserve the data semantic structure. For better semantic alignment in vision-language space, we develop cross-modal implicit contrastive learning module to regularize non-informative features, which aims to find the low-rankness of implicit representation space. We conducted extensive experiments on three benchmark datasets to prove the advantages of our proposed PIMA over state-of-the-art methods."}
{"paperId": "688e349a81010822df92762b5b1c0310fcba1604", "url": "https://www.semanticscholar.org/paper/688e349a81010822df92762b5b1c0310fcba1604", "title": "See, Localize and Verify: A GRPO-Powered Framework for Enhancing Factual Accuracy in Multimodal Models", "venue": "Proceedings of the 1st International Workshop on MLLM for Unified Comprehension and Generation", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746276.3760467?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746276.3760467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book"], "publicationDate": "2025-10-26", "authors": [{"authorId": "2388115759", "name": "Xuan Li"}, {"authorId": "2307404565", "name": "Fengzhao Sun"}, {"authorId": "2258009330", "name": "Jun Yu"}, {"authorId": "2305608536", "name": "Yunxiang Zhang"}], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in integrating vision and language. However, their widespread practical deployment is severely hindered by the phenomenon of hallucination, that the model's generated textual output is inconsistent with the provided visual content or lacks factual grounding. Existing research indicates that the hallucination and fact-checking problem in MLLMs is primarily caused by the misalignment between visual and text modalities and the neglect of visual tokens in deep attention layers. Consequently, guiding the model to enhance its attention to visual signals within the deep network, balance its focus across both modalities, and achieve faithful cross-modal reasoning has become a critical pathway to addressing this issue. This paper explores the use of reinforcement learning to guide the model to increase its focus on the visual modality, thereby enabling faithful cross-modal reasoning and mitigating the hallucination phenomenon in MLLMs. Specifically, we design a reward function for cross-modal reasoning, tailored for the GRPO reinforcement learning method. This function guides the model to first locate the coordinates of objects in the image that are associated with the descriptive text, and subsequently detect hallucinations in the text by referencing grounded visual evidence. Through end-to-end reinforcement learning with this method, our model, with only 7B parameters, surpassed its 32B counterpart on hallucination detection and fact-checking tasks, and significantly outperformed models trained with supervised fine-tuning method. Our code will be released on https://github.com/TioeAre/SLAV."}
{"paperId": "68bae5b680bcfae9d4a5d9b2f37f030cb6a8f08f", "url": "https://www.semanticscholar.org/paper/68bae5b680bcfae9d4a5d9b2f37f030cb6a8f08f", "title": "A Review on Sign Language to Text and Speech Conversion", "venue": "International Journal for Research in Applied Science and Engineering Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22214/ijraset.2025.67869?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22214/ijraset.2025.67869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2352725802", "name": "Prof. Sweta Wankhade"}], "abstract": "Sign language functions as an important type of communication for people with hearing and speech impairments. .\nHowever, the broad lack of knowledge in sign language in the general population indicates an important communication\nbarrier. This review paper examines the progress of the sign language recognition system, converting gestures into text and\nlanguage, facilitating actual communication. Although traditional methods are based on sensor-based or computer vision\ntechniques, the latest developments in deep learning, particularly in frameworks such as Neuron Networks (CNNS) and\nMediaPipe have significantly improved identification accuracy. This article examines various approaches and highlights their\nadvantages, limitations, and their practical applicability. Furthermore, you can use issues such as data. We will explain the\nflashiness of the model and the actual time processing limits. By reviewing existing research and technological advances, this\nstudy aims to provide insight into the optimization of speech translation systems for broader accessibility and practical delivery.\nVision, Signal Language Translation, Text-to-Speech (TTS) Synthesis, Google Text-to-Speech (GTTS), Natural Language\nProcessing (NLP)."}
{"paperId": "68d9def90b4c9f928778406726bd6f5a9fbe9770", "url": "https://www.semanticscholar.org/paper/68d9def90b4c9f928778406726bd6f5a9fbe9770", "title": "FLASH: Deadline-Aware Flexible LLC Arbitration and Scheduling for Hardware Accelerators", "venue": "ACM Transactions on Embedded Computing Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3757742?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3757742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-30", "authors": [{"authorId": "145999187", "name": "Ayushi Agarwal"}, {"authorId": "2374168263", "name": "Pulkit Goel"}, {"authorId": "2333646421", "name": "P.J. Joseph"}, {"authorId": "2333630756", "name": "Prokash Ghosh"}, {"authorId": "2333858219", "name": "Sourav Roy"}, {"authorId": "1747329", "name": "P. Panda"}], "abstract": "Integrating domain-specific hardware accelerators on modern systems on chips (SoCs) has enabled complex applications, such as vision, natural language processing, autonomous driving, and augmented reality, on small form factors. This leads to challenges in the integration of accelerators, with high memory bandwidth requirements and strict deadlines, on the system’s memory hierarchy. The system-level shared cache, or last-level cache (LLC), is a critical resource shared by multi-core processors, GPUs, and hardware accelerators in modern heterogeneous SoCs. It significantly reduces the bottleneck at the off-chip memory and delivers high performance. With the integration of accelerators on the LLC gaining momentum, the on-chip shared cache management becomes vital. If not managed intelligently, the interference between cache requests from the cores and the accelerators can significantly deteriorate their performance. Given the architectural differences between DRAM and cache systems, the off-chip memory management strategies explored by previous works cannot be extended to the LLC. We propose a deadline-aware flexible LLC arbitration and scheduling framework, FLASH, to dynamically partition the LLC bandwidth between the accelerators and multi-core processors to meet the deadline given for the accelerator while minimizing the impact on the performance of the cores. FLASH arbitrates between the requests from the cores and the accelerators and schedules the requests depending on the accelerator’s progress and its chances of meeting the deadline. We evaluate FLASH across different workloads and hardware accelerator configurations to show that it not only achieves significantly better performance for the cores than other static scheduling policies but also significantly reduces the deadline miss rates of the accelerator."}
{"paperId": "68eef1bab3937e28e5ae0ab9352b7d1db6d27888", "url": "https://www.semanticscholar.org/paper/68eef1bab3937e28e5ae0ab9352b7d1db6d27888", "title": "One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01468, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-02", "authors": [{"authorId": "2224847051", "name": "Filippo Betello"}, {"authorId": "2214617052", "name": "Antonio Purificato"}, {"authorId": "2345028849", "name": "Vittoria Vineis"}, {"authorId": "2651748", "name": "Gabriele Tolomei"}, {"authorId": "2325900532", "name": "Fabrizio Silvestri"}], "abstract": "The environmental impact of Artificial Intelligence (AI) is emerging as a significant global concern, particularly regarding model training. In this paper, we introduce GREEN (Guided Recommendations of Energy-Efficient Networks), a novel, inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across diverse AI domains and tasks. Our approach directly addresses the limitations of current eco-efficient neural architecture search methods, which are often restricted to specific architectures or tasks. Central to this work is EcoTaskSet, a dataset comprising training dynamics from over 1767 experiments across computer vision, natural language processing, and recommendation systems using both widely used and cutting-edge architectures. Leveraging this dataset and a prediction model, our approach demonstrates effectiveness in selecting the best model configuration based on user preferences. Experimental results show that our method successfully identifies energy-efficient configurations while ensuring competitive performance."}
{"paperId": "6954e983c36c6b7c943611115f84657ce40fdf46", "url": "https://www.semanticscholar.org/paper/6954e983c36c6b7c943611115f84657ce40fdf46", "title": "Fine-Tuning with Uncertainty-Aware Priors Makes Vision and Language Foundation Models More Reliable", "venue": "International Conference on Artificial Intelligence and Statistics", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2107677984", "name": "Tim G. J. Rudner"}, {"authorId": "2375970803", "name": "Xiang Pan"}, {"authorId": "2269777246", "name": "Yucen Lily Li"}, {"authorId": "1411405900", "name": "Ravid Shwartz-Ziv"}, {"authorId": "2306762930", "name": "Andrew Gordon Wilson"}], "abstract": null}
{"paperId": "696244b797927f8aa62ae9be116d29c58a4cebbf", "url": "https://www.semanticscholar.org/paper/696244b797927f8aa62ae9be116d29c58a4cebbf", "title": "Enhancing Intermodal Interaction for Unified Vision-Language Understanding and Generation", "venue": "Data Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3724/2096-7004.di.2025.0034?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3724/2096-7004.di.2025.0034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2276185186", "name": "Yang Qin"}, {"authorId": "2181815708", "name": "Huiming Xie"}, {"authorId": "2110550004", "name": "Yujie Li"}, {"authorId": "34578251", "name": "Benying Tan"}, {"authorId": "2238627809", "name": "Shuxue Ding"}], "abstract": "The majority of vision-language pre-training (VLP) models rely on pre-trained object detectors, which incur high costs and restrict the recognition of object classes. Additionally, their encoder-based structures hinder their ability to perform text generation tasks effectively. To mitigate these challenges, we propose a Detector-free Vision-and-Language Pre-training (D-VLP) model designed to bolster intermodal interaction for unified understanding and generation tasks. Our D-VLP model employs a co-modality decoder equipped with a fused multi-attention self-attention module, enhancing feature fusion and information alignment between images and text. It is pre-trained using a novel Prefix Masked Language Modeling (prefixMLM) approach, leveraging the strengths of masked language modeling and unidirectional language modeling, which enables bidirectional processing and autoregressive token generation. Extensive experiments demonstrate that D-VLP surpasses state-of-the-art models in vision-language tasks, highlighting its superior performance and adaptability across various image-text tasks with minimal adjustments."}
{"paperId": "69af1f62cbadc710e123b7c887f68622951a4ed7", "url": "https://www.semanticscholar.org/paper/69af1f62cbadc710e123b7c887f68622951a4ed7", "title": "Scientific practices for understanding, applying and creating with artificial intelligence in K‐12 education: A scoping review", "venue": "Revista de educación", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/rev3.70098?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/rev3.70098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-01", "authors": [{"authorId": "2303488546", "name": "Jhon Alé"}, {"authorId": "2334643730", "name": "Beatrice Ávalos"}, {"authorId": "2362178275", "name": "Roberto Araya"}], "abstract": "This scoping review examines the integration of artificial intelligence (AI) tools into scientific education practices in school settings. Following the PRISMA statement guidelines, a literature search was conducted in the Web of Science and Scopus databases, identifying 2892 articles published between 2020 and 2024. After applying the eligibility criteria, 75 primary studies with empirical data demonstrating the outcomes of school‐based scientific practices using AI tools were selected. The studies were coded based on the type of AI, the type of scientific practice, AI competencies, and other contextual and pedagogical aspects. The results indicate that the most frequently used tools by students are computer vision, natural language processing and data mining. Most studies focus on secondary education levels, combining strategies such as project‐based learning, scientific inquiry and the STEM approach. Although the evidence suggests that these tools enhance scientific skills like data interpretation and computational thinking, limitations were identified, including the use of low‐reliability or paid software and a lack of representation of ethical competencies. Finally, the review highlights the need to strengthen interdisciplinary collaboration and the design of curricular programmes that integrate AI through an ethical framework. The findings are discussed to provide guidance for pedagogy, public policies and future research.\nRationale for this study: This study was conducted to examine current evidence on how AI tools are being integrated into K‐12 scientific practices.Why the new findings matter: The findings presented are relevant for analysing current trends, identifying research gaps and exploring opportunities to update interdisciplinary pedagogical frameworks and digital literacy curriculum policies.Implications for educational practitioners, policy makers and researchers: For education practitioners, this study provides a planning model with practical examples for integrating AI into K‐12 scientific practices. For policy makers, it highlights the urgency of updating curricular frameworks and public education policies to ensure equitable access to trustworthy technologies, as well as initial and ongoing teacher training in AI, while fostering interdisciplinary collaboration, particularly between science and mathematics. For researchers, there is a clear need to expand empirical studies that examine scientific practices in virtual and hybrid modalities, in underexplored levels such as elementary education, and that take into account ethical competencies in the use of AI.\n"}
{"paperId": "69f60ccf84e712c7e7c88239a5e649aa91f9e28b", "url": "https://www.semanticscholar.org/paper/69f60ccf84e712c7e7c88239a5e649aa91f9e28b", "title": "Learning Representations Through Contrastive Neural Model Checking", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.01853, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2383306528", "name": "Vladimir Krsmanovic"}, {"authorId": "2210267524", "name": "Matthias Cosler"}, {"authorId": "2283933985", "name": "Mohamed Ghanem"}, {"authorId": "2383306812", "name": "Bernd Finkbeiner"}], "abstract": "Model checking is a key technique for verifying safety-critical systems against formal specifications, where recent applications of deep learning have shown promise. However, while ubiquitous for vision and language domains, representation learning remains underexplored in formal verification. We introduce Contrastive Neural Model Checking (CNML), a novel method that leverages the model checking task as a guiding signal for learning aligned representations. CNML jointly embeds logical specifications and systems into a shared latent space through a self-supervised contrastive objective. On industry-inspired retrieval tasks, CNML considerably outperforms both algorithmic and neural baselines in cross-modal and intra-modal settings. We further show that the learned representations effectively transfer to downstream tasks and generalize to more complex formulas. These findings demonstrate that model checking can serve as an objective for learning representations for formal languages."}
{"paperId": "6a1ed7fe1eac5973695313c484a0d2ac769c6023", "url": "https://www.semanticscholar.org/paper/6a1ed7fe1eac5973695313c484a0d2ac769c6023", "title": "Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-08-29", "authors": [{"authorId": "2290025090", "name": "Muhammad Ali"}, {"authorId": "2210828030", "name": "Salman H. Khan"}], "abstract": "Recent advancements in Large Language Models (LLMs) have paved the way for Vision Large Language Models (VLLMs) capable of performing a wide range of visual understanding tasks. While LLMs have demonstrated impressive performance on standard natural images, their capabilities have not been thoroughly explored in cluttered datasets where there is complex environment having deformed shaped objects. In this work, we introduce a novel dataset specifically designed for waste classification in real-world scenarios, characterized by complex environments and deformed shaped objects. Along with this dataset, we present an in-depth evaluation approach to rigorously assess the robustness and accuracy of VLLMs. The introduced dataset and comprehensive analysis provide valuable insights into the performance of VLLMs under challenging conditions. Our findings highlight the critical need for further advancements in VLLM's robustness to perform better in complex environments. The dataset and code for our experiments will be made publicly available."}
{"paperId": "6a761935275e727c19f5c26775f822db87843e4b", "url": "https://www.semanticscholar.org/paper/6a761935275e727c19f5c26775f822db87843e4b", "title": "Semantic-guided Representation Learning for Multi-Label Recognition", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-04", "authors": [{"authorId": "2288244082", "name": "Ruhui Zhang"}, {"authorId": "2065400224", "name": "H. Qiao"}, {"authorId": "2288221710", "name": "Pengcheng Xu"}, {"authorId": "2309004403", "name": "Mingsheng Shang"}, {"authorId": "2339640445", "name": "Lin Chen"}], "abstract": "Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at https://github.com/MVL-Lab/SigRL."}
{"paperId": "6ae4b522df43e9a34f19dff1aa0569b036b3b578", "url": "https://www.semanticscholar.org/paper/6ae4b522df43e9a34f19dff1aa0569b036b3b578", "title": "Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-30", "authors": [{"authorId": "102434825", "name": "F. Bazikar"}, {"authorId": "2373490775", "name": "Hossein Moosaei"}, {"authorId": "2389400119", "name": "Atefeh Hemmati"}, {"authorId": "1759608", "name": "P. Pardalos"}], "abstract": "Multi-task learning (MTL) enables simultaneous training across related tasks, leveraging shared information to improve generalization, efficiency, and robustness, especially in data-scarce or high-dimensional scenarios. While deep learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor, and effectiveness with small datasets. This chapter surveys MTL approaches based on SVM and TWSVM, highlighting shared representations, task regularization, and structural coupling strategies. Special attention is given to emerging TWSVM extensions for multi-task settings, which show promise but remain underexplored. We compare these models in terms of theoretical properties, optimization strategies, and empirical performance, and discuss applications in fields such as computer vision, natural language processing, and bioinformatics. Finally, we identify research gaps and outline future directions for building scalable, interpretable, and reliable margin-based MTL frameworks. This work provides a comprehensive resource for researchers and practitioners interested in SVM- and TWSVM-based multi-task learning."}
{"paperId": "6b1dc6878ddc0ea04fb61ab52e019c1c604fbbff", "url": "https://www.semanticscholar.org/paper/6b1dc6878ddc0ea04fb61ab52e019c1c604fbbff", "title": "Map-SemNav: Advancing Zero-Shot Continuous Vision-and-Language Navigation Through Visual Semantics and Map Integration", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA55743.2025.11127282?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA55743.2025.11127282, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2326643957", "name": "Shuai Wu"}, {"authorId": "2304360884", "name": "Ruonan Liu"}, {"authorId": "2387131311", "name": "Zongxia Xie"}, {"authorId": "2361747189", "name": "Zhibo Pang"}], "abstract": "This paper explores zero-shot Vision-and-Language Navigation (VLN), enabling agents to generalize navigation to unseen data classes. Most current approaches rely on large models, but these are not specifically tailored for VLN, lacking direct learning from navigation environments and slowing down agents due to their overwhelming size. To tackle this, we propose Map-Semantic Zero-shot Navigation (Map-SemNav), which does not rely on large models for navigation planning. Map-SemNav utilizes three key cues: direction, object, and scene, to acquire relational knowledge instead of memorizing specific classes, which enables generalization to unseen data. Direction is guided by a top-down semantic map, while object and scene information is decoupled from environment knowledge. Extensive experiments demonstrate that Map-SemNav outperforms state-of-the-art large model-based methods in zero-shot VLN tasks within continuous environments, while also offering higher efficiency due to its simplified architecture."}
{"paperId": "6b22a89d2fc2f9257210d6fb400dea046073f14b", "url": "https://www.semanticscholar.org/paper/6b22a89d2fc2f9257210d6fb400dea046073f14b", "title": "Capturing Semantic Flow of ML-based Systems", "venue": "SIGSOFT FSE Companion", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10310, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2025-03-13", "authors": [{"authorId": "2343827994", "name": "Shin Yoo"}, {"authorId": "2288142837", "name": "Robert Feldt"}, {"authorId": "2344028677", "name": "Somin Kim"}, {"authorId": "2335423291", "name": "Naryeong Kim"}], "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems."}
{"paperId": "6c56821553e2966bd7baab62f4b69189f45e09a8", "url": "https://www.semanticscholar.org/paper/6c56821553e2966bd7baab62f4b69189f45e09a8", "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-26", "authors": [{"authorId": "2188811859", "name": "Roie Kazoom"}, {"authorId": "2382925176", "name": "Yuval Ratzabi"}, {"authorId": "2382925888", "name": "Etamar Rothstein"}, {"authorId": "2803728", "name": "O. Hadar"}], "abstract": "Adversarial robustness in structured data remains an underexplored frontier compared to vision and language domains. In this work, we introduce a novel black-box, decision-based adversarial attack tailored for tabular data. Our approach combines gradient-free direction estimation with an iterative boundary search, enabling efficient navigation of discrete and continuous feature spaces under minimal oracle access. Extensive experiments demonstrate that our method successfully compromises nearly the entire test set across diverse models, ranging from classical machine learning classifiers to large language model (LLM)-based pipelines. Remarkably, the attack achieves success rates consistently above 90%, while requiring only a small number of queries per instance. These results highlight the critical vulnerability of tabular models to adversarial perturbations, underscoring the urgent need for stronger defenses in real-world decision-making systems."}
{"paperId": "6c59e77510d434623c21df3a1807f2b643bb2f94", "url": "https://www.semanticscholar.org/paper/6c59e77510d434623c21df3a1807f2b643bb2f94", "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 24, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-10", "authors": [{"authorId": "2188084738", "name": "Damiano Marsili"}, {"authorId": "2344755157", "name": "Rohun Agrawal"}, {"authorId": "2238214339", "name": "Yisong Yue"}, {"authorId": "2397488699", "name": "Georgia Gkioxari"}], "abstract": "Visual reasoning – the ability to interpret the visual world–is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/"}
{"paperId": "6c5cdcffa00ef8cd99e1bc86a78a558ea205e702", "url": "https://www.semanticscholar.org/paper/6c5cdcffa00ef8cd99e1bc86a78a558ea205e702", "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.06006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-06", "authors": [{"authorId": "2159539050", "name": "Yifu Qiu"}, {"authorId": "7264689", "name": "Yftah Ziser"}, {"authorId": "2266464518", "name": "Anna Korhonen"}, {"authorId": "2277600097", "name": "Shay B. Cohen"}, {"authorId": "3381663", "name": "E. Ponti"}], "abstract": "To what extent do vision-and-language foundation models possess a realistic world model (observation $\\times$ action $\\rightarrow$ observation) and a dynamics model (observation $\\times$ observation $\\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench."}
{"paperId": "6c7851b5043aa40bc6cfbf3bed7054633bfbfe6e", "url": "https://www.semanticscholar.org/paper/6c7851b5043aa40bc6cfbf3bed7054633bfbfe6e", "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-15", "authors": [{"authorId": "2393004107", "name": "Shuochen Chang"}, {"authorId": "2392901560", "name": "Xiaofeng Zhang"}, {"authorId": "2144832213", "name": "Qingyang Liu"}, {"authorId": "2371139459", "name": "Li Niu"}], "abstract": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM."}
{"paperId": "6ca331518ee673c53e942171fc6ac43fb3112701", "url": "https://www.semanticscholar.org/paper/6ca331518ee673c53e942171fc6ac43fb3112701", "title": "Granularity-Aware Hyperbolic Representation for Text-Based Person Search", "venue": "IEEE Transactions on Information Forensics and Security", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIFS.2025.3574970?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIFS.2025.3574970, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2364245248", "name": "Chenghuan Qi"}, {"authorId": "1477973008", "name": "Xi Yang"}, {"authorId": "2284094963", "name": "Nan Wang"}, {"authorId": "2239964615", "name": "Xinbo Gao"}], "abstract": "Text-based person search aims to identify specific target person from the database according to the given text description. Early work adopted separately pretrained encoders to extract visual and textual features, but benefit from the bloom of visual language pre-training, recent work uses unified pretrained visual language models such as CLIP as backbone. However, visual language models are generally pretrained from coarse-grained image-text pairs, while image-text pairs in text-based person search are more fine-grained to distinguish different persons. In addition, visual and linguistic concepts naturally organize themselves in a hierarchy, which is not explicitly captured by current large-scale vision and language models such as CLIP. To bridge this gap, we propose a novel Granularity-Aware Hyperbolic Representation learning method for mining granularity and capturing semantic hierarchy. Notably, we consider both token-level and instance-level granularity. For token-granularity alignment, we present a Bidirectional Attention Interaction module to explicitly learn the matching between fine-grained visual tokens and text tokens. For instance-granularity alignment, we equip the contrastive learning loss with Semantic Margin Softmax so that image-text pairs can perceive the similarity granularity of different samples during training. Besides, the global features of images and texts are mapped into hyperbolic space through Hyperbolic Representation Learning to embed tree-like data to capture semantic hierarchy. Extensive experiments verify the effectiveness of our proposed modules and show that our method achieves state-of-the-art results on the three widely acknowledged benchmarks, namely CUHK-PEDES, ICFG-PEDES, and RSTPReID. Our code is available at https://github.com/7chQ/GAHR"}
{"paperId": "6cbdf7cef9e8a3897469967c69a2864837f1cbb3", "url": "https://www.semanticscholar.org/paper/6cbdf7cef9e8a3897469967c69a2864837f1cbb3", "title": "SPARTA: An Optimization Framework for Differentially Private Sparse Fine-Tuning", "venue": "Knowledge Discovery and Data Mining", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2025-03-17", "authors": [{"authorId": "2343633632", "name": "Mehdi Makni"}, {"authorId": "10706882", "name": "Kayhan Behdin"}, {"authorId": "2288280045", "name": "Gabriel Afriat"}, {"authorId": "2343776896", "name": "Zheng Xu"}, {"authorId": "2282962033", "name": "Sergei Vassilvitskii"}, {"authorId": "2282969107", "name": "Natalia Ponomareva"}, {"authorId": "2858060", "name": "Hussein Hazimeh"}, {"authorId": "2285731917", "name": "Rahul Mazumder"}], "abstract": "Differentially private stochastic gradient descent (DP-SGD) is broadly considered to be the gold standard for training and fine-tuning neural networks under differential privacy (DP). With the increasing availability of high-quality pre-trained model checkpoints (e.g., vision and language models), fine-tuning has become a popular strategy. However, despite recent progress in understanding and applying DP-SGD for private transfer learning tasks, significant challenges remain - most notably, the performance gap between models fine-tuned with DP-SGD and their non-private counterparts. Sparse fine-tuning on private data has emerged as an alternative to full-model fine-tuning -- recent work has shown that privately fine-tuning only a small subset of model weights and keeping the rest of the weights fixed can lead to better performance. In this work, we propose a new approach for sparse fine-tuning of neural networks under DP. Existing work on private sparse finetuning often used fixed choice of trainable weights (e.g., updating only the last layer), or relied on public model's weights to choose the subset of weights to modify. Such choice of weights remains suboptimal. In contrast, we explore an optimization-based approach, where our selection method makes use of the private gradient information, while using off the shelf privacy accounting techniques. Our numerical experiments on several computer vision models and datasets show that our parameter selection method leads to better prediction accuracy, compared to full-model private fine-tuning or existing private sparse fine-tuning approaches. Our code is available here: https://github.com/mazumder-lab/SPARTA/tree/main"}
{"paperId": "6cceee54319b67f612611c66d48a1904ed7bd21e", "url": "https://www.semanticscholar.org/paper/6cceee54319b67f612611c66d48a1904ed7bd21e", "title": "Mitigating scale imbalance and conflicting gradients in deep multi-task learning", "venue": "Frontiers of Computer Science", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11704-024-40632-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11704-024-40632-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-16", "authors": [{"authorId": "2028908279", "name": "Yuepeng Jiang"}, {"authorId": "2132286026", "name": "Yunhao Gou"}, {"authorId": "2386209042", "name": "Wenbo Zhang"}, {"authorId": "2292027871", "name": "Xuehao Wang"}, {"authorId": "2386391899", "name": "Yu Zhang"}, {"authorId": "2273022993", "name": "Qiang Yang"}], "abstract": null}
{"paperId": "6ce35333f711e117078c330bbcd83eef8fe75501", "url": "https://www.semanticscholar.org/paper/6ce35333f711e117078c330bbcd83eef8fe75501", "title": "Demystifying Deep Learning and Neural Networks: A Technical Overview", "venue": "European journal of computer science and information technology", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.37745/ejcsit.2013/vol13n8123?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.37745/ejcsit.2013/vol13n8123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-03-15", "authors": [{"authorId": "2357927319", "name": "Uthra Sridhar"}], "abstract": "Deep learning has revolutionized artificial intelligence by enabling machines to learn hierarchically from data with minimal human intervention. Neural networks, inspired by the human brain's structure, form the foundation of this paradigm shift, processing information through interconnected layers of artificial neurons to extract complex patterns from data. These architectures have transformed numerous domains including computer vision, natural language processing, and specialized applications such as autonomous vehicles and drug discovery. Despite remarkable achievements, significant challenges persist in interpretability, computational requirements, and data dependencies. Solutions including interpretable AI techniques, model compression, and transfer learning are actively addressing these limitations. The evolution of neural network designs, training methodologies, and optimization approaches continues to expand the capabilities and applications of deep learning while raising important considerations about ethics, sustainability, and accessibility."}
{"paperId": "6ce4c59948d4ffcfd0773f2ab7742959f892ded0", "url": "https://www.semanticscholar.org/paper/6ce4c59948d4ffcfd0773f2ab7742959f892ded0", "title": "Evaluating the Trade-offs Between Machine Learning and Deep Learning: A Multi-Dimensional Analysis", "venue": "Journal of Computer, Software, and Program", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.69739/jcsp.v2i1.254?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.69739/jcsp.v2i1.254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-06", "authors": [{"authorId": "2257056014", "name": "Nagwa Elmobark"}], "abstract": "The proliferation of artificial intelligence applications necessitates a clear understanding of the fundamental distinctions between Machine Learning (ML) and Deep Learning (DL) approaches. This study presents a systematic comparative analysis through a multi-dimensional evaluation framework. We analyzed 150 implementations across three domains (computer vision, natural language processing, and structured data analysis), evaluating performance metrics, resource utilization, and architectural complexities. Our findings reveal that while DL architectures achieve superior accuracy in complex pattern recognition tasks (mean improvement: 27.3%, p < 0.001), they require substantially higher computational resources (GPU utilization: 89.2% vs. 23.7% for ML). Traditional ML demonstrates notable advantages in scenarios with limited datasets (<10,000 samples), exhibiting 3.8x faster training times and a 72% lower memory footprint. To guide implementation decisions, we developed a quantitative decision matrix based on five critical parameters: data volume, computational constraints, problem complexity, interpretability requirements, and time sensitivity. The matrix achieved 91.4% accuracy in predicting the optimal approach across 50 independent test cases. This research provides empirical evidence for the trade-offs between ML and DL, offering practitioners a structured framework for algorithm selection while considering resource constraints and performance requirements."}
{"paperId": "6ce6b000c515445eb2a99b2ce55707de77223a5d", "url": "https://www.semanticscholar.org/paper/6ce6b000c515445eb2a99b2ce55707de77223a5d", "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-05", "authors": [{"authorId": "2187159401", "name": "N. Diep"}, {"authorId": "2205949274", "name": "Hien Dang"}, {"authorId": "2384126016", "name": "Tuan Truong"}, {"authorId": "2384125826", "name": "Tan Dinh"}, {"authorId": "144810262", "name": "Huy Nguyen"}, {"authorId": "2261283719", "name": "Nhat Ho"}], "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models."}
{"paperId": "6e1bb30f14433ec4835cfbd87e3d4bf3502d1212", "url": "https://www.semanticscholar.org/paper/6e1bb30f14433ec4835cfbd87e3d4bf3502d1212", "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-18", "authors": [{"authorId": "2240803189", "name": "Peiran Xu"}, {"authorId": "2387658589", "name": "Xicheng Gong"}, {"authorId": "2240531027", "name": "Yadong Mu"}], "abstract": "In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method."}
{"paperId": "6ecbf8a5e595bf0704a21b7b3b51626f72dae9cc", "url": "https://www.semanticscholar.org/paper/6ecbf8a5e595bf0704a21b7b3b51626f72dae9cc", "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 30, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-18", "authors": [{"authorId": "2327199957", "name": "Lingfeng Zhang"}, {"authorId": "2301468688", "name": "Xiaoshuai Hao"}, {"authorId": "2295955471", "name": "Qinwen Xu"}, {"authorId": "2230253877", "name": "Qiang Zhang"}, {"authorId": "2227589472", "name": "Xinyao Zhang"}, {"authorId": "2338357829", "name": "Pengwei Wang"}, {"authorId": "2301324703", "name": "Jing Zhang"}, {"authorId": "2338315894", "name": "Zhongyuan Wang"}, {"authorId": "2346116279", "name": "Shanghang Zhang"}, {"authorId": "2243405867", "name": "Renjing Xu"}], "abstract": "Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field."}
{"paperId": "6eceae9368fcb542a09ece223c400f81f9e6f6d2", "url": "https://www.semanticscholar.org/paper/6eceae9368fcb542a09ece223c400f81f9e6f6d2", "title": "PhishingHook: Catching Phishing Ethereum Smart Contracts leveraging EVM Opcodes", "venue": "2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-S)", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DSN-S65789.2025.00075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DSN-S65789.2025.00075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-23", "authors": [{"authorId": "2302424585", "name": "Pasquale De Rosa"}, {"authorId": "2193297716", "name": "Simon Queyrut"}, {"authorId": "49030221", "name": "Yérom-David Bromberg"}, {"authorId": "2240547002", "name": "Pascal Felber"}, {"authorId": "2106027", "name": "V. Schiavoni"}], "abstract": "Phishing attacks are a prevalent threat to users and investors on the Ethereum blockchain. While modern detection systems can leverage live transaction analysis, these may raise privacy concerns and entail high overhead. In this work, we propose PhishingHook, a framework that detects phishing activities directly from the on-chain bytecode of Ethereum smart contracts. By disassembling contracts’ bytecode and transforming it into suitable representations, PhishingHook leverages multiple machine learning models. We compare 16 models from four categories (histogram similarity classifiers, vision models, language models, and vulnerability detection models) on 7,000 real-world Ethereum contracts. Our results demonstrate the efficiency of PhishingHook in performing phishing classification systems, with about 90% average accuracy among all the models."}
{"paperId": "6f13adfe5a304cc7688311507f2ecf07a1236f2e", "url": "https://www.semanticscholar.org/paper/6f13adfe5a304cc7688311507f2ecf07a1236f2e", "title": "A Comprehensive Survey on Subspace Clustering: Methods and Applications", "venue": "Artificial Intelligence Review", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10462-025-11349-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10462-025-11349-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-21", "authors": [{"authorId": "2262404547", "name": "Jianyu Miao"}, {"authorId": "2376460549", "name": "Xiaochan Zhang"}, {"authorId": "2319388528", "name": "Tiejun Yang"}, {"authorId": "2153501012", "name": "Chao Fan"}, {"authorId": "2306874163", "name": "Yingjie Tian"}, {"authorId": "2285590274", "name": "Yong Shi"}, {"authorId": "2308899279", "name": "Mingliang Xu"}], "abstract": null}
{"paperId": "6f24a81d7ff87640055a36ad4a6ada811c0118bf", "url": "https://www.semanticscholar.org/paper/6f24a81d7ff87640055a36ad4a6ada811c0118bf", "title": "Backdoor Attack and Defense on Deep Learning: A Survey", "venue": "IEEE Transactions on Computational Social Systems", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSS.2024.3482723?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSS.2024.3482723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-01", "authors": [{"authorId": "2313977748", "name": "Yang Bai"}, {"authorId": "2312945850", "name": "Gaojie Xing"}, {"authorId": "2312920417", "name": "Hongyan Wu"}, {"authorId": "2312965170", "name": "Zhihong Rao"}, {"authorId": "2313047384", "name": "Chuan Ma"}, {"authorId": "2329527697", "name": "Shiping Wang"}, {"authorId": "2329710719", "name": "Xiaolei Liu"}, {"authorId": "2314077380", "name": "Yimin Zhou"}, {"authorId": "2329731015", "name": "Jiajia Tang"}, {"authorId": "2331375662", "name": "Kaijun Huang"}, {"authorId": "2329641992", "name": "Jiale Kang"}], "abstract": "Deep learning, as an important branch of machine learning, has been widely applied in computer vision, natural language processing, speech recognition, and more. However, recent studies have revealed that deep learning systems are vulnerable to backdoor attacks. Backdoor attackers inject a hidden backdoor into the deep learning model, such that the predictions of the infected model will be maliciously changed if the hidden backdoor is activated by input with a backdoor trigger while behaving normally on any benign sample. This kind of attack can potentially result in severe consequences in the real world. Therefore, research on defending against backdoor attacks has emerged rapidly. In this article, we have provided a comprehensive survey of backdoor attacks, detections, and defenses previously demonstrated on deep learning. We have investigated widely used model architectures, benchmark datasets, and metrics in backdoor research and have classified attacks, detections and defenses based on different criteria. Furthermore, we have analyzed some limitations in existing methods and, based on this, pointed out several promising future research directions. Through this survey, beginners can gain a preliminary understanding of backdoor attacks and defenses. Furthermore, we anticipate that this work will provide new perspectives and inspire extra research into the backdoor attack and defense methods in deep learning."}
{"paperId": "6f9a152b3a3dfc79651494d243dc3a5c44655964", "url": "https://www.semanticscholar.org/paper/6f9a152b3a3dfc79651494d243dc3a5c44655964", "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.05693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-05", "authors": [{"authorId": null, "name": "Zhiying Du"}, {"authorId": "2397280982", "name": "Bei Liu"}, {"authorId": "2333468608", "name": "Yaobo Liang"}, {"authorId": "2397336969", "name": "Yichao Shen"}, {"authorId": "2347189486", "name": "Haidong Cao"}, {"authorId": "2397579147", "name": "Xiangyu Zheng"}, {"authorId": "2365226924", "name": "Zhiyuan Feng"}, {"authorId": "3099139", "name": "Zuxuan Wu"}, {"authorId": "2237946707", "name": "Jiaolong Yang"}, {"authorId": "2116115246", "name": "Yu-Gang Jiang"}], "abstract": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA."}
{"paperId": "6fe825793e6e533b2a568d05da54182cab6ece49", "url": "https://www.semanticscholar.org/paper/6fe825793e6e533b2a568d05da54182cab6ece49", "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model", "venue": "arXiv.org", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.10416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-14", "authors": [{"authorId": "2375948273", "name": "Zhuoyuan Yu"}, {"authorId": "2242982685", "name": "Yuxing Long"}, {"authorId": "2376121157", "name": "Zihan Yang"}, {"authorId": "2375980992", "name": "Chengyan Zeng"}, {"authorId": "1405871317", "name": "Hongwei Fan"}, {"authorId": "2239160954", "name": "Jiyao Zhang"}, {"authorId": "2293669442", "name": "Hao Dong"}], "abstract": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following."}
{"paperId": "6ff2a279e93fc74e7975a100c6a5dd0a466ab695", "url": "https://www.semanticscholar.org/paper/6ff2a279e93fc74e7975a100c6a5dd0a466ab695", "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.07984, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-10", "authors": [{"authorId": "2306129092", "name": "Jingli Lin"}, {"authorId": "2243446820", "name": "Chenming Zhu"}, {"authorId": "2087110249", "name": "Runsen Xu"}, {"authorId": "2307735489", "name": "Xiaohan Mao"}, {"authorId": "2253851118", "name": "Xihui Liu"}, {"authorId": "2257008641", "name": "Tai Wang"}, {"authorId": "2277447920", "name": "Jiangmiao Pang"}], "abstract": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/"}
{"paperId": "706a8dd31ecd7679379ab30382af7d9e823b5f9f", "url": "https://www.semanticscholar.org/paper/706a8dd31ecd7679379ab30382af7d9e823b5f9f", "title": "Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.09597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-11", "authors": [{"authorId": "2051653538", "name": "M. Behmanesh"}, {"authorId": "2371073035", "name": "Erkan Turan"}, {"authorId": "1690177", "name": "M. Ovsjanikov"}], "abstract": "Graph alignment, the problem of identifying corresponding nodes across multiple graphs, is fundamental to numerous applications. Most existing unsupervised methods embed node features into latent representations to enable cross-graph comparison without ground-truth correspondences. However, these methods suffer from two critical limitations: the degradation of node distinctiveness due to oversmoothing in GNN-based embeddings, and the misalignment of latent spaces across graphs caused by structural noise, feature heterogeneity, and training instability, ultimately leading to unreliable node correspondences. We propose a novel graph alignment framework that simultaneously enhances node distinctiveness and enforces geometric consistency across latent spaces. Our approach introduces a dual-pass encoder that combines low-pass and high-pass spectral filters to generate embeddings that are both structure-aware and highly discriminative. To address latent space misalignment, we incorporate a geometry-aware functional map module that learns bijective and isometric transformations between graph embeddings, ensuring consistent geometric relationships across different representations. Extensive experiments on graph benchmarks demonstrate that our method consistently outperforms existing unsupervised alignment baselines, exhibiting superior robustness to structural inconsistencies and challenging alignment scenarios. Additionally, comprehensive evaluation on vision-language benchmarks using diverse pretrained models shows that our framework effectively generalizes beyond graph domains, enabling unsupervised alignment of vision and language representations."}
{"paperId": "70bdd43eb4af0c459c2651ec48698fb9cfb040e2", "url": "https://www.semanticscholar.org/paper/70bdd43eb4af0c459c2651ec48698fb9cfb040e2", "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-29", "authors": [{"authorId": "2339983959", "name": "Jiahong Chen"}, {"authorId": "2359005494", "name": "Jing Wang"}, {"authorId": "2359957373", "name": "Long Chen"}, {"authorId": "2389499468", "name": "Chuwei Cai"}, {"authorId": "2390142796", "name": "Jinghui Lu"}], "abstract": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware."}
{"paperId": "719a399abfa8ea41da404921dd165b8d179943bf", "url": "https://www.semanticscholar.org/paper/719a399abfa8ea41da404921dd165b8d179943bf", "title": "From History to Goal: Enhanced Vision-and-Language Navigation with Historical Traceability", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICME59968.2025.11209526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICME59968.2025.11209526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2391040673", "name": "Xinguang Zhu"}, {"authorId": "2145298424", "name": "Min Wang"}, {"authorId": "2156060279", "name": "Li Li"}, {"authorId": "2153938030", "name": "Weng Zhou"}, {"authorId": "2251738329", "name": "Houqiang Li"}], "abstract": "In Vision-and-Language Navigation (VLN), most methods ignore the navigation history after each episode, which is unrealistic for navigation in a persistent environment. Recent methods concatenate historical trajectories with the current one for history awareness, but also introduce redundant observations that offer little information gain and even harmful noise. To this end, we propose a history-traceable framework for VLN named TraceNav, which selects relevant historical information during persistent navigation. Specifically, it employs a multi-granularity matching strategy that consists of image-text matching and instruction-trajectory matching. In image-text matching, we accurately identify target candidates from historical observations using vision-language models and object detection models. Instruction-trajectory matching employs a cross-modal transformer to infer the degree of match between candidates’ trajectories and text instructions. We validate our method in similar pre-exploration and iterative setups, achieving performance that exceeds existing methods. We also validate our method on multiple datasets and achieve significant performance improvements over several state-of-the-art VLN methods. Our code is released on https://github.com/zhuxinguang33/TraceNav."}
{"paperId": "72ab70692fdb7efa239f8b36cc41d72dc9e05dc7", "url": "https://www.semanticscholar.org/paper/72ab70692fdb7efa239f8b36cc41d72dc9e05dc7", "title": "PERFORMANCE AND CLINICAL APPLICABILITY OF LARGE LANGUAGE MODELS IN OPHTHALMOLOGY", "venue": "PARIPEX-INDIAN JOURNAL OF RESEARCH", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36106/paripex/2806152?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36106/paripex/2806152, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-06-15", "authors": [{"authorId": "2370413962", "name": "Yugindu Raizada"}, {"authorId": "2370413127", "name": "Reem Javeed"}, {"authorId": "2318155992", "name": "Divyanshu Gupta"}, {"authorId": "2330092386", "name": "Renu Hashia Dhar"}], "abstract": "This review synthesises current evidence on Large Language Models (LLMs) and Vision Large Language Models\n(VLLMs) in ophthalmology. LLMs show high accuracy in text-based tasks, with some models exceeding human\nperformance in areas like glaucoma diagnosis.However,VLLMs exhibit a significant performance drop in image-based\ntasks without textual descriptions, and while abnormality detection is high, diagnostic description quality is often\nsuboptimal. Common errors include misinterpreting data and overlooking clinical signs. Despite their potential, LLMs\nand VLLMs are not yet suitable for autonomous clinical decision-making, emphasising the ongoing need for human\noversight due to issues like \"hallucinations\" and data limitations."}
{"paperId": "72bcb1e716bbd9da632c99578eb394c864f7676c", "url": "https://www.semanticscholar.org/paper/72bcb1e716bbd9da632c99578eb394c864f7676c", "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12466, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-17", "authors": [{"authorId": "2374473181", "name": "Xuhui Zhan"}, {"authorId": "2376193213", "name": "Tyler Derr"}], "abstract": "Traditional multimodal learning approaches require expensive alignment pre-training to bridge vision and language modalities, typically projecting visual features into discrete text token spaces. We challenge both fundamental assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel approach that eliminates alignment pre-training entirely while inverting the conventional mapping direction. Rather than projecting visual features to text space, our method maps text embeddings into continuous visual representation space and performs fusion within transformer intermediate layers. Through selective additive components in attention mechanisms, we enable dynamic integration of visual and textual representations without requiring massive image-text alignment datasets. Comprehensive experiments across nine multimodal benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing expected decreases in perception tasks requiring memorized visual-text associations (celebrity recognition: -49.5%, OCR: -21.3%). These results provide the first empirical evidence that alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning tasks. Our work establishes the feasibility of a new paradigm that reduces computational requirements by 45%, challenges conventional wisdom about modality fusion, and opens new research directions for efficient multimodal architectures that preserve modality-specific characteristics. Our project website with code and additional resources is available at https://inverse-llava.github.io."}
{"paperId": "731c50b0d6af4c1cb8d95f506541681ea487973b", "url": "https://www.semanticscholar.org/paper/731c50b0d6af4c1cb8d95f506541681ea487973b", "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots", "venue": "arXiv.org", "year": 2025, "citationCount": 344, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14734, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-18", "authors": [{"authorId": "2334017135", "name": "Nvidia"}, {"authorId": "46278353", "name": "Johan Bjorck"}, {"authorId": "2350859158", "name": "Fernando Castaneda"}, {"authorId": "2350861671", "name": "Nikita Cherniadev"}, {"authorId": "2350863929", "name": "Xingye Da"}, {"authorId": "2350936924", "name": "Runyu Ding"}, {"authorId": "2350861618", "name": "LinxiJimFan"}, {"authorId": "2351241177", "name": "Yu Fang"}, {"authorId": "2258436157", "name": "Dieter Fox"}, {"authorId": "2352992614", "name": "Fengyuan Hu"}, {"authorId": "2350990085", "name": "Spencer Huang"}, {"authorId": "2333419032", "name": "Joel Jang"}, {"authorId": "2319430243", "name": "Zhenyuan Jiang"}, {"authorId": "2364684748", "name": "Jan Kautz"}, {"authorId": "1471804481", "name": "Kaushil Kundalia"}, {"authorId": "2350862410", "name": "Lawrence Lao"}, {"authorId": "2348789944", "name": "Zhiqi Li"}, {"authorId": "2350991916", "name": "Zongyu Lin"}, {"authorId": "2315897190", "name": "Kevin Lin"}, {"authorId": "2269510103", "name": "Guilin Liu"}, {"authorId": "2192605890", "name": "Edith Llontop"}, {"authorId": "2350862986", "name": "Loic Magne"}, {"authorId": "49686756", "name": "Ajay Mandlekar"}, {"authorId": "2014184266", "name": "Avnish Narayan"}, {"authorId": "3457048", "name": "Soroush Nasiriany"}, {"authorId": "2344615904", "name": "Scott Reed"}, {"authorId": "2350869215", "name": "Y. Tan"}, {"authorId": "96374437", "name": "Guanzhi Wang"}, {"authorId": "2350859563", "name": "Zu Wang"}, {"authorId": "2350827994", "name": "Jing Wang"}, {"authorId": "2326830174", "name": "Qi Wang"}, {"authorId": "2057477005", "name": "Jiannan Xiang"}, {"authorId": "2218866691", "name": "Yuqi Xie"}, {"authorId": "2351665438", "name": "Yinzhen Xu"}, {"authorId": "2318097278", "name": "Zhen-Teng Xu"}, {"authorId": "2152111477", "name": "Seonghyeon Ye"}, {"authorId": "2269841405", "name": "Zhiding Yu"}, {"authorId": "2350752882", "name": "Ao Zhang"}, {"authorId": "2316357156", "name": "Hao Zhang"}, {"authorId": "2350998025", "name": "Yizhou Zhao"}, {"authorId": "2345931905", "name": "Ruijie Zheng"}, {"authorId": "2253507326", "name": "Yuke Zhu"}], "abstract": "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency."}
{"paperId": "7343a5fd9e28b0974d8bad61b5c2719d8040e864", "url": "https://www.semanticscholar.org/paper/7343a5fd9e28b0974d8bad61b5c2719d8040e864", "title": "A vision and language hierarchical alignment for multimodal aspect-based sentiment analysis", "venue": "Pattern Recognition", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2025.111369?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2025.111369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2216906840", "name": "Wang Zou"}, {"authorId": "2334255449", "name": "Xia Sun"}, {"authorId": "2262196954", "name": "Qiang Lu"}, {"authorId": "2327288094", "name": "Xuxin Wang"}, {"authorId": "6972447", "name": "Jun Feng"}], "abstract": null}
{"paperId": "7346b3bd448be0fea6d2e49b95aa666f7418d368", "url": "https://www.semanticscholar.org/paper/7346b3bd448be0fea6d2e49b95aa666f7418d368", "title": "HAVEN: From Human Guidance to Assistant by Evolution Network in Vision-and-Language Navigation", "venue": "IEEE International Joint Conference on Neural Network", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN64981.2025.11227307?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN64981.2025.11227307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2295941820", "name": "Pingrui Lai"}, {"authorId": "2392708152", "name": "Zihao Xie"}, {"authorId": "2392652203", "name": "Hua Yang"}], "abstract": "Vision-and-Language Navigation (VLN) is a critical task that enables robots to comprehend human instructions. The premise of current VLN tasks is built on the human’s familiarity with the environment structure, guiding the agent to complete the navigation task with explicit instructions, referred to as Human Guidance VLN (HG-VLN). However, real-world scenarios often involve humans unfamiliar with new environments, relying on agents to assist with navigation. In such tasks, humans can only provide destination-related information, requiring the agent to perform the path planning. We term this scenario Human Assistant VLN (HA-VLN). HA-VLN poses greater demands on the agent, therefore, we have restructured the classic Room to Room (R2R) dataset to introduce the Room to Room Assistant (R2RA) dataset, tailored for HA-VLN tasks. To address the challenges existing methods face when processing HA-VLN task instructions, we propose HAVEN: Human Assistant Vision-and- Language Navigation Evolution Network. This network integrates a Large Language Model (LLM) with an embedded memory system, achieving the paradigm shift from mainstream VLN methods to the HA-VLN task without requiring additional information. Our experiments demonstrate that algorithms incorporating HAVEN can achieve higher success rates in reaching destinations, shorter path selection, and lower navigation error rates in HA-VLN tasks. HAVEN can be integrated as an end-to-end module into any VLN method. The code and dataset is available at: https://github.com/longziyu/R2RA-Dataset"}
{"paperId": "73dd4c6a593f5efe7ae67cdf1bdd491295e37906", "url": "https://www.semanticscholar.org/paper/73dd4c6a593f5efe7ae67cdf1bdd491295e37906", "title": "TopoNets: High Performing Vision and Language Models with Brain-Like Topography", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.16396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-27", "authors": [{"authorId": "2382925421", "name": "Mayukh Deb"}, {"authorId": "2342506394", "name": "Mainak Deb"}, {"authorId": "2342506323", "name": "N. A. R. Murty"}], "abstract": "Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present TopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively TopoNets. TopoNets are the highest-performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain's visual and language cortices. Together, this work establishes a robust and generalizable framework for integrating topography into leading model architectures, advancing the development of high-performing models that more closely emulate the computational strategies of the human brain."}
{"paperId": "743473c549534d70e9d30332fede05fdceae6f78", "url": "https://www.semanticscholar.org/paper/743473c549534d70e9d30332fede05fdceae6f78", "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 12, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-26", "authors": [{"authorId": "2320314271", "name": "Shuang Zeng"}, {"authorId": "2382761853", "name": "Dekang Qi"}, {"authorId": "2320818428", "name": "Xinyuan Chang"}, {"authorId": "2382761717", "name": "Feng Xiong"}, {"authorId": "2383113599", "name": "Shichao Xie"}, {"authorId": "2382838519", "name": "Xiaolong Wu"}, {"authorId": "2371201656", "name": "Shiyi Liang"}, {"authorId": "2363407946", "name": "Mu Xu"}, {"authorId": "2320296208", "name": "Xing Wei"}], "abstract": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/."}
{"paperId": "748b4295703d92027c23bb00d49c0cb738eba489", "url": "https://www.semanticscholar.org/paper/748b4295703d92027c23bb00d49c0cb738eba489", "title": "MoE-MSC: Mixture of Experts with Multi-Stream Connector for Modality-Aware Medical Image Captioning", "venue": "Proceedings of the 16th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3765612.3767248?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3765612.3767248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-12", "authors": [{"authorId": "1384281486", "name": "Al Shahriar Rubel"}, {"authorId": "2281688528", "name": "Frank Y. Shih"}, {"authorId": "2355207872", "name": "Fadi P Deek"}], "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising capabilities in medical image captioning task. Their architecture usually integrates a vision encoder, a Large Language Model (LLM), and a connector specifically designed to bridge the modality gap between vision and language. However, a light-weight connector struggles to filter appropriate visual features for an LLM to generate structured captions for medical images across various modalities. This paper proposes MoE-MSC: Mixture of Experts (MoE) with Multi-Stream Connector (MSC) for modality-aware structured medical image captioning. The captions are structured with identified modality, anatomical structures, Region-of-Interest (RoI) analysis, lesion findings, and local-global relationships indicating potential impact of lesion findings in RoI to other regions. The MSC with multiple Cross Attentions projects visual features from a vision encoder to a representation that enables an LLM to generate captions focusing on different aspects in a caption related to modality and anatomical structures, RoI, and local-global relationships. Furthermore, the MoE enables modality-aware captioning, in which modality-specific visual features from a vision encoder are routed to specialized experts, offering scalability, enhanced interpretability, and reduced computational cost during inference. Our extensive experiments demonstrate the capabilities of our model with superior quantitative and qualitative results, compared to relevant methods. The source code is available at https://github.com/alshahriarrubel/MoE-MSC."}
{"paperId": "74b6d37afec01bcf9201f977d6f7e69d92f0ae3c", "url": "https://www.semanticscholar.org/paper/74b6d37afec01bcf9201f977d6f7e69d92f0ae3c", "title": "Self-Adaptive Vision-Language Tracking With Context Prompting", "venue": "IEEE Transactions on Image Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2025.3635016?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2025.3635016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-08", "authors": [{"authorId": "2143817885", "name": "Jie Zhao"}, {"authorId": null, "name": "Xin Chen"}, {"authorId": "2244143306", "name": "Shengming Li"}, {"authorId": "1918691", "name": "Chunjuan Bo"}, {"authorId": "2335662741", "name": "Dong Wang"}, {"authorId": "2241418668", "name": "Huchuan Lu"}], "abstract": "Due to the substantial gap between vision and language modalities, along with the mismatch problem between fixed language descriptions and dynamic visual information, existing vision-language tracking methods exhibit performance on par with or slightly worse than vision-only tracking. Effectively exploiting the rich semantics of language to enhance tracking robustness remains an open challenge. To address these issues, we propose a self-adaptive vision-language tracking framework that leverages the pre-trained multi-modal CLIP model to obtain well-aligned visual-language representations. A novel context-aware prompting mechanism is introduced to dynamically adapt linguistic cues based on the evolving visual context during tracking. Specifically, our context prompter extracts dynamic visual features from the current search image and integrates them into the text encoding process, enabling self-updating language embeddings. Furthermore, our framework employs a unified one-stream Transformer architecture, supporting joint training for both vision-only and vision-language tracking scenarios. Our method not only bridges the modality gap but also enhances robustness by allowing language features to evolve with visual context. Extensive experiments on four vision-language tracking benchmarks demonstrate that our method effectively leverages the advantages of language to enhance visual tracking. Our large model can obtain 55.0% AUC on $\\text {LaSOT}_{\\text {EXT}}$ and 69.0% AUC on TNL2K. Additionally, our language-only tracking model achieves performance comparable to that of state-of-the-art vision-only tracking methods on TNL2K. Code is available at https://github.com/zj5559/SAVLT"}
{"paperId": "7528c6726491498184776470f3550bab19e766fe", "url": "https://www.semanticscholar.org/paper/7528c6726491498184776470f3550bab19e766fe", "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18698, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-24", "authors": [{"authorId": "2185422948", "name": "Can Yaras"}, {"authorId": "2338831486", "name": "Alec S. Xu"}, {"authorId": "1484065124", "name": "Pierre Abillama"}, {"authorId": "2365037315", "name": "Changwoo Lee"}, {"authorId": "2247365147", "name": "Laura Balzano"}], "abstract": "Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the Transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences $(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$ for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts. Our code is available at https://github.com/cjyaras/monarch-attention."}
{"paperId": "7556dbba734e21d2c61a2a13cc06656bfedf074a", "url": "https://www.semanticscholar.org/paper/7556dbba734e21d2c61a2a13cc06656bfedf074a", "title": "Generative and Discriminative Models in Multimodal AI: An Analysis of Vision-Language Tasks", "venue": "Transactions on Computer Science and Intelligent Systems Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.62051/hdjsgp39?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.62051/hdjsgp39, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-19", "authors": [{"authorId": "2378096248", "name": "Fengjiang He"}], "abstract": "The transformer architecture has triggered groundbreaking works in multimodal vision and language (V+L). This article offers brief look into the two main modeling paradigms—generative and discriminative—from their roots in natural language processing (NLP) specifically generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), respectively. The core ideas of these two paradigms are then examined to show how they have been modified to handle V+L tasks, resulting in different architectural paths and pre-training methods. The paradigms are also surveyed by core dimensions, analyzing the challenges along the path from distributed paradigms to unified models (e.g., model hallucination, limited evaluation capability and scalability). This work aims to provide a well-organized and clear view on how V+L modeling has evolved and possibly evolved into for researchers as well as practitioners."}
{"paperId": "755dc2b9d6a93d5330e1fd6c629736e76da62d69", "url": "https://www.semanticscholar.org/paper/755dc2b9d6a93d5330e1fd6c629736e76da62d69", "title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding", "venue": "International Journal of Computer Vision", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.09906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-14", "authors": [{"authorId": "35659935", "name": "Thanh-Dat Truong"}, {"authorId": "2269131655", "name": "Hoang-Quan Nguyen"}, {"authorId": "1959025244", "name": "Xuan-Bac Nguyen"}, {"authorId": "2166046829", "name": "Ashley Dowling"}, {"authorId": "2268627059", "name": "Xin Li"}, {"authorId": "2243185830", "name": "Khoa Luu"}], "abstract": "Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks Project Page: https://uarkcviu.github.io/projects/insectfoundation."}
{"paperId": "75a984a09b03b40790f4e3d5cdf32c0d7fd6a16a", "url": "https://www.semanticscholar.org/paper/75a984a09b03b40790f4e3d5cdf32c0d7fd6a16a", "title": "Deep Probabilistic Binary Embedding via Learning Reliable Uncertainty for Cross-Modal Retrieval", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754811?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2390312750", "name": "Kun Cheng"}, {"authorId": "2066431003", "name": "Qibing Qin"}, {"authorId": "50549775", "name": "Wenfeng Zhang"}, {"authorId": "2257931780", "name": "Lei Huang"}, {"authorId": "2228976538", "name": "Jie Nie"}], "abstract": "The field of cross-modal retrieval aims to construct a shared representation space for samples from multiple modalities, typically within the vision and language domains. Deep hashing, with its high computational efficiency and low storage costs, has emerged as a central focus in this field and has garnered significant attention in recent research. However, current hash retrieval, concentrating on deterministic methods, struggles to effectively capture semantically ambiguous correspondences between cross-modal samples, where heterogeneous data have complex-semantic many-to-many relationships in the latent space. To address this limitation, we propose a novel Deep Probabilistic Binary Embedding (DPBE) framework, designed to generate discriminative, modality-invariant hash codes that facilitate accurate and reliable cross-modal retrieval. In contrast to contemporary probabilistic methods, we focus on optimizing hash networks to learn more accurate binary embeddings by using the learning mode of probabilistic embeddings. We introduce the first Bayesian encoder for hash learning, which employs Laplace Approximation to model a distribution over network weights. Extensive experimental results demonstrate that our approach not only outperforms deterministic methods in retrieval performance but also provides uncertainty estimates, enhancing the interpretability of the embeddings. The corresponding code is available at https://github.com/QinLab-WFU/DPBE."}
{"paperId": "7646ffe0469e929b1133ca51e9c73317f073ea9c", "url": "https://www.semanticscholar.org/paper/7646ffe0469e929b1133ca51e9c73317f073ea9c", "title": "IMU: Influence-guided Machine Unlearning", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.01620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-03", "authors": [{"authorId": "2375035097", "name": "Xindi Fan"}, {"authorId": "2149264949", "name": "Jing Wu"}, {"authorId": "2315086314", "name": "Mingyi Zhou"}, {"authorId": "2374486176", "name": "Pengwei Liang"}, {"authorId": "2142223393", "name": "D. Phung"}], "abstract": "Recent studies have shown that deep learning models are vulnerable to attacks and tend to memorize training data points, raising significant concerns about privacy leakage. This motivates the development of machine unlearning (MU), i.e., a paradigm that enables models to selectively forget specific data points upon request. However, most existing MU algorithms require partial or full fine-tuning on the retain set. This necessitates continued access to the original training data, which is often impractical due to privacy concerns and storage constraints. A few retain-data-free MU methods have been proposed, but some rely on access to auxiliary data and precomputed statistics of the retain set, while others scale poorly when forgetting larger portions of data. In this paper, we propose Influence-guided Machine Unlearning (IMU), a simple yet effective method that conducts MU using only the forget set. Specifically, IMU employs gradient ascent and innovatively introduces dynamic allocation of unlearning intensities across different data points based on their influences. This adaptive strategy significantly enhances unlearning effectiveness while maintaining model utility. Results across vision and language tasks demonstrate that IMU consistently outperforms existing retain-data-free MU methods."}
{"paperId": "779a6b6b6f45f8584c89776fc1be4d12eb2b87d0", "url": "https://www.semanticscholar.org/paper/779a6b6b6f45f8584c89776fc1be4d12eb2b87d0", "title": "ViLAaD: Enhancing \"Attracting and Dispersing\" Source-Free Domain Adaptation with Vision-and-Language Model", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-30", "authors": [{"authorId": "2912439", "name": "Shuhei Tarashima"}, {"authorId": "2352968755", "name": "Xinqi Shu"}, {"authorId": "2188095966", "name": "Norio Tagawa"}], "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to a target dataset from a different domain without access to the source data. Conventional SFDA methods are limited by the information encoded in the pre-trained source model and the unlabeled target data. Recently, approaches leveraging auxiliary resources have emerged, yet remain in their early stages, offering ample opportunities for research. In this work, we propose a novel method that incorporates auxiliary information by extending an existing SFDA framework using Vision-and-Language (ViL) models. Specifically, we build upon Attracting and Dispersing (AaD), a widely adopted SFDA technique, and generalize its core principle to naturally integrate ViL models as a powerful initialization for target adaptation. Our approach, called ViL-enhanced AaD (ViLAaD), preserves the simplicity and flexibility of the AaD framework, while leveraging ViL models to significantly boost adaptation performance. We validate our method through experiments using various ViL models, demonstrating that ViLAaD consistently outperforms both AaD and zero-shot classification by ViL models, especially when both the source model and ViL model provide strong initializations. Moreover, the flexibility of ViLAaD allows it to be seamlessly incorporated into an alternating optimization framework with ViL prompt tuning and extended with additional objectives for target model adaptation. Extensive experiments on four SFDA benchmarks show that this enhanced version, ViLAaD++, achieves state-of-the-art performance across multiple SFDA scenarios, including Closed-set SFDA, Partial-set SFDA, and Open-set SFDA."}
{"paperId": "77d9b0712c79a5f334b383590e6f02c8fe6833b2", "url": "https://www.semanticscholar.org/paper/77d9b0712c79a5f334b383590e6f02c8fe6833b2", "title": "Cognitive Foundation Agents for Generalizable Vision-and-Language Navigation", "venue": "Proceedings of the 33rd ACM International Conference on Advances in Geographic Information Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3748636.3760462?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3748636.3760462, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2397877916", "name": "Sherry Chalotra"}, {"authorId": "2398043357", "name": "Zhiqiang Jiang"}, {"authorId": "2284521464", "name": "Xin Wang"}], "abstract": "Vision-and-Language Navigation (VLN) is a key task in embodied AI, yet most agents remain reactive, task specific, and cognitively limited. As these systems extend to real-world areas like assistive guidance, disaster response, and multi agent teaming, the lack of ability to reason, reflect, and adapt presents critical flaws. This paper introduces Cognitive Foundation Agent (CFA), a conceptual model that reconceives VLN as a problem of spatial cognition and collaborative intelligence. CFA integrates perception, language, memory, and planning in a cognitive process that supports real time adaptation in complex environments. The model comprises five asynchronous modules: multimodal perception, meta-cognition, self-evolving world model, spatiotemporal planning, and multi agent collaboration, linked by a real-time Cognitive Feedback Loop (CFL) that enables agents to perceive, coordinate, reason, and adapt across tasks and environments. To drive progress in this space, this paper outlines the need for CFA-Bench, a dedicated evaluation suite for cognitively grounded navigation. CFA represents a shift toward embodied agents that move, reason, and collaborate with human-aligned spatial intelligence."}
{"paperId": "77db39bb22c3831ac2a243456afe8568279be610", "url": "https://www.semanticscholar.org/paper/77db39bb22c3831ac2a243456afe8568279be610", "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-05", "authors": [{"authorId": "2303364025", "name": "Arian Raje"}, {"authorId": "2137332270", "name": "Baris Askin"}, {"authorId": "2048029359", "name": "Divyansh Jhunjhunwala"}, {"authorId": "2292169475", "name": "Gauri Joshi"}], "abstract": "Large language models (LLMs) have not yet effectively leveraged the vast amounts of edge-device data, and federated learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computation and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose \\textsc{Ravan}, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads $s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$ in which only the core matrices $\\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that \\textsc{Ravan} improves test accuracy by 2-8\\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs."}
{"paperId": "783b96a70306f0c3982a8e8b31c17e215eb31048", "url": "https://www.semanticscholar.org/paper/783b96a70306f0c3982a8e8b31c17e215eb31048", "title": "Vision and Language Synergy for Rehearsal Free Continual Learning", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "1410084757", "name": "M. A. Ma'sum"}, {"authorId": "2281032883", "name": "Mahardhika Pratama"}, {"authorId": "2273559077", "name": "Savitha Ramasamy"}, {"authorId": "2295728056", "name": "Lin Liu"}, {"authorId": "2220546957", "name": "Habibullah"}, {"authorId": "2220546517", "name": "Ryszard Kowalczyk"}], "abstract": null}
{"paperId": "78417ae18091c9b311ed4980bdcc83a3b2957766", "url": "https://www.semanticscholar.org/paper/78417ae18091c9b311ed4980bdcc83a3b2957766", "title": "Energy-Efficient Parametric Activation for Deep Hardware Learning Applications", "venue": "2025 IEEE 20th Conference on Industrial Electronics and Applications (ICIEA)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIEA65512.2025.11149197?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIEA65512.2025.11149197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-03", "authors": [{"authorId": "2379951530", "name": "Kaijun Cao"}, {"authorId": "2379971575", "name": "Charles Z. Liu"}], "abstract": "The rapid advancement of deep learning applications in computer vision, natural language processing, and autonomous systems has intensified the demand for energy-efficient neural network hardware. In response to the limitations of conventional activation function implementations—such as high power consumption, complex analog-digital hybrid circuits, or large lookup tables—we propose a novel circuit design based on parallel three-bit comparators combined with a parameterized 16-bit floating-point multiplier. Unlike traditional designs, our approach enables flexible realization of both linear and non-linear activation functions while significantly reducing power and propagation delay. Through hierarchical comparator batching and critical path optimization techniques, the circuit achieves over 40% energy savings compared to baseline implementations without compromising functional accuracy. Additionally, our floating-point multiplier architecture enhances support for sub-threshold and negative value modeling, offering greater robustness and adaptability. This work contributes a scalable, low-power hardware solution suitable for deployment in energy-constrained edge AI systems, and establishes a foundation for future research in efficient neural network activation."}
{"paperId": "78734845227f459b122d05faa0a6ccf92b1f5509", "url": "https://www.semanticscholar.org/paper/78734845227f459b122d05faa0a6ccf92b1f5509", "title": "A review of the emotion recognition model of robots", "venue": "Applied intelligence (Boston)", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-025-06245-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-025-06245-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-22", "authors": [{"authorId": "2341758081", "name": "Mingyi Zhao"}, {"authorId": "2342575424", "name": "Linrui Gong"}, {"authorId": "2295735251", "name": "Abdul Sattar Din"}], "abstract": null}
{"paperId": "79598177e582c4c4ba86448d7e32253db9aec0d4", "url": "https://www.semanticscholar.org/paper/79598177e582c4c4ba86448d7e32253db9aec0d4", "title": "BeetleVerse: A study on taxonomic classification of ground beetles", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-18", "authors": [{"authorId": "2372061439", "name": "S. M. Rayeed"}, {"authorId": "2356407407", "name": "Alyson East"}, {"authorId": "2356405492", "name": "Samuel Stevens"}, {"authorId": "2314528698", "name": "Sydne Record"}, {"authorId": "2356405880", "name": "Charles Stewart"}], "abstract": "Ground beetles are a highly sensitive and speciose biological indicator, making them vital for monitoring biodiversity. However, they are currently an underutilized resource due to the manual effort required by taxonomic experts to perform challenging species differentiations based on subtle morphological differences, precluding widespread applications. In this paper, we evaluate 12 vision models on taxonomic classification across four diverse, long-tailed datasets spanning over 230 genera and 1769 species, with images ranging from controlled laboratory settings to challenging field-collected (in-situ) photographs. We further explore taxonomic classification in two important real-world contexts: sample efficiency and domain adaptation. Our results show that the Vision and Language Transformer combined with an MLP head is the best performing model, with 97% accuracy at genus and 94% at species level. Sample efficiency analysis shows that we can reduce train data requirements by up to 50% with minimal compromise in performance. The domain adaptation experiments reveal significant challenges when transferring models from lab to in-situ images, highlighting a critical domain gap. Overall, our study lays a foundation for large-scale automated taxonomic classification of beetles, and beyond that, advances sample-efficient learning and cross-domain adaptation for diverse long-tailed ecological datasets."}
{"paperId": "7985ab7ae783becbee34478c4311928d3cf72b64", "url": "https://www.semanticscholar.org/paper/7985ab7ae783becbee34478c4311928d3cf72b64", "title": "Efficient text-to-video retrieval via multi-modal multi-tagger derived pre-screening", "venue": "Visual Intelligence", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "https://doi.org/10.1007/s44267-025-00073-2", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s44267-025-00073-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s44267-025-00073-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-06", "authors": [{"authorId": "2300520206", "name": "Yingjia Xu"}, {"authorId": "2263922016", "name": "Mengxia Wu"}, {"authorId": "2338533661", "name": "Zixin Guo"}, {"authorId": "2349223746", "name": "Min Cao"}, {"authorId": "2298911143", "name": "Mang Ye"}, {"authorId": "144376259", "name": "J. Laaksonen"}], "abstract": "Text-to-video retrieval (TVR) has made significant progress with advances in vision and language representation learning. Most existing methods use real-valued and hash-based embeddings to represent the video and text, allowing retrieval by computing their similarities. However, these methods are often inefficient for large volumes of video, and require significant storage and computing resources. In this work, we present a plug-and-play multi-modal multi-tagger-driven pre-screening framework, which pre-screens a substantial number of videos before applying any TVR algorithms, thereby efficiently reducing the search space of videos. We predict discrete semantic tags for video and text with our proposed multi-modal multi-tagger module, and then leverage an inverted index for space-efficient and fast tag matching to filter out irrelevant videos. To avoid filtering out relevant videos for text queries due to inconsistent tags, we utilize contrastive learning to align video and text embeddings, which are then fed into a shared multi-tag head. Extensive experimental results demonstrate that our proposed method significantly accelerates the TVR process while maintaining high retrieval accuracy on various TVR datasets."}
{"paperId": "79a5cf70d85c036d199a0931d06689519ee36b20", "url": "https://www.semanticscholar.org/paper/79a5cf70d85c036d199a0931d06689519ee36b20", "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.09658, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-07", "authors": [{"authorId": "2364055215", "name": "Filippo Rinaldi"}, {"authorId": "2180987161", "name": "Aniello Panariello"}, {"authorId": "2385466675", "name": "Giacomo Salici"}, {"authorId": "2332082364", "name": "Fengyuan Liu"}, {"authorId": "2385466669", "name": "Marco Ciccone"}, {"authorId": "51119730", "name": "Angelo Porrello"}, {"authorId": "2180561002", "name": "Simone Calderara"}], "abstract": "When a new release of a foundation model is published, practitioners typically need to repeat full fine-tuning, even if the same task has already been solved in the previous version. A promising alternative is to reuse the parameter changes (i.e., task vectors) that capture how a model adapts to a specific task. However, they often fail to transfer across different pre-trained models due to their misaligned parameter space. In this work, we show that the key to successful transfer lies in the sign structure of the gradients of the new model. Based on this insight, we propose GradFix, a novel method that approximates the ideal gradient sign structure and leverages it to transfer knowledge using only a handful of labeled samples. Notably, this requires no additional fine-tuning: the adaptation is achieved by computing a few gradients at the target model and masking the source task vector accordingly. This yields an update that is locally aligned with the target loss landscape, effectively rebasing the task vector onto the new pre-training. We provide a theoretical guarantee that our method ensures first-order descent. Empirically, we demonstrate significant performance gains on vision and language benchmarks, consistently outperforming naive task vector addition and few-shot fine-tuning."}
{"paperId": "79ae02c33f8008c520741531afa264c75be922c2", "url": "https://www.semanticscholar.org/paper/79ae02c33f8008c520741531afa264c75be922c2", "title": "Практика и возможности применения искусственного интеллекта в современном менеджменте", "venue": "Industrial Economics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.47576/2949-1886.2025.4.4.027?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.47576/2949-1886.2025.4.4.027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-10", "authors": [{"authorId": "74660990", "name": "Н. О. Козырева"}, {"authorId": "2308920385", "name": "П.Б. Рукобратский"}], "abstract": "В статье рассмотрены виды технологий искусственного интеллекта, такие как машинное обучение, компьютерное зрение, обработка естественного языка, нейронные сети, когнитивные вычисления, робототехника. Изучены опыт и возможности их применения для управления компаниями в сферах маркетинга, логистики, подбора и управления персоналом, финансовой, банковской, коммуникационной. Проанализирована уже имеющаяся российская и зарубежная практика внедрения искусственного интеллекта в менеджмент, негативные эффекты и риски, связанные с их внедрением: утечка персональных и других конфиденциальных данных, подделка документов, изображений, видео, голоса людей, хакерские атаки, генерация недостоверных данных, выход технологий искусственного интеллекта из под контроля человека, потеря рабочих мест.\n The article discusses the types of artificial intelligence technologies such as machine learning, computer vision, natural language processing, neural networks, cognitive computing, and robotics. The experience and possibilities of their application for the management of companies in the fields of marketing, logistics, recruitment and personnel management, finance, banking, and communications have been studied. The article analyzes the existing Russian and foreign practice of introducing artificial intelligence into management, the negative effects and risks associated with their implementation: leakage of personal and other confidential data, forgery of documents, images, videos, people’s voices, hacker attacks, generation of false data, the release of artificial intelligence technologies from human control, loss of jobs."}
{"paperId": "79d9a66f870077cc168bb783a2be5a3cd2d4a7cd", "url": "https://www.semanticscholar.org/paper/79d9a66f870077cc168bb783a2be5a3cd2d4a7cd", "title": "AI generations: from AI 1.0 to AI 4.0", "venue": "Frontiers Artif. Intell.", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review", "JournalArticle"], "publicationDate": "2025-02-16", "authors": [{"authorId": "2276968025", "name": "Jiahao Wu"}, {"authorId": "2184785759", "name": "Hengxu You"}, {"authorId": "2258638732", "name": "Jing Du"}], "abstract": "This paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI), AI 3.0 (Physical AI), and a speculative AI 4.0 (Conscious AI). Each AI generation is driven by shifting priorities among algorithms, computing power, and data. AI 1.0 accompanied breakthroughs in pattern recognition and information processing, fueling advances in computer vision, natural language processing, and recommendation systems. AI 2.0 is built on these foundations through real-time decision-making in digital environments, leveraging reinforcement learning and adaptive planning for agentic AI applications. AI 3.0 extended intelligence into physical contexts, integrating robotics, autonomous vehicles, and sensor-fused control systems to act in uncertain real-world settings. Building on these developments, the proposed AI 4.0 puts forward the bold vision of self-directed AI capable of setting its own goals, orchestrating complex training regimens, and possibly exhibiting elements of machine consciousness. This paper traces the historical foundations of AI across roughly 70 years, mapping how changes in technological bottlenecks from algorithmic innovation to high-performance computing to specialized data have stimulated each generational leap. It further highlights the ongoing synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the ethical, regulatory, and philosophical challenges that arise when artificial systems approach (or aspire to) human-like autonomy. Ultimately, understanding these evolutions and their interdependencies is pivotal for guiding future research, crafting responsible governance, and ensuring that AI’s transformative potential benefits society."}
{"paperId": "79faf5e35652acb16c16bde43964de13260dbfc2", "url": "https://www.semanticscholar.org/paper/79faf5e35652acb16c16bde43964de13260dbfc2", "title": "Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-19", "authors": [{"authorId": "2381058431", "name": "Ran Hong"}, {"authorId": "2381735395", "name": "Feng Lu"}, {"authorId": "2395725870", "name": "Leilei Cao"}, {"authorId": "2381057458", "name": "An Yan"}, {"authorId": "2376488097", "name": "Youhai Jiang"}, {"authorId": "2376425012", "name": "Fengjie Zhu"}], "abstract": "Referential Video Object Segmentation (RVOS) aims to segment all objects in a video that match a given natural language description, bridging the gap between vision and language understanding. Recent work, such as Sa2VA, combines Large Language Models (LLMs) with SAM~2, leveraging the strong video reasoning capability of LLMs to guide video segmentation. In this work, we present a training-free framework that substantially improves Sa2VA's performance on the RVOS task. Our method introduces two key components: (1) a Video-Language Checker that explicitly verifies whether the subject and action described in the query actually appear in the video, thereby reducing false positives; and (2) a Key-Frame Sampler that adaptively selects informative frames to better capture both early object appearances and long-range temporal context. Without any additional training, our approach achieves a J&F score of 64.14% on the MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge at ICCV 2025."}
{"paperId": "7ab10fec046316f5458637c2524936d72778f0e1", "url": "https://www.semanticscholar.org/paper/7ab10fec046316f5458637c2524936d72778f0e1", "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.01050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-02", "authors": [{"authorId": "2303624642", "name": "Kai Hu"}, {"authorId": "2258712398", "name": "Weicheng Yu"}, {"authorId": "2359097661", "name": "Li Zhang"}, {"authorId": "66684655", "name": "Alexander Robey"}, {"authorId": "2316393753", "name": "Andy Zou"}, {"authorId": "2316565296", "name": "Chengming Xu"}, {"authorId": "2359181752", "name": "Haoqi Hu"}, {"authorId": "2337690647", "name": "Matt Fredrikson"}], "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary VLLMs such as GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. Furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary VLLMs. Our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of VLLMs."}
{"paperId": "7ac2d85d2fdf07bfb9e4a70d3cd3c59d3b069bad", "url": "https://www.semanticscholar.org/paper/7ac2d85d2fdf07bfb9e4a70d3cd3c59d3b069bad", "title": "Harmonic Loss Trains Interpretable AI Models", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-03", "authors": [{"authorId": "2283307513", "name": "David D. Baek"}, {"authorId": "2145253202", "name": "Ziming Liu"}, {"authorId": "2343738880", "name": "Riya Tyagi"}, {"authorId": "2253461463", "name": "Max Tegmark"}], "abstract": "In this paper, we introduce harmonic loss as an alternative supervisory signal for training neural networks and large language models (LLMs). Harmonic loss differs from standard cross-entropy loss by (a) replacing the usual SoftMax normalization with a scale-invariant HarMax function and (b) computing logits via Euclidean distance rather than a dot product. Harmonic loss enables improved interpretability and faster convergence, owing to its scale invariance and finite convergence point by design, which can be interpreted as a class center. We first validate the performance of harmonic models across algorithmic, vision, and language datasets. Through extensive experiments, we demonstrate that models trained with harmonic loss perform better than standard models by: (a) enhancing interpretability, (b) requiring less data for generalization, and (c) reducing grokking. Moreover, we compare a GPT-2 model trained with harmonic loss to the standard GPT-2, illustrating that the harmonic model develops more interpretable representations. Looking forward, we believe harmonic loss may become a valuable tool in domains with limited data availability or in high-stakes applications where interpretability and reliability are paramount, paving the way for more robust and efficient neural network models."}
{"paperId": "7b35905fc8b54fd298edc6d10cb855e87629e1bc", "url": "https://www.semanticscholar.org/paper/7b35905fc8b54fd298edc6d10cb855e87629e1bc", "title": "Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-25", "authors": [{"authorId": "2273540855", "name": "Zhuo Chen"}, {"authorId": "47120498", "name": "Xinyu Wang"}, {"authorId": "2256747040", "name": "Yong Jiang"}, {"authorId": "2329286912", "name": "Zhen Zhang"}, {"authorId": "2347039781", "name": "Xinyu Geng"}, {"authorId": "35930962", "name": "Pengjun Xie"}, {"authorId": "2276428076", "name": "Fei Huang"}, {"authorId": "40341553", "name": "Kewei Tu"}], "abstract": "Despite the advancements made in Vision Large Language Models (VLLMs), like text Large Language Models (LLMs), they have limitations in addressing questions that require real-time information or are knowledge-intensive. Indiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an effective yet expensive way to enable models to answer queries beyond their knowledge scopes. To mitigate the dependence on retrieval and simultaneously maintain, or even improve, the performance benefits provided by retrieval, we propose a method to detect the knowledge boundary of VLLMs, allowing for more efficient use of techniques like RAG. Specifically, we propose a method with two variants that fine-tune a VLLM on an automatically constructed dataset for boundary identification. Experimental results on various types of Visual Question Answering datasets show that our method successfully depicts a VLLM's knowledge boundary, based on which we are able to reduce indiscriminate retrieval while maintaining or improving the performance. In addition, we show that the knowledge boundary identified by our method for one VLLM can be used as a surrogate boundary for other VLLMs. Code will be released at https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary"}
{"paperId": "7b5180a52665aa6a86758e9b6b8c0ff02b0929cf", "url": "https://www.semanticscholar.org/paper/7b5180a52665aa6a86758e9b6b8c0ff02b0929cf", "title": "Vision to Voice: Transforming Images into Audio Descriptions with Deep Learning", "venue": "International Conference Computing Methodologies and Communication", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCMC65190.2025.11140710?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCMC65190.2025.11140710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-23", "authors": [{"authorId": "2379256450", "name": "N. S. Vodnala"}, {"authorId": "2379255915", "name": "Sharan Adhvy Kanvapuri"}, {"authorId": "2379256669", "name": "Sai Praneetha"}, {"authorId": "2379256693", "name": "Katikireddy Sai"}, {"authorId": "2379639141", "name": "Sahithi Mohammed"}, {"authorId": "2379256176", "name": "Shaji Affan"}], "abstract": "The visually impaired have great difficulties sensing and engaging with the visual environment. Activities such as exploring public areas, recognizing objects, or retrieving visual information prove to be challenging when not properly supported. Although assistive technologies are available, the majority are short on contextual awareness and natural interaction. In this paper, we introduce \"Vision to Voice: Translating Images to Audio Descriptions with Deep Learning\", a system that utilizes the BLIP (Bootstrapped Language-Image Pre-training) model to produce precise, context-aware image captions, which are synthesized into speech via text-to-speech (TTS) synthesis. People can upload or photograph to receive real-time audio descriptions, making it easier with a more natural, voice-based interface. Our approach integrates progress in computer vision, natural language understanding, and deep learning to deliver semantically accurate and logically consistent descriptions. It could be applied in mobile apps, voice assistants, and blind-friendly self-navigating devices with offline availability, multilingual support, and AI-powered personalization."}
{"paperId": "7b7abad2a3f04c0b15e2e96fb2299e67ea872d7c", "url": "https://www.semanticscholar.org/paper/7b7abad2a3f04c0b15e2e96fb2299e67ea872d7c", "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-09", "authors": [{"authorId": "2316521512", "name": "Yunzhe Xu"}, {"authorId": "2316526437", "name": "Yiyuan Pan"}, {"authorId": "2384783089", "name": "Zhe Liu"}], "abstract": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir."}
{"paperId": "7b8ca9b86fb4409d08f3057a114ff72ddc12a5a6", "url": "https://www.semanticscholar.org/paper/7b8ca9b86fb4409d08f3057a114ff72ddc12a5a6", "title": "Multi-Modal Vision and Language Models for Real-Time Emergency Response", "venue": "IEEE International Conference on Tools with Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICTAI66417.2025.00175?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICTAI66417.2025.00175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-11-03", "authors": [{"authorId": null, "name": "Adil Zhiyenbayev"}, {"authorId": null, "name": "Rakhat Abdrakhmanov"}, {"authorId": null, "name": "Huseyin Atakan Varol"}, {"authorId": null, "name": "Adnan Yazici"}], "abstract": null}
{"paperId": "7c98de6e1d7ecc6a4e058ebde51aab69ee95462f", "url": "https://www.semanticscholar.org/paper/7c98de6e1d7ecc6a4e058ebde51aab69ee95462f", "title": "PHORECAST: Enabling AI Understanding of Public Health Outreach Across Populations", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2307002140", "name": "Rifaa Qadri"}, {"authorId": "2219690286", "name": "Anh N. Nhu"}, {"authorId": "2383975029", "name": "Swati Ramnath"}, {"authorId": "2379600274", "name": "L. Zheng"}, {"authorId": "2383973920", "name": "Raj Bhansali"}, {"authorId": "2383974083", "name": "Sylvette La Touche-Howard"}, {"authorId": "2383974113", "name": "Tracy Marie Zeeger"}, {"authorId": "2383975027", "name": "Tom Goldstein"}, {"authorId": "2384183160", "name": "Ming Lin"}], "abstract": "Understanding how diverse individuals and communities respond to persuasive messaging holds significant potential for advancing personalized and socially aware machine learning. While Large Vision and Language Models (VLMs) offer promise, their ability to emulate nuanced, heterogeneous human responses, particularly in high stakes domains like public health, remains underexplored due in part to the lack of comprehensive, multimodal dataset. We introduce PHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a multimodal dataset curated to enable fine-grained prediction of both individuallevel behavioral responses and community-wide engagement patterns to health messaging. This dataset supports tasks in multimodal understanding, response prediction, personalization, and social forecasting, allowing rigorous evaluation of how well modern AI systems can emulate, interpret, and anticipate heterogeneous public sentiment and behavior. By providing a new dataset to enable AI advances for public health, PHORECAST aims to catalyze the development of models that are not only more socially aware but also aligned with the goals of adaptive and inclusive health communication"}
{"paperId": "7d474200508ff2ab765e320fd0db4450230180cc", "url": "https://www.semanticscholar.org/paper/7d474200508ff2ab765e320fd0db4450230180cc", "title": "Comparative analysis of generic vision-language models in detecting and diagnosing inherited retinal diseases using fundus photographs", "venue": "Eye", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41433-025-04013-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41433-025-04013-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-07", "authors": [{"authorId": "2385553712", "name": "Xiang Meng"}, {"authorId": "2279975227", "name": "W. Wong"}, {"authorId": "2233655333", "name": "Krithi Pushpanathan"}, {"authorId": "2302084555", "name": "Sahana Srinivasan"}, {"authorId": "2301898622", "name": "Cancan Xue"}, {"authorId": "2384435980", "name": "Meng Wang"}, {"authorId": "2270444946", "name": "Heng Miao"}, {"authorId": "2360220548", "name": "Hengtong Li"}, {"authorId": "2384693711", "name": "Liping Yang"}, {"authorId": "2384434743", "name": "Ling-Ping Cen"}, {"authorId": "2287762606", "name": "Li Jia Chen"}, {"authorId": "2384818013", "name": "Hwei Wuen Chan"}, {"authorId": "6412770", "name": "Y. Tham"}, {"authorId": "2326727799", "name": "Ching-Yu Cheng"}], "abstract": null}
{"paperId": "7e22a72238575248fe7bbc99c0dd71ee08d2ec22", "url": "https://www.semanticscholar.org/paper/7e22a72238575248fe7bbc99c0dd71ee08d2ec22", "title": "Querying Labeled Time Series Data with Scenario Programs", "venue": "NASA Formal Methods", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.10627, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-13", "authors": [{"authorId": "50078672", "name": "Edward Kim"}, {"authorId": "2367142579", "name": "Devan Shanker"}, {"authorId": "2367141556", "name": "Varun Bharadwaj"}, {"authorId": "2294675389", "name": "Hongbeen Park"}, {"authorId": "2330219887", "name": "Jinkyu Kim"}, {"authorId": "3377980", "name": "Hazem Torfah"}, {"authorId": "2256347196", "name": "Daniel J. Fremont"}, {"authorId": "1775517", "name": "S. Seshia"}], "abstract": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data."}
{"paperId": "7e6a67e869c0c5389c11895b770b03f227d257d1", "url": "https://www.semanticscholar.org/paper/7e6a67e869c0c5389c11895b770b03f227d257d1", "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.18031, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-24", "authors": [{"authorId": "2316426420", "name": "Ahmad Albarqawi"}, {"authorId": "2304454949", "name": "Mahmoud Nazzal"}, {"authorId": "2219032227", "name": "Issa Khalil"}, {"authorId": "2304454957", "name": "Abdallah Khreishah"}, {"authorId": "11032760", "name": "Nhathai Phan"}], "abstract": "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity."}
{"paperId": "7eb90c3dffddcd1c7084080855f903c3a9a3f8b9", "url": "https://www.semanticscholar.org/paper/7eb90c3dffddcd1c7084080855f903c3a9a3f8b9", "title": "Low-Rank Adaptation for Parameter-Efficient Fine-Tuning in Composed Image Retrieval", "venue": "International Conference on Multimedia Retrieval", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3731715.3733377?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3731715.3733377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2368556939", "name": "Jiaxin Luo"}, {"authorId": "2293773247", "name": "Mingbo Zhao"}, {"authorId": "2289505582", "name": "Hongtao Zhang"}], "abstract": "Composed Image Retrieval (CIR) aims to accurately model users' retrieval intentions by combining reference images with modification text to search for desired images. Currently, the most advanced CIR models rely on Vision-and-Language Pretrained (VLP) models as their backbone. However, since this approach typically depends on full fine-tuning to adapt to downstream retrieval tasks, it may lead to the forgetting of pre-trained knowledge in the VLP model, thereby reducing its generalization ability and potentially causing overfitting. This paper introduces a Parameter-Efficient Fine-Tuning (PEFT) approach, leveraging Low-Rank Adaptation (LoRA) to fine-tune the CLIP model, i.e., a widely used VLP model for CIR tasks. Unlike conventional full fine-tuning methods, our approach preserves the original weights of the VLP model while effectively adapting it to downstream tasks, achieving superior performance in CIR tasks. Experiments on two benchmark datasets demonstrate that our LoRA-based PEFT method significantly improves retrieval recall, particularly when training data is limited."}
{"paperId": "7f11249a42a6d901cfd2160a78ce6dcaea05c46a", "url": "https://www.semanticscholar.org/paper/7f11249a42a6d901cfd2160a78ce6dcaea05c46a", "title": "Application of AI-Driven Multi-Scale Feature Fusion in Small Target Detection in Aerial Photography", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.tj23215?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.tj23215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-05-22", "authors": [{"authorId": "2362963516", "name": "Yutong Huang"}], "abstract": "Healthcare, banking, manufacturing, and education are just a few of the areas that artificial intelligence (AI) is transforming. In healthcare, AI-powered diagnostic tools enhance early disease detection through automated analysis of medical imaging, while in finance, machine learning algorithms optimize fraud detection and algorithmic trading strategies. Rapid developments in computer vision, Natural language processing (NLP), and deep learning have greatly improved AI's capacity for data analysis, automated decision-making, and intelligent human-machine interaction. For instance, computer vision enables autonomous vehicles to navigate complex environments, and NLP-driven chatbots streamline customer service interactions across sectors. AI-driven innovations are improving efficiency, accuracy, and productivity, but they also introduce challenges related to data privacy, ethical concerns, and technological limitations. This paper examines AIs key applications across multiple sectors, analyzing both its transformative potential and the obstacles hindering its widespread adoption. Additionally, it explores emerging trends, such as explainable AI, AI-driven automation, and regulatory developments, highlighting their implications for future research and policy-making. By conducting a comprehensive review of current advancements and challenges, this study provides insights into AIs evolving role and proposes strategic recommendations for its responsible and sustainable integration across industries."}
{"paperId": "7f1ea88b7d1aa06e4b6a58049d6a7bf464aeabf3", "url": "https://www.semanticscholar.org/paper/7f1ea88b7d1aa06e4b6a58049d6a7bf464aeabf3", "title": "PLOVAD: Prompting Vision-Language Models for Open Vocabulary Video Anomaly Detection", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2025.3528108?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2025.3528108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2340009564", "name": "Chenting Xu"}, {"authorId": "145031848", "name": "Ke Xu"}, {"authorId": "2297426051", "name": "Xinghao Jiang"}, {"authorId": "3307728", "name": "Tanfeng Sun"}], "abstract": "Video anomaly detection (VAD) confronts significant challenges arising from data scarcity in real-world open scenarios, encompassing sparse annotations, labeling costs, and limitations on closed-set class definitions, particularly when scene diversity surpasses available training data. Although current weakly-supervised VAD methods offer partial alleviation, their inherent confinement to closed-set paradigms renders them inadequate in open-world contexts. Therefore, this paper explores open vocabulary video anomaly detection (OVVAD), leveraging abundant vision-related language data to detect and categorize both seen and unseen anomalies. To this end, we propose a robust framework, PLOVAD, designed to prompt tuning large-scale pretrained image-based vision-language models (I-VLMs) for the OVVAD task. PLOVAD consists of two main modules: the Prompting Module, featuring a learnable prompt to capture domain-specific knowledge and an anomaly-specific prompt crafted by a large language model (LLM) to capture semantic nuances and enhance generalization; and the Temporal Module, which integrates temporal information using graph attention network (GAT) stacking atop frame-wise visual features to address the transition from static images to videos. Extensive experiments on four benchmarks demonstrate the superior detection and categorization performance of our approach in the OVVAD task without bringing excessive parameters."}
{"paperId": "7fab64a6436f57fa788a488691e7b36f91ebee34", "url": "https://www.semanticscholar.org/paper/7fab64a6436f57fa788a488691e7b36f91ebee34", "title": "Modeling the Internal and Contextual Attention for Self-Supervised Skeleton-Based Action Recognition", "venue": "Italian National Conference on Sensors", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12608129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-23", "authors": [{"authorId": "2004193826", "name": "Wentian Xin"}, {"authorId": "2388148058", "name": "Yue Teng"}, {"authorId": "2388117592", "name": "Jikang Zhang"}, {"authorId": "2233779826", "name": "Yi Liu"}, {"authorId": "1957459", "name": "Ruyi Liu"}, {"authorId": "2330228020", "name": "Yuzhi Hu"}, {"authorId": "2268219763", "name": "Qiguang Miao"}], "abstract": "Multimodal contrastive learning has achieved significant performance advantages in self-supervised skeleton-based action recognition. Previous methods are limited by modality imbalance, which reduces alignment accuracy and makes it difficult to combine important spatial–temporal frequency patterns, leading to confusion between modalities and weaker feature representations. To overcome these problems, we explore intra-modality feature-wise self-similarity and inter-modality instance-wise cross-consistency, and discover two inherent correlations that benefit recognition: (i) Global Perspective expresses how action semantics carry a broad and high-level understanding, which supports the use of globally discriminative feature representations. (ii) Focus Adaptation refers to the role of the frequency spectrum in guiding attention toward key joints by emphasizing compact and salient signal patterns. Building upon these insights, we propose a novel language–skeleton contrastive learning framework comprising two key components: (a) Feature Modulation, which constructs a skeleton–language action conceptual domain to minimize the expected information gain between vision and language modalities. (b) Frequency Feature Learning, which introduces a Frequency-domain Spatial–Temporal block (FreST) that focuses on sparse key human joints in the frequency domain with compact signal energy. Extensive experiments demonstrate the effectiveness of our method achieves remarkable action recognition performance on widely used benchmark datasets, including NTU RGB+D 60 and NTU RGB+D 120. Especially on the challenging PKU-MMD dataset, MICA has achieved at least a 4.6% improvement over classical methods such as CrosSCLR and AimCLR, effectively demonstrating its ability to capture internal and contextual attention information."}
{"paperId": "7fe75b5f731d7e07e264ba596cf2dc36e13167b3", "url": "https://www.semanticscholar.org/paper/7fe75b5f731d7e07e264ba596cf2dc36e13167b3", "title": "Semantic Understanding and Response Model of Service Robots Based on Image Processing and Human-Computer Interaction", "venue": "2025 5th International Conference on Computer Science and Blockchain (CCSB)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCSB66722.2025.11154222?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCSB66722.2025.11154222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-01", "authors": [{"authorId": "2381264579", "name": "Shan Liang"}], "abstract": "In response to the problem that the existing service robots' semantic understanding relies on a single language input and the responses lack scene adaptability, this paper proposes a semantic understanding and response model that integrates image processing and human-computer interaction. This model extracts the attributes and associated information of environmental targets through the visual semantic parsing module (based on the improved YOLOv8 for object detection and GNN for scene relationship modeling), combines the language semantic understanding module (BERT for intent classification and entity linking) to parse user instructions, and realizes cross-modal semantic alignment and scene adaptation reasoning through the attention mechanism, ultimately generating actions and language responses. Experimental results show that on the household service scenario dataset, the visual - language entity matching rate of this model reaches 92.3 %, and the success rate of action responses is 89.2 %, which is significantly higher than the pure language model (78.5%, 65.3%) and the unoptimized fusion model (85.1%, 72.5%), verifying the effectiveness of the integration of vision and language in improving the accuracy of service robot semantic understanding and response adaptability."}
{"paperId": "7ffaefadd7b0b72bdc1d35acb6b7da04fbe487fc", "url": "https://www.semanticscholar.org/paper/7ffaefadd7b0b72bdc1d35acb6b7da04fbe487fc", "title": "Beyond the Failures: Rethinking Foundation Models in Pathology", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.23807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-10-27", "authors": [{"authorId": "2267733600", "name": "H. R. Tizhoosh"}], "abstract": "Despite their successes in vision and language, foundation models have stumbled in pathology, revealing low accuracy, instability, and heavy computational demands. These shortcomings stem not from tuning problems but from deeper conceptual mismatches: dense embeddings cannot represent the combinatorial richness of tissue, and current architectures inherit flaws in self-supervision, patch design, and noise-fragile pretraining. Biological complexity and limited domain innovation further widen the gap. The evidence is clear-pathology requires models explicitly designed for biological images rather than adaptations of large-scale natural-image methods whose assumptions do not hold for tissue."}
{"paperId": "80788ae4f103b1b83c9770d3d6e250a284969395", "url": "https://www.semanticscholar.org/paper/80788ae4f103b1b83c9770d3d6e250a284969395", "title": "Efficient Fine-Tuning of Multimodal Language Models for Medical AI via LoRA and 4-bit Quantization on Qwen2.5-VL", "venue": "2025 7th International Conference on Data-driven Optimization of Complex Systems (DOCS)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DOCS67533.2025.11200901?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DOCS67533.2025.11200901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-19", "authors": [{"authorId": "2386561823", "name": "Zhijian Du"}, {"authorId": "2318249762", "name": "Yang Gao"}, {"authorId": "2317130477", "name": "Lianbo Ma"}], "abstract": "Multimodal large-language models (MLLMs) hold great promise for clinical vision-language tasks but are challenging to fine-tune and deploy due to their substantial computational and memory requirements. This paper presents an efficient finetuning strategy that integrates 4-bit quantization via the BitsAndBytes (bnb) framework with Low-Rank Adaptation (LoRA), targeting the Qwen2.5-VL-3B-Instruct model for medical visual question answering (Med-VQA). By injecting LoRA adapters into both the vision and language transformer layers while keeping the base model weights in 4bit format, the proposed method achieves significant resource savings with minimal performance degradation. Evaluated on a filtered subset of the MedTrinity-25M dataset, the 4-bit + LoRA model retains more than $96 \\%$ of the full-precision performance. It reduces inference latency by over $\\mathbf{8 0 \\%}$ (down to $\\mathbf{8 7}$ milliseconds per sample) and increases throughput by more than threefold (up to 18.4 samples per second), all within an 8 GB GPU memory footprint. This work demonstrates that real-time medical inference using large multimodal models is feasible on commodity hardware, offering a practical, modular, and reproducible pathway for deploying MLLMs in resource-constrained clinical environments."}
{"paperId": "811488aefc1a31e79493ed1eeb82124013fb97c5", "url": "https://www.semanticscholar.org/paper/811488aefc1a31e79493ed1eeb82124013fb97c5", "title": "DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person Retrieval", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-06", "authors": [{"authorId": "2242965080", "name": "Yating Liu"}, {"authorId": "2243875443", "name": "Zimo Liu"}, {"authorId": "2348742609", "name": "Xiangyuan Lan"}, {"authorId": "145451225", "name": "Wenming Yang"}, {"authorId": "2243105794", "name": "Yaowei Li"}, {"authorId": "2242922181", "name": "Qingmin Liao"}], "abstract": "Text-based person retrieval (TPR) has gained significant attention as a fine-grained and challenging task that closely aligns with practical applications. Tailoring CLIP to person domain is now a emerging research topic due to the abundant knowledge of vision-language pretraining, but challenges still remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is computationally expensive and prone to overfitting.(ii) Existing parameter-efficient transfer learning (PETL) for TPR lacks of fine-grained feature extraction. To address these issues, we propose Domain-Aware Mixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and PETL to enhance fine-grained feature representations while maintaining efficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to MLP layers in both vision and language branches, where different experts specialize in distinct aspects of person knowledge to handle features more finely. To promote the router to exploit domain information effectively and alleviate the routing imbalance, Domain-Aware Router is then developed by building a novel gating function and injecting learnable domain-aware prompts. Extensive experiments show that our DM-Adapter achieves state-of-the-art performance, outperforming previous methods by a significant margin."}
{"paperId": "816f4698d4b4aa010b737800360f56429eac0a1e", "url": "https://www.semanticscholar.org/paper/816f4698d4b4aa010b737800360f56429eac0a1e", "title": "Multi-Grained Vision-and-Language Model for Medical Image and Text Alignment", "venue": "IEEE transactions on multimedia", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2025.3590930?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2025.3590930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2347188709", "name": "Huimin Yan"}, {"authorId": "2233979100", "name": "Xian Yang"}, {"authorId": "2075397079", "name": "Liang Bai"}, {"authorId": "2372128546", "name": "Jiamin Li"}, {"authorId": "2237914945", "name": "Jiye Liang"}], "abstract": "The increasing interest in learning from paired medical images and textual reports highlights the need for methods that can achieve multi-grained alignment between these two modalities. However, most existing approaches overlook fine-grained semantic alignment, which can constrain the quality of the generated representations. To tackle this problem, we propose the Multi-Grained Vision-and-Language Alignment (MGVLA) model, which effectively leverages multi-grained correspondences between medical images and texts at different levels, including disease, instance, and token levels. For disease-level alignment, our approach adopts the concept of contrastive learning and uses medical terminologies detected from textual reports as soft labels to guide the alignment process. At the instance level, we propose a strategy for sampling hard negatives, where images and texts with the same disease type but differing in details such as disease locations and severity are considered as hard negatives. This strategy helps our approach to better distinguish between positive and negative image-text pairs, ultimately enhancing the quality of our learned representations. For token-level alignment, we employ a masking and recovery technique to achieve fine-grained semantic alignment between patches and sub-words. This approach effectively aligns the different levels of granularity between the image and language modalities. To assess the efficacy of our MGVLA model, we conduct comprehensive experiments on the image-text retrieval and phrase grounding tasks."}
{"paperId": "81c7caff824b9759e496737df336302405f599be", "url": "https://www.semanticscholar.org/paper/81c7caff824b9759e496737df336302405f599be", "title": "CLIP-like Model as a Foundational Density Ratio Estimator", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.22881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-06-28", "authors": [{"authorId": "2325095322", "name": "Fumiya Uchiyama"}, {"authorId": "2293613460", "name": "Rintaro Yanagi"}, {"authorId": "2184919803", "name": "Shohei Taniguchi"}, {"authorId": "2323750981", "name": "Shota Takashiro"}, {"authorId": "2327006455", "name": "Masahiro Suzuki"}, {"authorId": "2358263052", "name": "Hirokatsu Kataoka"}, {"authorId": "1715282", "name": "Yusuke Iwasawa"}, {"authorId": "2257260914", "name": "Yutaka Matsuo"}], "abstract": "Density ratio estimation is a core concept in statistical machine learning because it provides a unified mechanism for tasks such as importance weighting, divergence estimation, and likelihood-free inference, but its potential in vision and language models has not been fully explored. Modern vision-language encoders such as CLIP and SigLIP are trained with contrastive objectives that implicitly optimize log density ratios between joint and marginal image-text distributions, which implicitly learn similarity scores proportional to log density ratios. However, prior work has largely focused on their embedding utility, and the density-ratio structure induced by contrastive learning has not been systematically examined or exploited in multimodal applications. To address this gap, we reinterpret CLIP-style models as pretrained and general-purpose density ratio estimators and show that this perspective enables new algorithmic capabilities. We present a unified explanation of how contrastive objectives estimate density ratios and propose two practical applications: Importance Weight Learning and KL divergence estimation. Our Importance Weight Learning method requires only a single additional prompt and improves F1 scores by up to 7 points. We further show that CLIP-based density ratios support estimation of KL divergences that quantify how conditioning on an image or text alters the distribution of the other modality. Through qualitative examples and an N-gram analysis of captions, we find that these divergences capture semantic diversity and mode structure in multimodal data. Leveraging this property, we introduce a simple KL-guided data curation method that achieves performance competitive with LAION2B filtering."}
{"paperId": "822edee270d5efbe97edb3449e28a0b082d52d75", "url": "https://www.semanticscholar.org/paper/822edee270d5efbe97edb3449e28a0b082d52d75", "title": "Towards Superior Quantization Accuracy: A Layer-sensitive Approach", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-09", "authors": [{"authorId": "2348580483", "name": "Feng Zhang"}, {"authorId": "2348075237", "name": "Yanbin Liu"}, {"authorId": "2274076746", "name": "Weihua Li"}, {"authorId": "2349961381", "name": "Jie Lv"}, {"authorId": "2274095822", "name": "Xiaodan Wang"}, {"authorId": "71203570", "name": "Quan-wei Bai"}], "abstract": "Large Vision and Language Models have exhibited remarkable human-like intelligence in tasks such as natural language comprehension, problem-solving, logical reasoning, and knowledge retrieval. However, training and serving these models require substantial computational resources, posing a significant barrier to their widespread application and further research. To mitigate this challenge, various model compression techniques have been developed to reduce computational requirements. Nevertheless, existing methods often employ uniform quantization configurations, failing to account for the varying difficulties across different layers in quantizing large neural network models. This paper tackles this issue by leveraging layer-sensitivity features, such as activation sensitivity and weight distribution Kurtosis, to identify layers that are challenging to quantize accurately and allocate additional memory budget. The proposed methods, named SensiBoost and KurtBoost, respectively, demonstrate notable improvement in quantization accuracy, achieving up to 9% lower perplexity with only a 2% increase in memory budget on LLama models compared to the baseline."}
{"paperId": "824ab671f78dfe3b176d295661bacb72d6562aeb", "url": "https://www.semanticscholar.org/paper/824ab671f78dfe3b176d295661bacb72d6562aeb", "title": "AdaViP: Aligning Multi-modal LLMs via Adaptive Vision-enhanced Preference Optimization", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-22", "authors": [{"authorId": "2350682480", "name": "Jinda Lu"}, {"authorId": "2293567480", "name": "Jinghan Li"}, {"authorId": "2283847842", "name": "Yuan Gao"}, {"authorId": "2260622979", "name": "Junkang Wu"}, {"authorId": "1491035012", "name": "Jiancan Wu"}, {"authorId": "2343828495", "name": "Xiang Wang"}, {"authorId": "2240825631", "name": "Xiangnan He"}], "abstract": "Preference alignment through Direct Preference Optimization (DPO) has demonstrated significant effectiveness in aligning multimodal large language models (MLLMs) with human preferences. However, existing methods focus primarily on language preferences while neglecting the critical visual context. In this paper, we propose an Adaptive Vision-enhanced Preference optimization (AdaViP) that addresses these limitations through two key innovations: (1) vision-based preference pair construction, which integrates multiple visual foundation models to strategically remove key visual elements from the image, enhancing MLLMs' sensitivity to visual details; and (2) adaptive preference optimization that dynamically balances vision- and language-based preferences for more accurate alignment. Extensive evaluations across different benchmarks demonstrate our effectiveness. Notably, our AdaViP-7B achieves 93.7% and 96.4% reductions in response-level and mentioned-level hallucination respectively on the Object HalBench, significantly outperforming current state-of-the-art methods."}
{"paperId": "8256130b0b5a5e70fd3c2c01be762ab2fb2d2cbb", "url": "https://www.semanticscholar.org/paper/8256130b0b5a5e70fd3c2c01be762ab2fb2d2cbb", "title": "Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-02", "authors": [{"authorId": "2381397868", "name": "Hong-Jie You"}, {"authorId": "50878225", "name": "Jiejing Shao"}, {"authorId": "2189204591", "name": "Xiao-Wen Yang"}, {"authorId": "2119414036", "name": "Lin Jia"}, {"authorId": "2350432846", "name": "Lan-Zhe Guo"}, {"authorId": "2288553539", "name": "Yu-Feng Li"}], "abstract": "Existing methods for expressive music performance rendering rely on supervised learning over small labeled datasets, which limits scaling of both data volume and model size, despite the availability of vast unlabeled music, as in vision and language. To address this gap, we introduce Pianist Transformer, with four key contributions: 1) a unified Musical Instrument Digital Interface (MIDI) data representation for learning the shared principles of musical structure and expression without explicit annotation; 2) an efficient asymmetric architecture, enabling longer contexts and faster inference without sacrificing rendering quality; 3) a self-supervised pre-training pipeline with 10B tokens and 135M-parameter model, unlocking data and model scaling advantages for expressive performance rendering; 4) a state-of-the-art performance model, which achieves strong objective metrics and human-level subjective ratings. Overall, Pianist Transformer establishes a scalable path toward human-like performance synthesis in the music domain."}
{"paperId": "827b688e9f6d296ae6604ad8e4a576b99c83a997", "url": "https://www.semanticscholar.org/paper/827b688e9f6d296ae6604ad8e4a576b99c83a997", "title": "A LLM-Based Video Frame Rate Up-Conversion Method for the Low-Cost IoT Node", "venue": "International Journal of High Speed Electronics and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1142/s0129156425406035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1142/s0129156425406035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-02", "authors": [{"authorId": "2247431909", "name": "Ran Li"}, {"authorId": "2364743416", "name": "Zhen Yang"}, {"authorId": "2053747578", "name": "Xiao Tu"}], "abstract": "The rapid development of the Internet of Things (IoT) has significantly expanded its application scope, making it a cornerstone for emerging technologies such as big data and artificial intelligence. IoT systems, particularly low-cost nodes, face unique challenges in video processing tasks, including real-time video streaming and analytics. Frame Rate Up-Conversion (FRUC) is a critical technique that enhances video quality and user experience by increasing video frame rates. Central to FRUC is Bidirectional Motion Estimation (BME), which estimates motion between frames to generate intermediate frames. However, traditional BME-based methods often suffer from motion blur and edge artifacts, which degrade interpolation quality. Neural network-based FRUC approaches have improved interpolation accuracy but are computationally intensive and require extensive labeled datasets, making them unsuitable for low-cost IoT nodes with limited resources. To address these limitations, we propose a novel FRUC framework specifically designed for resource-constrained IoT devices. This framework leverages the inference capabilities of Large Language Models (LLMs) combined with Vision-to-Language (V2L) tokenization to enable efficient and accurate video frame interpolation. Contextual learning samples are extracted from consecutive video frames and transformed into structured tokens using the V2L tokenizer. These tokens, along with task-specific prompts, are processed by the LLM to generate high-quality interpolated frames. The proposed method eliminates the need for task-specific network training, significantly reducing computational overhead and enabling real-time processing on low-cost IoT nodes.Experiments conducted on the Vimeo90K dataset demonstrate that our approach achieves superior interpolation quality compared to traditional methods while maintaining computational efficiency, making it a practical solution for IoT applications. This framework highlights the potential of integrating LLM-based reasoning with lightweight video processing technologies, paving the way for enhanced multimedia capabilities in the next generation of IoT devices."}
{"paperId": "829919d739e81245130c5cf773b2b094f9fbbb64", "url": "https://www.semanticscholar.org/paper/829919d739e81245130c5cf773b2b094f9fbbb64", "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.16516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-23", "authors": [{"authorId": "2356971209", "name": "Junrong Yue"}, {"authorId": "2315640931", "name": "Yifan Zhang"}, {"authorId": "2356853098", "name": "Chuan Qin"}, {"authorId": "2368409268", "name": "Bo Li"}, {"authorId": "2356852653", "name": "Xiaomin Lie"}, {"authorId": "2357466198", "name": "Xinlei Yu"}, {"authorId": "2315628290", "name": "Wenxin Zhang"}, {"authorId": "2357865344", "name": "Zhendong Zhao"}], "abstract": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation."}
{"paperId": "832128fa44868362e97bb29386c5c1425f428e78", "url": "https://www.semanticscholar.org/paper/832128fa44868362e97bb29386c5c1425f428e78", "title": "UA-VLFM: An Uncertainty-aware Vision-Language Foundation Model for Auxiliary Diagnosis of Vitreoretinal Iymphoma.", "venue": "IEEE journal of biomedical and health informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JBHI.2025.3611985?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JBHI.2025.3611985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-19", "authors": [{"authorId": "2381237282", "name": "Wenwen Wang"}, {"authorId": "145494496", "name": "Aidi Lin"}, {"authorId": "2306763037", "name": "Tian Lin"}, {"authorId": "2392820841", "name": "Zhen Liang"}, {"authorId": "2339256596", "name": "Kai Xu"}, {"authorId": "2337101605", "name": "Tao Li"}, {"authorId": "2336548075", "name": "Dan Liang"}, {"authorId": "2336324532", "name": "Shanshan Yu"}, {"authorId": "2336655432", "name": "Jing Luo"}, {"authorId": "2336674145", "name": "Ling Gao"}, {"authorId": "2277997567", "name": "Dawei Sun"}, {"authorId": "2306477326", "name": "Xinjian Chen"}, {"authorId": "2279897995", "name": "Haoyu Chen"}, {"authorId": "1796227131", "name": "Yuanyuan Peng"}], "abstract": "Vitreoretinal lymphoma (VRL) is a rare malignant ocular tumor, and its early diagnosis is crucial for patient prognosis. However, due to its insidious and diverse clinical manifestations, it is often misdiagnosed as other ophthalmic diseases, leading to blindness or even fatal outcomes. In this study, an uncertainty-aware visionlanguage foundational model (UA-VLFM) based on contrastive learning and uncertainty estimation is developed to achieve automatic classification of VRL and other 5 retinal diseases. First, we integrate MAE-based pretraining knowledge on large-scale optical coherence tomography (OCT) images and efficient Low-rank adaption (LoRA) optimization strategy to enhance the representation ability and optimization efficiency of the model. Moreover, an uncertainty-aware contrastive learning method based on Dirichlet distribution within the contrastive vision-language pretraining framework is proposed to further align vision and language feature in the high-dimensional embedding space and obtain prediction results with corresponding uncertainty scores, thereby enhancing the reliability of VRL diagnosis. In the test dataset with 5,563 OCT images, UA-VLFM achieves a higher average F1 score of 0.9684 than other state-of-the-art algorithms (0.8186-0.9427) and improves to 0.9839 with the threshold strategy. Notably, the proposed UA-VLFM achieves an F1 score of 0.9217 and 0.9544 before and after thresholding on VRL, the most challenging category, significantly outperforming other methods (0.5089-0.9366 and 0.6639-0.9133). Our UA-VLFM provides a trustworthy method for aiding in the diagnosis of VRL on retinal OCT images. The code has been released on Github: https://github.com/wang-wen-wen/UA-VLFM."}
{"paperId": "833e33fb986a3f852c37c8b99a49c348957bcd35", "url": "https://www.semanticscholar.org/paper/833e33fb986a3f852c37c8b99a49c348957bcd35", "title": "Accurate and Efficient Low-Rank Model Merging in Core Space", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.17786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-22", "authors": [{"authorId": "2180987161", "name": "Aniello Panariello"}, {"authorId": "2029559325", "name": "Daniel Marczak"}, {"authorId": "2220781605", "name": "Simone Magistri"}, {"authorId": "51119730", "name": "Angelo Porrello"}, {"authorId": "2470703", "name": "Bartłomiej Twardowski"}, {"authorId": "1749498", "name": "Andrew D. Bagdanov"}, {"authorId": "2180561002", "name": "Simone Calderara"}, {"authorId": "1834119810", "name": "J. Weijer"}], "abstract": "In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging."}
{"paperId": "834f8963ee40e4356dfba0b923cdcc461c3a4168", "url": "https://www.semanticscholar.org/paper/834f8963ee40e4356dfba0b923cdcc461c3a4168", "title": "Planning and Construction of Smart Community Based on AIoT Technology", "venue": "Proceedings of the 2025 International Conference on Artificial Intelligence and Smart Manufacturing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3756423.3756536?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3756423.3756536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-05-09", "authors": [{"authorId": "2318682852", "name": "Qiu-Chi Li"}, {"authorId": "2381496397", "name": "Fei Qi"}], "abstract": "This article systematically introduces the key role and mechanism of AI Internet of Things (AIoT) technology in empowering smart community. Based on AIoT technology, the overall architecture design of smart community is carried out, and the full chain integration empowers smart community. The applications in typical scenarios such as smart security, smart lighting, smart environmental protection, smart community healthcare, smart parking, smart home, smart community service management, and smart building management are elaborated in detail. Further research is needed to analyze how artificial intelligence can apply key technologies such as computer vision, natural language processing, machine learning, and knowledge graphs to practical scenarios in smart community. It can play an important role in the informationization construction, equipment construction, platform construction, and service construction of smart community, improve residents' quality of life, optimize community resource allocation, and enhance community management efficiency. With the iterative updates and widespread application of AIoT technology, more possibilities will be provided for the construction and management of smart community. This article provides important references for the further evolution of smart community towards digitization, intelligence, sustainable development, and other directions."}
{"paperId": "836114fce25feaa508c86e05232e405fe8b54f6c", "url": "https://www.semanticscholar.org/paper/836114fce25feaa508c86e05232e405fe8b54f6c", "title": "Monkey See, Model Knew: Large Language Models Accurately Predict Visual Brain Responses in Humans and Non-Human Primates", "venue": "bioRxiv", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "https://doi.org/10.1101/2025.03.05.641284", "status": "GREEN", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.03.05.641284?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.03.05.641284, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-04-09", "authors": [{"authorId": "117698162", "name": "Colin Conwell"}, {"authorId": "2349428112", "name": "Emalie MacMahon"}, {"authorId": "2330764460", "name": "Akshay Jagadeesh"}, {"authorId": "5927638", "name": "Kasper Vinken"}, {"authorId": "2299441837", "name": "Saloni Sharma"}, {"authorId": "144937473", "name": "Jacob S. Prince"}, {"authorId": "2238242675", "name": "George A. Alvarez"}, {"authorId": "2745756", "name": "Talia Konkle"}, {"authorId": "2323211059", "name": "Margaret S. Livingstone"}, {"authorId": "2254394810", "name": "Leyla Isik"}], "abstract": "Recent progress in multimodal AI and ‘language-aligned’ visual representation learning has rekindled debates about the role of language in shaping the human visual system. In particular, the emergent ability of ‘language-aligned’ vision models (e.g. CLIP) – and even pure language models (e.g. BERT) – to predict image-evoked brain activity has led some to suggest that human visual cortex itself may be ‘language-aligned’ in comparable ways. But what would we make of this claim if the same procedures could model visual activity in a species without language? Here, we conducted controlled comparisons of pure-vision, pure-language, and multimodal vision-language models in their prediction of human (N=4) and rhesus macaque (N=6, 5:IT, 1:V1) ventral visual activity to the same set of 1000 captioned natural images (the ‘NSD1000’). The results revealed markedly similar patterns in model predictivity of early and late ventral visual cortex across both species. This suggests that language model predictivity of the human visual system is not necessarily due to the evolution or learning of language perse, but rather to the statistical structure of the visual world that is reflected in natural language."}
{"paperId": "8375d8ebcebbcb9458b94ab5872a092c82e2f549", "url": "https://www.semanticscholar.org/paper/8375d8ebcebbcb9458b94ab5872a092c82e2f549", "title": "Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12776, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-18", "authors": [{"authorId": "2113252066", "name": "Daiki Chijiwa"}, {"authorId": "2273526972", "name": "Taku Hasegawa"}, {"authorId": "2006479562", "name": "Kyosuke Nishida"}, {"authorId": "2273606558", "name": "Kuniko Saito"}, {"authorId": "2284984838", "name": "Susumu Takeuchi"}], "abstract": "While foundation models have been exploited for various expert tasks through fine-tuning, any foundation model will become outdated due to its old knowledge or limited capability. Thus the underlying foundation model should be eventually replaced by new ones, which leads to repeated cost of fine-tuning these new models. Existing work addresses this problem by inference-time tuning, i.e., modifying the output probabilities from the new foundation model with the outputs from the old foundation model and its fine-tuned model, which involves an additional overhead in inference by the latter two models. In this paper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT), that reduces the inference overhead by its nature, based on the reformulation of fine-tuning as the reward maximization. Specifically, instead of fine-tuning parameters of the foundation models, PRT trains the reward model explicitly through the same loss function as in fine-tuning. During inference, the reward model can be used with any foundation model (with the same set of vocabularies or labels) through the formulation of reward maximization. Experimental results, covering both vision and language models, demonstrate that the PRT-trained model can achieve comparable accuracy to the existing work of inference-time tuning, with less inference cost."}
{"paperId": "838f7c0656f56f23f25ad42ceae3dcc7c340c7c7", "url": "https://www.semanticscholar.org/paper/838f7c0656f56f23f25ad42ceae3dcc7c340c7c7", "title": "Emu3.5: Native Multimodal Models are World Learners", "venue": "arXiv.org", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.26583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-30", "authors": [{"authorId": "2263702215", "name": "Yufeng Cui"}, {"authorId": "2389421205", "name": "Honghao Chen"}, {"authorId": "2269127146", "name": "Haoge Deng"}, {"authorId": "2392342028", "name": "Xu Huang"}, {"authorId": "2371133291", "name": "Xinghang Li"}, {"authorId": "2336001448", "name": "Jirong Liu"}, {"authorId": "2388179881", "name": "Yang Liu"}, {"authorId": "2361707226", "name": "Zhuoyan Luo"}, {"authorId": "2257130183", "name": "Jinsheng Wang"}, {"authorId": "2361667090", "name": "Wenxuan Wang"}, {"authorId": "2217456303", "name": "Yueze Wang"}, {"authorId": "2389050519", "name": "Chengyuan Wang"}, {"authorId": "2264274388", "name": "Fan Zhang"}, {"authorId": "2323418301", "name": "Yingli Zhao"}, {"authorId": "2384326059", "name": "Ting Pan"}, {"authorId": "2389412050", "name": "Xianduo Li"}, {"authorId": "2204581289", "name": "Zecheng Hao"}, {"authorId": "2362607296", "name": "Wen-Rui Ma"}, {"authorId": "2358408778", "name": "Zhuo Chen"}, {"authorId": "2315986393", "name": "Yulong Ao"}, {"authorId": "2257137554", "name": "Tiejun Huang"}, {"authorId": "2391766321", "name": "Zhongyuan Wang"}, {"authorId": "2263959521", "name": "Xinlong Wang"}], "abstract": "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research."}
{"paperId": "83ac1df9a7d8d29330b7daf5281c0848a66c4960", "url": "https://www.semanticscholar.org/paper/83ac1df9a7d8d29330b7daf5281c0848a66c4960", "title": "Multimodal Generative AI Framework for Therapeutic Decision Support in Autism Spectrum Disorder", "venue": "Ubiquitous Computing, Electronics & Mobile Communication Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/UEMCON67449.2025.11267611?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/UEMCON67449.2025.11267611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2386604803", "name": "Santosh Kumar"}], "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by deficits in social communication, repetitive behaviors, and atypical sensory processing. Traditional therapies, while effective, often face barriers such as limited availability of trained therapists, high costs, and inconsistent access. To address these challenges, this paper presents an AI-driven multimodal therapeutic framework that integrates computer vision, natural language processing, and sketch-based interpretation models to deliver affordable, scalable, and personalized ASD therapy. The proposed system comprises three modules: (i) a behavior therapy module leveraging pose estimation and emotion recognition to guide social skill acquisition, (ii) a speech therapy module utilizing lip-reading and speech-text alignment for articulation training, and (iii) an interpretation therapy module employing sketch recognition to foster creativity and conceptual understanding. Using datasets such as COCO (for pose estimation), QuickDraw (for sketch learning), and curated speech corpora, the system demonstrates competitive accuracy in activity recognition, word pronunciation scoring, and drawing similarity assessment. Experimental results highlight the framework's capacity to deliver consistent therapy sessions, achieve quantifiable progress tracking, and provide explainable feedback to caregivers and clinicians. This work represents a step toward AI-enabled precision therapies that bridge accessibility gaps and augment humancentered care."}
{"paperId": "84446ca4f4a8f370c586b6b6052ea3dde29e15e4", "url": "https://www.semanticscholar.org/paper/84446ca4f4a8f370c586b6b6052ea3dde29e15e4", "title": "SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding", "venue": "IEEE transactions on multimedia", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-24", "authors": [{"authorId": "2302774963", "name": "Liangtao Shi"}, {"authorId": "2238259402", "name": "Ting Liu"}, {"authorId": "2220701228", "name": "Xiantao Hu"}, {"authorId": "2284120737", "name": "Yue Hu"}, {"authorId": "2283850663", "name": "Quanjun Yin"}, {"authorId": "2331327732", "name": "Richang Hong"}], "abstract": "Visual grounding aims to ground an image region through natural language, which heavily relies on cross-modal alignment. Most existing methods transfer visual/linguistic knowledge separately by fully fine-tuning uni-modal pre-trained models, followed by a simple stack of visual-language transformers for multimodal fusion. However, these approaches not only limit adequate interaction between visual and linguistic contexts, but also incur significant computational costs. Therefore, to address these issues, we explore a step-wise multimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVG proposes step-wise multimodal prompts (Swip) and cross-modal interactive adapters (CIA) for visual grounding, replacing the cumbersome transformer stacks for multimodal fusion. Swip can improve {the} alignment between the vision and language representations step by step, in a token-level fusion manner. In addition, weight-level CIA further promotes multimodal fusion by cross-modal interaction. Swip and CIA are both parameter-efficient paradigms, and they fuse the cross-modal features from shallow to deep layers gradually. Experimental results on four widely-used benchmarks demonstrate that SwimVG achieves remarkable abilities and considerable benefits in terms of efficiency. Our code is available at https://github.com/liuting20/SwimVG."}
{"paperId": "845cd963db83b7a4ea90eb786a060fcfab571aec", "url": "https://www.semanticscholar.org/paper/845cd963db83b7a4ea90eb786a060fcfab571aec", "title": "AI-Powered Mock Interview Platform using Computer Vision, Natural Language Processing and Generative AI", "venue": "2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSSAS66150.2025.11080941?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSSAS66150.2025.11080941, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-06-11", "authors": [{"authorId": "2323636680", "name": "Tanishque Sharma"}, {"authorId": "2323748082", "name": "Anmol Singh"}, {"authorId": "2372383374", "name": "Sanjay Singh"}, {"authorId": "2372393921", "name": "Ganesh Gupta"}], "abstract": "In this paper, we propose an AI-based mock interview platform addressing effective interviewing techniques with real-time results and intelligent mock interviews for candidates. It is a combination of generative AI, computer vision, and natural language processing (NLP) that provides a simulated experience of real-time interviews. It still continues to use methods such as CNNs for Facial expressions analysis, emotion detection from facial expressions and Voice recognition and NLP for tone, fluency, and confidence detection of a candidate. The platform compares responses with relevant industry standards using keyword-based semantic analysis in order to evaluate domain knowledge. The main highlight of this work includes emotion and answer-based feedback, where it provides the candidate with a performance report and suggestions for improvement. This ensures that users receive dynamic, multimodal insights that are tailored to the user profile (job role and skill set) and go beyond the scope of traditional mock interviews, where all interview questions feel the same. Overall, the platform is designed to help minimize stress, enhance communication capabilities, and empower career progression, while contributing to the evolution of AI in talent development and acquisition."}
{"paperId": "850ac017e0944905fd988b5cbc5de97402f70c0c", "url": "https://www.semanticscholar.org/paper/850ac017e0944905fd988b5cbc5de97402f70c0c", "title": "UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.10342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-12", "authors": [{"authorId": "2291142283", "name": "Jun Yin"}, {"authorId": "2367550951", "name": "Jing Zhong"}, {"authorId": "2302402308", "name": "Peilin Li"}, {"authorId": "2291560239", "name": "Pengyu Zeng"}, {"authorId": "2292088353", "name": "Miao Zhang"}, {"authorId": "2366069714", "name": "Ran Luo"}, {"authorId": "2290813528", "name": "Shuai Lu"}], "abstract": "Urban cultures and architectural styles vary significantly across cities due to geographical, chronological, historical, and socio-political factors. Understanding these differences is essential for anticipating how cities may evolve in the future. As representative cases of historical continuity and modern innovation in China, Beijing and Shenzhen offer valuable perspectives for exploring the transformation of urban streetscapes. However, conventional approaches to urban cultural studies often rely on expert interpretation and historical documentation, which are difficult to standardize across different contexts. To address this, we propose a multimodal research framework based on vision-language models, enabling automated and scalable analysis of urban streetscape style differences. This approach enhances the objectivity and data-driven nature of urban form research. The contributions of this study are as follows: First, we construct UrbanDiffBench, a curated dataset of urban streetscapes containing architectural images from different periods and regions. Second, we develop UrbanSense, the first vision-language-model-based framework for urban streetscape analysis, enabling the quantitative generation and comparison of urban style representations. Third, experimental results show that Over 80% of generated descriptions pass the t-test (p less than 0.05). High Phi scores (0.912 for cities, 0.833 for periods) from subjective evaluations confirm the method's ability to capture subtle stylistic differences. These results highlight the method's potential to quantify and interpret urban style evolution, offering a scientifically grounded lens for future design."}
{"paperId": "857a1d8e76458042a2696bf8d0b4075743a7bd04", "url": "https://www.semanticscholar.org/paper/857a1d8e76458042a2696bf8d0b4075743a7bd04", "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.07307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-10", "authors": [{"authorId": "2277786851", "name": "Haiyang Guo"}, {"authorId": "2349314046", "name": "Fei Zhu"}, {"authorId": "2292223077", "name": "Hongbo Zhao"}, {"authorId": "2320314790", "name": "Fanhu Zeng"}, {"authorId": "2230167644", "name": "Wenzhuo Liu"}, {"authorId": "2223948460", "name": "Shijie Ma"}, {"authorId": "2334898696", "name": "Da-Han Wang"}, {"authorId": "2373752870", "name": "Xu-Yao Zhang"}], "abstract": "Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib."}
{"paperId": "8589be970f54190e91b772c803518d69353dfdb3", "url": "https://www.semanticscholar.org/paper/8589be970f54190e91b772c803518d69353dfdb3", "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2070827752", "name": "Ujjwal Sharma"}, {"authorId": "2815960", "name": "Stevan Rudinac"}, {"authorId": "2390407955", "name": "Ana Mi'ckovi'c"}, {"authorId": "8368098", "name": "W. Dolen"}, {"authorId": "1717056", "name": "M. Worring"}], "abstract": "In this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate messaging on platforms such as X (formerly Twitter), we employ an ensemble of large language models (LLMs) to annotate a large corpus of corporate tweets on their topical alignment with the 17 Sustainable Development Goals (SDGs). This approach avoids the need for costly, task-specific annotations and explores the potential of such models as ad-hoc annotators for social media data that can efficiently capture both explicit and implicit references to sustainability themes in a scalable manner. Complementing this textual analysis, we utilize vision-language models (VLMs), within a visual understanding framework that uses semantic clusters to uncover patterns in visual sustainability communication. This integrated approach reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, environmental, social, governance (ESG) risks, and consumer engagement. Our methods-automatic label generation and semantic visual clustering-are broadly applicable to other domains and offer a flexible framework for large-scale social media analysis."}
{"paperId": "85ed44cd2fadd866447501380558884fb91c1c39", "url": "https://www.semanticscholar.org/paper/85ed44cd2fadd866447501380558884fb91c1c39", "title": "Artificial intelligence affordances for urban mobility", "venue": "Industrial management & data systems", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/imds-09-2024-0878?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/imds-09-2024-0878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-12", "authors": [{"authorId": "2271493963", "name": "Mingye Li"}, {"authorId": "2270588895", "name": "Alemayehu Molla"}, {"authorId": "2288088172", "name": "S. Duan"}], "abstract": "PurposeArtificial intelligence (AI) has been touted as one of the viable solutions to address urban mobility issues. Despite a growing body of research on AI across various sectors, its use in the mobility sector remains underexplored. This study addresses this limitation by investigating AI applications and identifying the AI material properties and use cases that offer mobility-specific affordances.Design/methodology/approachAlthough AI applications in mobility are growing, academic research on the subject has yet to catch up. Therefore, we follow a systematic review and analysis of practitioner literature. We conducted a comprehensive search for relevant documents through Advanced Google and OECD databases and identified 173 sources. We selected 40 sources published between 2015 and 2022 and analysed the corpus of evidence through abductive qualitative analysis technique.FindingsThe analysis reveals that mobility organisations are implementing various AI technologies and systems such as cameras, sensors, IoT, computer vision, natural language processing, robotic process automation, machine learning, deep learning and neural networks. These technologies offer material properties for sensing mobility objects and events, comprehending mobility data, automating mobility activities and learning from mobility data. By exploiting these material properties, mobility organisations are integrating urban mobility management, personalising and automating urban mobility, enabling the smartification of infrastructure and asset management, developing better urban transport planning and management, and enabling automatic driving.Originality/valueThe study contributes a mid-range theory of the affordances of AI for mobility (AI4M) at the infrastructure, operation and service levels. This contribution extends the existing understanding of AI and offers an interconnected perspective of AI affordances for further research. For practitioners, the study provides insights on how to explore AI in alignment with organisational goals to collectively transform urban mobility to be affordable, efficient and sustainable."}
{"paperId": "865ed65a139551cd39a2c8649d765d565c27cc75", "url": "https://www.semanticscholar.org/paper/865ed65a139551cd39a2c8649d765d565c27cc75", "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.14222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-16", "authors": [{"authorId": null, "name": "Xichen Ding"}, {"authorId": null, "name": "Jianzhe Gao"}, {"authorId": null, "name": "Cong Pan"}, {"authorId": "2398976569", "name": "Wenguan Wang"}, {"authorId": null, "name": "Jie Qin"}], "abstract": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component."}
{"paperId": "86d181c0a4f2163573af5fd1eff23dfd4dcf822d", "url": "https://www.semanticscholar.org/paper/86d181c0a4f2163573af5fd1eff23dfd4dcf822d", "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-05", "authors": [{"authorId": "2248946985", "name": "Rui Zhao"}, {"authorId": "2258958963", "name": "Weijia Mao"}, {"authorId": "2344762475", "name": "Mike Zheng Shou"}], "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle."}
{"paperId": "86f8d75709636c23e61430d973a388a38f2e4495", "url": "https://www.semanticscholar.org/paper/86f8d75709636c23e61430d973a388a38f2e4495", "title": "VLONS: A Vision-and-Language On-device Navigation System with Multimodal Fusion and Modular Framework", "venue": "IEEE transactions on consumer electronics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tce.2025.3638139?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tce.2025.3638139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2153118892", "name": "Jianyang Shi"}, {"authorId": "2187723675", "name": "Haijun Zhang"}, {"authorId": "2268743435", "name": "Yuhan Zhang"}, {"authorId": "2296540279", "name": "T. Lam"}, {"authorId": "2395633931", "name": "Lin Zhang"}, {"authorId": "2396717729", "name": "Hu Huang"}, {"authorId": "2382758420", "name": "Yuan Gao"}], "abstract": null}
{"paperId": "874aee0ab46f3a565ef06543d464b8292c814f69", "url": "https://www.semanticscholar.org/paper/874aee0ab46f3a565ef06543d464b8292c814f69", "title": "Improve accuracy in CNNs while using approximate computing methods", "venue": "Journal of Supercomputing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11227-024-06901-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11227-024-06901-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2340771743", "name": "Mohammadreza Rafieinejad"}, {"authorId": "2068016", "name": "Mohammadreza Binesh Marvasti"}, {"authorId": "2262901", "name": "S. A. Asghari"}, {"authorId": "2340764336", "name": "Kimiya Shahbakhti"}], "abstract": null}
{"paperId": "87c2d0718075aef1bcc9095db5851f3e2e73b91e", "url": "https://www.semanticscholar.org/paper/87c2d0718075aef1bcc9095db5851f3e2e73b91e", "title": "Enhancing Cross-Modal Grounding in Vision-and-Language Navigation Through Attention Based Visual Encoder", "venue": "International Conference Intelligent Data Communication Technologies and Internet Things", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICICI65870.2025.11069974?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICICI65870.2025.11069974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-06-04", "authors": [{"authorId": "2279387491", "name": "Ashutosh Pandey"}, {"authorId": "14456434", "name": "A. Parihar"}], "abstract": "Vision-and-Language Navigation (VLN) constitutes a fundamental challenge in autonomous navigation systems, wherein the agent needs to navigate within photorealistic environments guided by natural language instructions and corresponding visual observations. The successful execution of this task critically depends on establishing robust cross-modal grounding between linguistic instructions and visual inputs. This paper presents a systematic investigation into enhancing cross-modal grounding in VLN through the implementation of attention-based visual encoders. Through empirical analysis of convolutional and transformer-based architectures, we demonstrate that attention mechanisms significantly strengthen the semantic alignment between visual and linguistic modalities. Our experimentation on the Room-to-Room (R2R) benchmark dataset reveals that our attention-based approach achieves state-of-the-art performance, yielding a 1% improvement in Success Rate (SR) and a 2.6% enhancement in Success weighted by Path Length (SPL). The results validate the effectiveness of attention-based visual encoders in enhancing cross-modal grounding within VLN tasks."}
{"paperId": "87ecb112a04d21cda9d7b4f367021adca0a939e9", "url": "https://www.semanticscholar.org/paper/87ecb112a04d21cda9d7b4f367021adca0a939e9", "title": "VLN-KHVR: Knowledge-And-History Aware Visual Representation for Continuous Vision-and-Language Navigation", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA55743.2025.11127961?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA55743.2025.11127961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2326495684", "name": "Ping Kong"}, {"authorId": "2304360884", "name": "Ruonan Liu"}, {"authorId": "2387131311", "name": "Zongxia Xie"}, {"authorId": "2361747189", "name": "Zhibo Pang"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate with lowlevel actions following natural language instructions in 3D environments. Most existing approaches utilize observation features from the current step to represent the viewpoint. However, these representations often conflate redundant and essential information for navigation, introducing ambiguity into the agent's action prediction. To address the problem of inadequate representation, we propose a Knowledge-andHistory Aware Visual Representation for Continuous Vision-and-Language Navigation (VLN-KHVR). The proposed approach constructs enriched visual representations tailored to navigation instructions, enhancing agents' navigation performance. Specifically, VLN-KHVR extracts image features from the current observation, retrieves relevant knowledge in the knowledge base, and obtains the history of the navigation episode. Subsequently, the knowledge and history features are filtered to eliminate the information irrelevant to navigation instruction. These refined features are integrated with the instruction for further interaction. Finally, the aggregated features are used to guide navigation. Our model outperforms previous methods on the VLN-CE benchmark, demonstrating the effectiveness of the proposed method."}
{"paperId": "87fbb41c44be6556e3e0755251ce0464641a83b6", "url": "https://www.semanticscholar.org/paper/87fbb41c44be6556e3e0755251ce0464641a83b6", "title": "Surfer: A World Model-Based Framework for Vision-Language Robot Manipulation", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2025.3594117?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2025.3594117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-05", "authors": [{"authorId": "2310610292", "name": "Pengzhen Ren"}, {"authorId": "2316526250", "name": "Kaidong Zhang"}, {"authorId": "2220886900", "name": "Hetao Zheng"}, {"authorId": "2380063278", "name": "Zixuan Li"}, {"authorId": "2153698189", "name": "Yuhang Wen"}, {"authorId": "2266125350", "name": "Fengda Zhu"}, {"authorId": "2220617506", "name": "Shikui Ma"}, {"authorId": "2331834948", "name": "Xiaodan Liang"}], "abstract": "Considering how to make the model accurately understand and follow natural language instructions and perform actions consistent with world knowledge is a key challenge in robot manipulation. This mainly includes human fuzzy instruction reasoning and the following of physical knowledge. Therefore, the embodied intelligence agent must have the ability to model world knowledge from training data. However, most existing vision and language robot manipulation methods mainly operate in less realistic simulators and language settings and lack explicit modeling of world knowledge. To bridge this gap, we introduce a novel and simple robot manipulation framework, called Surfer. It is based on the world model, treats robot manipulation as a state transfer of the visual scene, and decouples it into two parts: action and scene. Then, the generalization ability of the model on new instructions and new scenes is enhanced by explicit modeling of the action and scene prediction in multimodal information. In addition, we built a robot manipulation simulation platform that supports physics execution based on the MuJoCo physics engine. It can automatically generate demonstration training data and test data, effectively reducing labor costs. To conduct a comprehensive and systematic evaluation of the visual-language understanding and physical execution of the manipulation model, we also created a robotic manipulation benchmark with different difficulty levels, called SeaWave. It contains four visual-language manipulation tasks of different difficulty levels and can provide a standardized testing platform for embedded AI agents in multimodal environments. Overall, we hope Surfer can freely surf in the robot’s SeaWave benchmark. Extensive experiments show that Surfer consistently outperforms all baselines significantly in all manipulation tasks. On average, Surfer achieved a success rate of 54.74% on the defined four levels of manipulation tasks, exceeding the best baseline performance of 51.07%. The simulator, code, and benchmarks are released at https://pzhren.github.io/Surfer."}
{"paperId": "8801116b6032cdeaa0ff97203947949804c8066e", "url": "https://www.semanticscholar.org/paper/8801116b6032cdeaa0ff97203947949804c8066e", "title": "FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-22", "authors": [{"authorId": "2308621440", "name": "Jianjian Li"}, {"authorId": "2347452705", "name": "Junquan Fan"}, {"authorId": "2347322796", "name": "Feng Tang"}, {"authorId": "2348066120", "name": "Gang Huang"}, {"authorId": "2347361531", "name": "Shitao Zhu"}, {"authorId": "2258678044", "name": "Songlin Liu"}, {"authorId": "2347348216", "name": "Nian Xie"}, {"authorId": "2347475747", "name": "Wulong Liu"}, {"authorId": "2348708224", "name": "Yong Liao"}], "abstract": "The rapid success of Vision Large Language Models (VLLMs) often depends on the high-resolution images with abundant visual tokens, which hinders training and deployment efficiency. Current training-free visual token compression methods exhibit serious performance degradation in tasks involving high-resolution, text-oriented image understanding and reasoning. In this paper, we propose an efficient visual token compression framework for text-oriented VLLMs in high-resolution scenarios. In particular, we employ a light-weight self-distillation pre-training stage to compress the visual tokens, requiring a limited numbers of image-text pairs and minimal learnable parameters. Afterwards, to mitigate potential performance degradation of token-compressed models, we construct a high-quality post-train stage. To validate the effectiveness of our method, we apply it to an advanced VLLMs, InternVL2. Experimental results show that our approach significantly reduces computational overhead while outperforming the baselines across a range of text-oriented benchmarks. We will release the models and code soon."}
{"paperId": "88193df4b43406a204698e72330aec62690fe505", "url": "https://www.semanticscholar.org/paper/88193df4b43406a204698e72330aec62690fe505", "title": "HeadArtist-VL: Vision / Language Guided 3D Head Generation with Self Score Distillation.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2025.3619576?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2025.3619576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-10", "authors": [{"authorId": "2273469505", "name": "Hongyu Liu"}, {"authorId": "2273705061", "name": "Xuan Wang"}, {"authorId": "2351929018", "name": "Ziyu Wan"}, {"authorId": "2273332687", "name": "Yujun Shen"}, {"authorId": "2244782194", "name": "Yibing Song"}, {"authorId": "2273664789", "name": "Jing Liao"}, {"authorId": "2244781221", "name": "Qifeng Chen"}], "abstract": "We present HeadArtist-VL, a 3D head generation method that suits either vision or language input. With a landmark-guided ControlNet serving as a generative prior, we come up with an efficient pipeline that optimizes a parameterized 3D head model under the supervision of the prior distillation itself. We name such a process self-score distillation (SSD). In detail, given a sampled camera pose, we first render an image and its corresponding landmarks from the head model, and add some particular level of noise onto the image. When the input is a language prompt, we fed the noisy image, landmarks, and the language prompt into a frozen ControlNet twice for noise prediction. We conduct two predictions via the same ControlNet structure but with different classifier-free guidance (CFG) weights. The difference between these two predicted results directs how the rendered image can better match the language instructions. When the input is a reference image, we follow the aforementioned pipeline but with two modifications. First, we use an image encoder to obtain the image identity embedding, which is then sent to the ControlNet. Second, we use a novel-view diffusion model to synthesize the same reference image under the sampled camera pose to guide the self-score distillation process. In the experiments, our HeadArtist-VL produces high-quality 3D head sculptures with rich geometry and photo-realistic appearance, which significantly outperforms state-of-the-art methods. We also show that our method supports editing operations on the generated heads, including both geometry deformation and appearance change. 3D Head Generation and Editing, Vision / Language Guidance, Self Score Distillation."}
{"paperId": "887632ce16851ce4a529b2534601d08eb7c884c0", "url": "https://www.semanticscholar.org/paper/887632ce16851ce4a529b2534601d08eb7c884c0", "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.22146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-28", "authors": [{"authorId": "2159224727", "name": "Guangfu Hao"}, {"authorId": "2307463612", "name": "Haojie Wen"}, {"authorId": "2375341406", "name": "Liang Guo"}, {"authorId": "2144353903", "name": "Yang Chen"}, {"authorId": "2363798027", "name": "Yanchao Bi"}, {"authorId": "2344760314", "name": "Shan Yu"}], "abstract": "Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Human evaluation studies validate our framework's alignment with human decision-making patterns, and generalization experiments demonstrate effective performance on novel tool categories. Ablation studies revealed that manipulation-related attributes (graspability, elongation, hand-relatedness) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks."}
{"paperId": "888df1d9ef7d900adb262fa07c647d6df23da0af", "url": "https://www.semanticscholar.org/paper/888df1d9ef7d900adb262fa07c647d6df23da0af", "title": "INTRODUZIONE ALLA LINGUA INGLESE FIN DALLA PRIMA INFANZIA: UN APPROCCIO BASATO SULL’USO “AMPLIFICATO” DEL CANALE VISIVO-GESTUALE", "venue": "Italiano LinguaDue", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.54103/2037-3597/27852", "status": "GOLD", "license": "CCBYSA", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54103/2037-3597/27852?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54103/2037-3597/27852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-02", "authors": [{"authorId": "2022673113", "name": "Erika Raniolo"}], "abstract": "Ad oggi in Italia il contatto con la lingua inglese fin dalla prima infanzia è piuttosto diffuso, sebbene la normativa lo renda obbligatorio solo a partire dalla scuola primaria; diverse strategie didattiche (in relazione ai diversi ordini e gradi scolastici) sono state definite. Lo studio propone un approccio basato su un ricorso “amplificato” al canale visivo-gestuale, canale presente in tutte le lingue in virtù della multimodalità. Il punto di partenza è il concetto di utterance visible actiondi Kendon, che elimina la netta differenziazione fra gesto e segno: data dunque la continuità tra gesti coverbali e segni delle lingue dei segni, con particolare riferimento alla LIS (Lingua dei Segni Italiana), vengono prese in considerazione le potenzialità glottodidattiche dell’accostare il gesto/segno alla parola in inglese, sfruttando l’iconicità e l’embodied cognition. L’approccio è stato sperimentato all’interno di un progetto condotto in provincia di Ragusa nell’a.s. 2022/2023, con destinatari di età compresa fra 3 mesi e 8 anni. Le attività proposte mostrano un miglioramento nella comprensione e produzione della lingua inglese. Il modello si pone come un approccio inclusivo e integrato, in linea con una visione gestaltica dell’educazione linguistica. \n  \nAn approach based on the ‘amplified’ use of the visual-gestural channel \nTo date, in Italy, contact with the English language from early childhood is quite widespread, although the legislation makes it obligatory only starting from primary school. Various didactic strategies, in relation to the different school levels, have been defined. \nThe study proposes an approach based on an ‘amplified’ use of the visual-gestural channel, which can be found in all languages ​​thanks to multimodality. The starting point is Kendon’s concept of utterance visible action, which eliminates the clear differentiation between gesture and sign. Given the continuity between coverbal gestures and signs of sign languages, with particular reference to LIS (Italian Sign Language), the glottodidactic potential of combining the gesture/sign with the word in English, exploiting iconicity and embodied cognition, is taken into consideration. The approach was tested within a project conducted in the province of Ragusa in the s.y. 2022/2023, with participants aged between 3 months and 8 years. The proposed activities show an improvement in understanding and producing the English language. The model turns out to be an inclusive and integrated approach, in line with a Gestalt vision of language education. \n "}
{"paperId": "88e7d8cd8093c730a9584f8eaa320de69b1c1237", "url": "https://www.semanticscholar.org/paper/88e7d8cd8093c730a9584f8eaa320de69b1c1237", "title": "SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52734.2025.00892?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52734.2025.00892, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-10", "authors": [{"authorId": "2351810939", "name": "Mingjin Zhang"}, {"authorId": "2356679315", "name": "Xiaolong Li"}, {"authorId": "2358813245", "name": "Fei Gao"}, {"authorId": "1492038265", "name": "Jie-Ru Guo"}, {"authorId": "2281910078", "name": "Xinbo Gao"}, {"authorId": "2352221169", "name": "Jing Zhang"}], "abstract": "Infrared Small Target Detection (IRSTD) aims to identify low signal-to-noise ratio small targets in infrared images with complex backgrounds, which is crucial for various applications. However, existing IRSTD methods typically rely solely on image modalities for processing, which fail to fully capture contextual information, leading to limited detection accuracy and adaptability in complex environments. Inspired by vision-language models, this paper proposes a novel framework, SAIST, which integrates textual information with image modalities to enhance IRSTD performance. The framework consists of two main components: Scene Recognition Contrastive Language-Image Pretraining (SR-CLIP) and CLIP-guided Segment Anything Model (CG-SAM). SR-CLIP generates a set of visual descriptions through object-object similarity and object-scene relevance, embedding them into learnable prompts to refine the textual description set. This reduces the domain gap between vision and language, generating precise textual and visual prompts. CG-SAM utilizes the prompts generated by SR-CLIP to accurately guide the Mask Decoder in learning prior knowledge of background features, while incorporating infrared imaging equations to improve small target recognition in complex backgrounds and significantly reduce the false alarm rate. Additionally, this paper introduces the first multimodal IRSTD dataset, MIRSTD, which contains abundant image-text pairs. Experimental results demonstrate that the proposed SAIST method outperforms existing state-of-the-art approaches."}
{"paperId": "89251aaa1f52ee68b851916730defd6a878a7ca2", "url": "https://www.semanticscholar.org/paper/89251aaa1f52ee68b851916730defd6a878a7ca2", "title": "جمشید کمبوہ کی شاعری کا فکری جائزہ", "venue": "Al-Aasar", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63878/aaj538?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63878/aaj538, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-05", "authors": [{"authorId": "2370708029", "name": "ارم سعید"}, {"authorId": "2370704127", "name": "حسنین حیدر"}], "abstract": "This article offers an intellectual critique of Jamshed Kamboh’s poetry, discussing the theme concerns, literary features, and style aspects that define his work. Through critical examination of Jamshed Kamboh’s poems, this research reveals the poet’s distinctive viewpoint on life, love, and social matters. The article discusses how Jamshed Kamboh’s poetry portrays his experiences, emotions, and observations and how his work fits into the literary scenario of Urdu poetry. This article presents an insightful view of Jamshed Kamboh’s poetic vision, his language, and how his work influences readers. Through the analytical and creative aspects of Jamshed Kamboh’s poetry, this review attempts to highlight the importance of his work on Urdu literature."}
{"paperId": "892b0dd001c2ffe60be9d2b60249f1fa2b2f4c10", "url": "https://www.semanticscholar.org/paper/892b0dd001c2ffe60be9d2b60249f1fa2b2f4c10", "title": "Medial Temporal Default Mode Network Selectively Encodes Autobiographical Visual Imagery", "venue": "bioRxiv", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.11.25.690576?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.11.25.690576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-28", "authors": [{"authorId": "2398744561", "name": "Andrew J. Anderson"}, {"authorId": "2280317128", "name": "Adam Turnbull"}, {"authorId": "2395589111", "name": "Feng V. Lin"}], "abstract": null}
{"paperId": "893c97fd59dabd7d2286203a52980b2bae7c7920", "url": "https://www.semanticscholar.org/paper/893c97fd59dabd7d2286203a52980b2bae7c7920", "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-19", "authors": [{"authorId": "2313463499", "name": "Yang Liu"}, {"authorId": "2114323920", "name": "Wentao Feng"}, {"authorId": "2351001060", "name": "Zhuoyao Liu"}, {"authorId": "2237407099", "name": "Shudong Huang"}, {"authorId": "2293779369", "name": "Jiancheng Lv"}], "abstract": "Enabling Visual Semantic Models to effectively handle multi-view description matching has been a longstanding challenge. Existing methods typically learn a set of embeddings to find the optimal match for each view's text and compute similarity. However, the visual and text embeddings learned through these approaches have limited information capacity and are prone to interference from locally similar negative samples. To address this issue, we argue that the information capacity of embeddings is crucial and propose Dense-to-Sparse Feature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the information capacity of sparse text by leveraging dense text distillation. Specifically, D2S-VSE is a two-stage framework. In the pre-training stage, we align images with dense text to enhance the information capacity of visual semantic embeddings. In the fine-tuning stage, we optimize two tasks simultaneously, distilling dense text embeddings to sparse text embeddings while aligning images and sparse texts, enhancing the information capacity of sparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority over recent state-of-the-art methods."}
{"paperId": "89c1c97adebeda844fe2141d32abc212acff8aae", "url": "https://www.semanticscholar.org/paper/89c1c97adebeda844fe2141d32abc212acff8aae", "title": "Advancing reliability in self-supervised transformer models through hierarchical mask attention heads", "venue": "Medical Imaging", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3047444?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3047444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-04-11", "authors": [{"authorId": "2355133355", "name": "Simon Baur"}, {"authorId": "2234345330", "name": "Amirhossein Vahidi"}, {"authorId": "2351496209", "name": "Mengyu Wang"}, {"authorId": "6339203", "name": "Nazlee Zebardast"}, {"authorId": "2237966813", "name": "Tobias Elze"}, {"authorId": "2299307802", "name": "Bernd Bischl"}, {"authorId": "2237992768", "name": "Mina Rezaei"}, {"authorId": "2066389123", "name": "Mohammad Eslami"}], "abstract": "Self-supervised learning (SSL) has proven to be a powerful technique across various domains, including computer vision, natural language processing, and, more recently, medical image analysis. In critical applications such as medical diagnosis and clinical decision-making, understanding a model’s predictive accuracy and confidence is essential for building trustworthy and reliable machine learning systems. However, despite the rapid advancements in SSL, few studies have focused on assessing or enhancing the reliability of these models. To address this gap, we build on Plex’s definition of reliability, which emphasizes robust generalization to new tasks, adaptability to new datasets, and accurate representation of uncertainty. We propose a simple yet effective technique to improve the reliability of SSL models by introducing randomness into self-supervised transformers while maintaining their accuracy. Our approach involves training a hierarchical mask on the multi-headed attention mechanism, a key component of transformer models, and implementing a masking scheduler to adjust the masking portion dynamically during training. Through extensive experiments on diverse tasks, including in-distribution generalization, out-of-distribution generalization, semi-supervised learning, and transfer learning, we demonstrate that our method enhances prediction reliability. Using chest X-ray and ophthalmic fundus datasets such as CheXpert, ChestX-ray14, EyePACS, and APTOS, we validate our approach on chest X-ray images and retinal color fundus photos, achieving improved calibration and accuracy compared to baseline models. Our method performs on par with ensemble techniques, offering a scalable and effective solution for building more robust and trustworthy SSL models in medical and clinical applications."}
{"paperId": "89c4124410976bc2473690eb5b54074d28668096", "url": "https://www.semanticscholar.org/paper/89c4124410976bc2473690eb5b54074d28668096", "title": "Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-09", "authors": [{"authorId": "2355355508", "name": "Luo Ling"}, {"authorId": "2355351598", "name": "Qianqian Bai"}], "abstract": "Enhancing the spatial perception capabilities of mobile robots is crucial for achieving embodied Vision-and-Language Navigation (VLN). Although significant progress has been made in simulated environments, directly transferring these capabilities to real-world scenarios often results in severe hallucination phenomena, causing robots to lose effective spatial awareness. To address this issue, we propose BrainNav, a bio-inspired spatial cognitive navigation framework inspired by biological spatial cognition theories and cognitive map theory. BrainNav integrates dual-map (coordinate map and topological map) and dual-orientation (relative orientation and absolute orientation) strategies, enabling real-time navigation through dynamic scene capture and path planning. Its five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine, Parietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion Execution Unit-mimic biological cognitive functions to reduce spatial hallucinations and enhance adaptability. Validated in a zero-shot real-world lab environment using the Limo Pro robot, BrainNav, compatible with GPT-4, outperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in Continuous Environments (VLN-CE) methods without fine-tuning."}
{"paperId": "89d3ffd6dd96b58c4e179918efc522197b97cff2", "url": "https://www.semanticscholar.org/paper/89d3ffd6dd96b58c4e179918efc522197b97cff2", "title": "CMA-VC: Large Vision-Language Model for Cross-Modal Alignment in Intention-Oriented Video Captioning", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3762060?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3762060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2258009330", "name": "Jun Yu"}, {"authorId": "2349955670", "name": "Xilong Lu"}, {"authorId": "2305608536", "name": "Yunxiang Zhang"}, {"authorId": "2180427566", "name": "Qiang Ling"}], "abstract": "Traditional video captioning methods often produce generic descriptions that fail to align with specific user intentions, limiting their applicability in scenarios requiring customized information extraction. This paper proposes a novel intention-oriented controllable video captioning approach, which leverages large-scale vision-language models (InternViT and InternLM) and achieves parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA). The proposed framework processes both video content and user-specified intentions via a unified cross-modal pipeline, dynamically aligning visual features with intent semantics to generate focused and contextually accurate captions. Experiments on the IntentVC dataset validate the effectiveness of the proposed method in generating intention-aligned captions, with the following performance metrics: BLEU@4 scores 44.38 on the public test set and 40.21 on the private test set; METEOR scores 63.79 and 60.07 respectively; CIDEr scores 230.33 and 208.15 respectively; ROUGE-L scores 61.75 and 57.14 respectively. Ablation studies confirm the significant effectiveness of joint LoRA adaptation on vision and language modules, as well as the sensitivity of performance to LoRA parameters. This work advances the field by enabling precise control over caption generation, enhancing the practical utility of video understanding systems in applications such as accessibility services and targeted video retrieval."}
{"paperId": "8a098add83a7dd7e2198fe16de742f57e77215cb", "url": "https://www.semanticscholar.org/paper/8a098add83a7dd7e2198fe16de742f57e77215cb", "title": "Recurrent Diffusion for Large-Scale Parameter Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.11587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-20", "authors": [{"authorId": "2314739123", "name": "Kai Wang"}, {"authorId": "2314775864", "name": "Dongwen Tang"}, {"authorId": "2292217857", "name": "Wangbo Zhao"}, {"authorId": "2314810632", "name": "Yang You"}], "abstract": "Parameter generation has long struggled to match the scale of today large vision and language models, curbing its broader utility. In this paper, we introduce Recurrent Diffusion for Large Scale Parameter Generation (RPG), a novel framework that generates full neural network parameters up to hundreds of millions on a single GPU. Our approach first partitions a networks parameters into non-overlapping tokens, each corresponding to a distinct portion of the model. A recurrent mechanism then learns the inter token relationships, producing prototypes which serve as conditions for a diffusion process that ultimately synthesizes the full parameters. Across a spectrum of architectures and tasks including ResNets, ConvNeXts and ViTs on ImageNet 1K and COCO, and even LoRA based LLMs RPG achieves performance on par with fully trained networks while avoiding excessive memory overhead. Notably, it generalizes beyond its training set to generate valid parameters for previously unseen tasks, highlighting its flexibility in dynamic and open ended scenarios. By overcoming the longstanding memory and scalability barriers, RPG serves as a critical advance in AI generating AI, potentially enabling efficient weight generation at scales previously deemed infeasible."}
{"paperId": "8a64e728cb87dc59f798e322d83ad1447f41b834", "url": "https://www.semanticscholar.org/paper/8a64e728cb87dc59f798e322d83ad1447f41b834", "title": "Impact of Layer Norm on Memorization and Generalization in Transformers", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.10566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-13", "authors": [{"authorId": "2209147909", "name": "Rishi Singhal"}, {"authorId": "2392337012", "name": "Jung-Eun Kim"}], "abstract": "Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers."}
{"paperId": "8a89a1a5166a16e55e224621d8eef957c66a84f1", "url": "https://www.semanticscholar.org/paper/8a89a1a5166a16e55e224621d8eef957c66a84f1", "title": "Metodologias ativas e sua importância para a inovação do ensino contribuições para o ensino de Língua Portuguesa em STP", "venue": "Revista Sêbê Non Linguagens", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21747/30517036/seba1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21747/30517036/seba1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2383356584", "name": "Jelson da Trindade"}, {"authorId": "119867827", "name": "Jovania Maria Perin Santos"}], "abstract": "This article aims to reflect on traditional or passive methodologies and active methodologies in Portuguese language teaching. Initially, we describe some common directions of traditional or passive methodological practices, which are often very common in public schools in São Tomé and Príncipe (STP) according to reports from students of the Portuguese Language Degree course, many of whom are already teachers. In addition, we have made observations in the teaching manuals and lesson plans used. Inorder to contribute to the advancement and updating of pedagogical practices in the area, we provide a description of active methodologies based on works by Silva (2021), Pinheiro (2021) and Moran (2018). Throughout this study, we argue in favor of active methodologies, especially in the context of teaching grammatical topics, our main interest in this work. To this end, we show a lesson plan prepared for teaching in STP in 2024 and, as a reference, we provide a lesson plan based on innovative methodologies. All these reflections are based on analyses and discussions held during undergraduate classes. As a result of these analyses, we share a dynamic teaching material with third-year students, designed from the active perspective of teaching Portuguese and following the vision of language as a social practice. With this proposal, we believe we can contribute to improving pedagogical practices in the country, to the development of teaching materials and to the training of Portuguese language teachers."}
{"paperId": "8aad05e5086a04cef872924d25964606f3156242", "url": "https://www.semanticscholar.org/paper/8aad05e5086a04cef872924d25964606f3156242", "title": "VLIPP: Towards Physically Plausible Video Generation with Vision and Language Informed Physical Prior", "venue": "", "year": 2025, "citationCount": 13, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-03-30", "authors": [{"authorId": "2353270224", "name": "Xindi Yang"}, {"authorId": "2298255478", "name": "Baolu Li"}, {"authorId": "2352985026", "name": "Yiming Zhang"}, {"authorId": "2336032911", "name": "Zhenfei Yin"}, {"authorId": "2238207972", "name": "Lei Bai"}, {"authorId": "2281794403", "name": "Liqian Ma"}, {"authorId": "2352986794", "name": "Zhiyong Wang"}, {"authorId": "2354295763", "name": "Jianfei Cai"}, {"authorId": "2352937138", "name": "Tien-Tsin Wong"}, {"authorId": "2249852347", "name": "Huchuan Lu"}, {"authorId": "2269431730", "name": "Xu Jia"}], "abstract": "Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics with vision and language informed physical prior. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation."}
{"paperId": "8acee0d59b543d25e015cd8879ece31e59bb3fe7", "url": "https://www.semanticscholar.org/paper/8acee0d59b543d25e015cd8879ece31e59bb3fe7", "title": "Optimizing AI Model Inference Performance with Dynamic Profiling", "venue": "International Journal of Science and Research Archive", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30574/ijsra.2025.16.1.2066?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30574/ijsra.2025.16.1.2066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-30", "authors": [{"authorId": "2377460759", "name": "Ankush Jitendrakumar Tyagi"}], "abstract": "Deep neural networks and Artificial Intelligence (AI) models have shown great success in areas that include computer vision, natural language processing, and autonomous systems. Yet, their application in real-world tasks is typically limited by inference performance drawbacks, in particular, when the specialized cutting-edge devices are needed to complete such tasks in real time and on resource-constrained devices. The main issue with the requirement to scale, efficient, and responsive AI systems is the key attention paid to the inference performance optimisation. Dynamic profiling, or the process of analysing AI models and system performance in real-time as they execute, has become a critical technique not only as a means to detect locations where performance is impeded but to inform the process of performance optimisation at runtime. In contrast to static profiling which performs an analysis before execution of specific and prepared traces of the executable (static profiling uses the pre-execution information about the program to perform an analysis of it), dynamic profiling allows a more dynamic and fine-grained inspection of problems like inefficiencies in memory access, imbalances in compute utilisation, layer-resolution latency, and power consumption. Dynamic performance tracing, profiling, and tools and frameworks such as TensorRT, Intel VTune, NVIDIA Nsight, and PyTorch Profiler enjoy wide support across the diverse hardware platforms, including CPU, GPU, and edge accelerator, with full support across platforms. These tools can offer useful information to guide fine-grained optimisations like operator fusion, quantisation, memory and computation scheduling, and replication strategies. Notably, with the seamless coupling of dynamic profiling to automated deployment pipelines, AI systems can dynamically optimise themselves at runtime and respond well to variations in workloads and system constraints. It helps achieve intelligent self-optimising AI applications that are also able to be kept at production level performance. As dynamic profiling is integrated into the AI model lifecycle, it allows continuous performance tracking and a sign-and-iterate cycle, hence facilitating the delivery of scalable, energy-efficient, and high-throughput AI approaches at scale in the cloud as well as at the edge. This paper will demonstrate that dynamic profiling is a very important technique to overcome the performance issues and drive the best future of AI deployment."}
{"paperId": "8aea8045179efc64752eb3f43872d179082e9451", "url": "https://www.semanticscholar.org/paper/8aea8045179efc64752eb3f43872d179082e9451", "title": "IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.03469, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-05", "authors": [{"authorId": "2109723855", "name": "Jiabing Yang"}, {"authorId": "2249763944", "name": "Chenhang Cui"}, {"authorId": "2250759613", "name": "Yiyang Zhou"}, {"authorId": "2366155958", "name": "Yixiang Chen"}, {"authorId": "2261083308", "name": "Peng Xia"}, {"authorId": "2375098604", "name": "Ying Wei"}, {"authorId": "2347681880", "name": "Tao Yu"}, {"authorId": "2375031786", "name": "Yan Huang"}, {"authorId": "2317093163", "name": "Liang Wang"}], "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress across multiple domains. However, these models still face the inherent challenge of integrating vision and language for collaborative inference, which often leads to\"hallucinations\", outputs that are not grounded in the corresponding images. Many efforts have been made to address these issues, but each comes with its own limitations, such as high computational cost or expensive dataset annotation. Recent research shows that LVLMs exhibit a long-term bias where hallucinations increase as the sequence length grows, yet the underlying cause remains poorly understood. Building on extensive research into attention mechanisms in LVLMs, we analyze the relationship between this long-term bias and visual attention. In our research, we identify a consistent phenomenon in current LVLMs: the model's attention to visual input diminishes as the generated sequence grows, which we hypothesize to be a key factor contributing to observed increasing hallucinations. Based on these insights, we propose Image attention-guided Key-value merging cOllaborative Decoding (IKOD), a collaborative decoding strategy generating more image-focused sequences. This method derives logits from shorter sequences with higher image attention through key-value merging and combines them with those from the original decoding, effectively mitigating attention degradation and suppressing hallucinations while not incurring too much inference cost. Extensive experiments on both hallucination and comprehensive benchmarks demonstrate IKOD's superior effectiveness in mitigating hallucinations and improving comprehensive capacities for LVLMs. Importantly, IKOD requires no additional training or external tools, making it a lightweight and efficient framework applicable to various models."}
{"paperId": "8b161a5201dbf018a8b539a58624f91e5a58d5b2", "url": "https://www.semanticscholar.org/paper/8b161a5201dbf018a8b539a58624f91e5a58d5b2", "title": "Scale-wise Bidirectional Alignment Network for Referring Remote Sensing Image Segmentation", "venue": "Isprs Journal of Photogrammetry and Remote Sensing", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.00851, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2258672998", "name": "Kun Li"}, {"authorId": "144289450", "name": "G. Vosselman"}, {"authorId": "2258604024", "name": "Michael Ying Yang"}], "abstract": "The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication."}
{"paperId": "8b16ff16ab9fe68cc288987d7ccb532d6f919818", "url": "https://www.semanticscholar.org/paper/8b16ff16ab9fe68cc288987d7ccb532d6f919818", "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.19288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-23", "authors": [{"authorId": "2125967932", "name": "Runwei Guan"}, {"authorId": "2302400826", "name": "Ningwei Ouyang"}, {"authorId": "2371105488", "name": "Tianhao Xu"}, {"authorId": "2350448137", "name": "Shaofeng Liang"}, {"authorId": "2371188026", "name": "Wei Dai"}, {"authorId": "2370957516", "name": "Yafeng Sun"}, {"authorId": "2348790763", "name": "Shang Gao"}, {"authorId": "2319829684", "name": "Songning Lai"}, {"authorId": "2126011", "name": "Shanliang Yao"}, {"authorId": "2328021149", "name": "Xuming Hu"}, {"authorId": "2370956945", "name": "Ryan Wen Liu"}, {"authorId": "2348516172", "name": "Yutao Yue"}, {"authorId": "2302402345", "name": "Hui Xiong"}], "abstract": "Automated waterway environment perception is crucial for enabling unmanned surface vessels (USVs) to understand their surroundings and make informed decisions. Most existing waterway perception models primarily focus on instance-level object perception paradigms (e.g., detection, segmentation). However, due to the complexity of waterway environments, current perception datasets and models fail to achieve global semantic understanding of waterways, limiting large-scale monitoring and structured log generation. With the advancement of vision-language models (VLMs), we leverage image captioning to introduce WaterCaption, the first captioning dataset specifically designed for waterway environments. WaterCaption focuses on fine-grained, multi-region long-text descriptions, providing a new research direction for visual geo-understanding and spatial scene cognition. Exactly, it includes 20.2k image-text pair data with 1.8 million vocabulary size. Additionally, we propose Da Yu, an edge-deployable multi-modal large language model for USVs, where we propose a novel vision-to-language projector called Nano Transformer Adaptor (NTA). NTA effectively balances computational efficiency with the capacity for both global and fine-grained local modeling of visual features, thereby significantly enhancing the model's ability to generate long-form textual outputs. Da Yu achieves an optimal balance between performance and efficiency, surpassing state-of-the-art models on WaterCaption and several other captioning benchmarks."}
{"paperId": "8b272e4c137dff9ef3f75770290bd4e4c1dc3dd3", "url": "https://www.semanticscholar.org/paper/8b272e4c137dff9ef3f75770290bd4e4c1dc3dd3", "title": "Multi-Model Workflows for Advanced Visual Understanding", "venue": "The Web Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3701716.3715188?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3701716.3715188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-05-08", "authors": [{"authorId": "2282467411", "name": "Arun George Zachariah"}, {"authorId": "2362919603", "name": "Varun Praveen"}, {"authorId": "2362921341", "name": "Samuel Ochoa"}, {"authorId": "2362926254", "name": "Parthasarathy Sriram"}], "abstract": "This paper introduces a novel task-agnostic system for visual question answering, designed to address the limitations of existing visual-language models in scenarios requiring fine-grained reasoning and contextual understanding. By integrating computer vision, natural language processing, and retrieval-augmented generation, the system dynamically selects models and interprets diverse visual inputs without necessitating task-specific retraining. It leverages scene graph extraction and a model multiplexer to process visual content contextually, while an instruction execution engine transforms multimodal data into actionable insights. The system emphasizes explainability through annotated outputs and textual responses, enhancing user trust and comprehension. Its modular design ensures scalability and adaptability, enabling seamless incorporation of new models and datasets for evolving applications. This work represents a significant step forward in visual question answering, offering a transparent and versatile solution tailored to real-world complexities. The complete source code can be found at https://github.com/NVIDIA/Multi-Model-Workflows."}
{"paperId": "8b61da70a828d3d83e0b6ce3174a960b8666ee8d", "url": "https://www.semanticscholar.org/paper/8b61da70a828d3d83e0b6ce3174a960b8666ee8d", "title": "Cross-domain few-shot object detection in infrared images using prompt tuning for vision and language models", "venue": "Defense + Commercial Sensing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.3054773?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.3054773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-05-29", "authors": [{"authorId": "2313102398", "name": "Shotaro Miwa"}, {"authorId": "2305408625", "name": "Shun Otsubo"}, {"authorId": "2360671229", "name": "Qu Jia"}, {"authorId": "2221719167", "name": "Yasuaki Susumu"}], "abstract": "Traditionally, computer vision systems, like object detection, primarily relied on supervised learning and predetermined object categories. However, this approach’s limitations in terms of generality and the need for additional labeled data are more pronounced for infrared images due to the difficulty of obtaining training datasets. In contrast, the rise of contrastive vision-language models, such as CLIP, has transformed the field. These models, pre-trained on vast image-text pairs, offer more versatile visual representations aligned with rich language semantics. CLIP’s feature transferability has become a foundation for various visible image tasks. This paper introduces cross-domain few-shot object detection for infrared images using prompt tuning for vision and language models, extending CLIP’s benefits in visible images to infrared images. We boosted the performance of pre-trained Vision-Language models across different domains by prompt tuning techniques. Experimental results show the promise of this approach, and the paper initiates a preliminary exploration of domain shift issues between infrared and visible images."}
{"paperId": "8bcdf68a8933043ab31d95b9d318395586806ff1", "url": "https://www.semanticscholar.org/paper/8bcdf68a8933043ab31d95b9d318395586806ff1", "title": "The time course of visuo-semantic representations in the human brain is captured by combining vision and language models", "venue": "", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.19497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-06-24", "authors": [{"authorId": "2339442746", "name": "Boyan Rong"}, {"authorId": "2159349255", "name": "A. T. Gifford"}, {"authorId": "152257776", "name": "E. Duzel"}, {"authorId": "2270319460", "name": "R. M. Cichy"}], "abstract": "The human visual system provides us with a rich and meaningful percept of the world, transforming retinal signals into visuo-semantic representations. For a model of these representations, here we leveraged a combination of two currently dominating approaches: vision deep neural networks (DNNs) and large language models (LLMs). Using large-scale human electroencephalography (EEG) data recorded during object image viewing, we built encoding models to predict EEG responses using representations from a vision DNN, an LLM, and their fusion. We show that the fusion encoding model outperforms encoding models based on either the vision DNN or the LLM alone, as well as previous modelling approaches, in predicting neural responses to visual stimulation. The vision DNN and the LLM complemented each other in explaining stimulus-related signal in the EEG responses. The vision DNN uniquely captured earlier and broadband EEG signals, whereas the LLM uniquely captured later and low frequency signals, as well as detailed visuo-semantic stimulus information. Together, this provides a more accurate model of the time course of visuo-semantic processing in the human brain."}
{"paperId": "8bfcebb45760d2df6d50fec14335e28ef5942a5e", "url": "https://www.semanticscholar.org/paper/8bfcebb45760d2df6d50fec14335e28ef5942a5e", "title": "Differential Gated Self-Attention", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-29", "authors": [{"authorId": "2308271575", "name": "Elpiniki Maria Lygizou"}, {"authorId": "2279753479", "name": "M'onika Farsang"}, {"authorId": "1787208", "name": "R. Grosu"}], "abstract": "Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self-attention treats all query-key interactions uniformly. Inspired by lateral inhibition in biological neural circuits and building on the recent use by the Differential Transformer's use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self-attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast-enhancement and self-attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability."}
{"paperId": "8c0ebb1aa0e3c16f074ecf820c67af0cfd8ae187", "url": "https://www.semanticscholar.org/paper/8c0ebb1aa0e3c16f074ecf820c67af0cfd8ae187", "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.19514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-18", "authors": [{"authorId": "2316431606", "name": "Andrew Kiruluta"}], "abstract": "We introduce a fully spectral learning framework that eliminates traditional neural layers by operating entirely in the wavelet domain. The model applies learnable nonlinear transformations, including soft-thresholding and gain-phase modulation, directly to wavelet coefficients. It also includes a differentiable wavelet basis selection mechanism, enabling adaptive processing using families such as Haar, Daubechies, and Biorthogonal wavelets. Implemented in PyTorch with full 3D support, the model maintains a spectral pipeline without spatial convolutions or attention. On synthetic 3D denoising and natural language tasks from the GLUE benchmark, including SST-2 sentiment classification, the model achieves 89.3 percent accuracy, close to a 4-layer Transformer baseline (90.1 percent), while using 72 percent fewer parameters and 58 percent less peak memory. Faster early convergence is observed due to spectral sparsity priors. In contrast to the quadratic complexity of self-attention and large matrix multiplications in Transformers, our approach uses linear-time wavelet transforms and pointwise nonlinearities, significantly reducing inference cost. This yields a compact, interpretable, and efficient alternative to neural models. Our results support the viability of principled spectral learning in both vision and language tasks, offering new directions for model design without overparameterized architectures."}
{"paperId": "8c33cf149431c8665ed59bcf3cfa30d19d673968", "url": "https://www.semanticscholar.org/paper/8c33cf149431c8665ed59bcf3cfa30d19d673968", "title": "The Evolution of Deep Learning: Models, Applications, and Future Directions", "venue": "2025 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/MIUCC66482.2025.11196834?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/MIUCC66482.2025.11196834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference", "Review"], "publicationDate": "2025-09-17", "authors": [{"authorId": "2382606291", "name": "Ayman Elshenawy"}, {"authorId": "2387226001", "name": "Ayman Mohammed"}, {"authorId": "2387002868", "name": "Saeed Hamouda"}], "abstract": "This paper presents a comprehensive survey of deep learning (DL) models, systematically categorizing them from foundational architectures such as multilayer perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs) to advanced frameworks like transformers, generative adversarial networks (GANs), large language models (LLMs), and large multimodal models (LMMs). The survey reviews core mechanisms, architectures, and applications across domains, including computer vision, natural language processing, healthcare, and robotics. It highlights emerging trends such as efficient model design, transfer learning, and multimodal integration while addressing key challenges including interpretability, scalability, generalization, and robustness. By critically analyzing the strengths, limitations, and future directions of DL, this work serves as a comprehensive reference for researchers and practitioners seeking to navigate the evolving deep learning landscape."}
{"paperId": "8c51485fd131518785d4a3d60df560cd2b7cc89a", "url": "https://www.semanticscholar.org/paper/8c51485fd131518785d4a3d60df560cd2b7cc89a", "title": "Acquisition and Utilization of Recursive Rules in Motor Sequence Generation", "venue": "Cognitive Sciences", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": "CCBYNC", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12404565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2379988294", "name": "Mauricio J. D. Martins"}, {"authorId": "2378728856", "name": "Zoe Bergmann"}, {"authorId": "2378738019", "name": "Elena Leonova"}, {"authorId": "39844729", "name": "Roberta Bianco"}, {"authorId": "2270411", "name": "D. Sammler"}, {"authorId": "2272452881", "name": "Arno Villringer"}], "abstract": "Abstract Recursive hierarchical embedding allows humans to generate multiple hierarchical levels using simple rules. We can acquire recursion from exposure to linguistic and visual examples, but only develop the ability to understand “multiple‐level” structures like “[[second] red] ball]” after mastering “same‐level” conjunctions like “[second] and [red] ball.” Whether we can also learn recursion in motor production remains unexplored. Here, we tested 40 adults’ ability to learn and generate sequences of finger movements using “multiple‐level” recursion and “same‐level” iteration rules (like linguistic conjunction). Rule order was counterbalanced. First, they learned the generative rules (without explicit rule instructions or feedback) by executing examples of motor sequences based on visual cues displayed on the screen (learning). Second, participants were asked to discriminate between correct and incorrect motor sequences beyond those to which they were previously exposed (discrimination). Finally, they were asked to use the rules to generate new hierarchical levels consistent with the previously given (generation). We repeated the procedure (all three phases) on 2 days, allowing for a night of sleep. We found that most participants could discriminate correct/incorrect sequences based on recursive rules and use recursive rules to generate new hierarchical levels in motor sequences, but mostly on the second day of testing, and when they had acquired iterative before recursive rules. This aligns with previous literature on vision and language and with literature showing that sleep is necessary to generate abstract knowledge of motor sequences. Lastly, we found that the ability to discriminate well‐formed motor sequences using recursion was insufficient for motor generativity."}
{"paperId": "8d530aab1976a447bc341f606727c8a9a410baa3", "url": "https://www.semanticscholar.org/paper/8d530aab1976a447bc341f606727c8a9a410baa3", "title": "LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-07", "authors": [{"authorId": "2029506943", "name": "Avishree Khare"}, {"authorId": "2256348492", "name": "Hideki Okamoto"}, {"authorId": "1729953", "name": "Bardh Hoxha"}, {"authorId": "1682745", "name": "Georgios Fainekos"}, {"authorId": "1710176", "name": "R. Alur"}], "abstract": "Neural models such as YOLO and HuBERT can be used to detect local properties such as objects (\"car\") and emotions (\"angry\") in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g.,\"does the speaker eventually sound happy in this audio clip?\"), and ranked retrieval (e.g.,\"retrieve top 5 videos with a 10 second scene where a car is detected until a pedestrian is detected\"). In this work, we formalize this problem of assigning Scores for TempOral Properties (STOPs) over sequences, given potentially noisy score predictors for local properties. We then propose a scoring function called LogSTOP that can efficiently compute these scores for temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP, with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and other Temporal Logic-based baselines by at least 16% on query matching with temporal properties over objects-in-videos and emotions-in-speech respectively. Similarly, on ranked retrieval with temporal properties over objects and actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a 19% and 16% increase in mean average precision and recall over zero-shot text-to-video retrieval baselines respectively."}
{"paperId": "8db39f0b09d7b6a9781324cee5f4271807e6c65d", "url": "https://www.semanticscholar.org/paper/8db39f0b09d7b6a9781324cee5f4271807e6c65d", "title": "A Literature Survey On Sentiment Analysis Using Image Processing", "venue": "IOSR Journal of Computer Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.9790/0661-2702015768?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.9790/0661-2702015768, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2355797787", "name": "Srijan Sen"}, {"authorId": "2355535791", "name": "Shourov Sikde"}], "abstract": "Sentiment analysis, the automated process of determining emotions and opinions expressed in content, has\nevolved to encompass visual media, allowing for a deeper understanding of sentiments conveyed in images. This\npaper explores the application of image processing techniques, coupled with Python programming, to conduct\nsentiment analysis by extracting emotional cues from visual data.\nThe study begins with an overview of the significance and challenges of sentiment analysis in images, emphasizing\nthe need for advanced tools to analyze the ever-growing volume of visual content on the internet. Leveraging\nPython's rich ecosystem of libraries, the paper delves into the technical aspects of sentiment analysis using image\ndata.\nKey components of this research include the utilization of Convolutional Neural Networks (CNNs) for feature\nextraction, pre-trained models for sentiment recognition, and the development of custom datasets to train and\nvalidate sentiment analysis models. Python libraries like TensorFlow and Keras provide a robust framework for\nbuilding and deploying deep learning models.\nThe paper discusses the ethical considerations related to image-based sentiment analysis, addressing concerns\nabout privacy, bias, and cultural nuances. It also explores the potential applications of this technology, ranging\nfrom brand sentiment analysis in marketing to monitoring public sentiment on social media platforms.\nFurthermore, the study identifies challenges and opportunities in the field, paving the way for future research\nendeavors. By bridging the domains of computer vision, natural language processing, and machine learning, sentiment analysis by image processing in Python opens up new avenues for understanding the emotional impact of visual content in an increasingly digital and visually driven world."}
{"paperId": "8ee4e11d08d1fd1c272a203a9b7aaf4340a1b7de", "url": "https://www.semanticscholar.org/paper/8ee4e11d08d1fd1c272a203a9b7aaf4340a1b7de", "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.18845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-24", "authors": [{"authorId": "2328143873", "name": "Changxin Huang"}, {"authorId": "2396877071", "name": "Lv Tang"}, {"authorId": "2301932789", "name": "Zhaohuan Zhan"}, {"authorId": "2302279883", "name": "Lisha Yu"}, {"authorId": "2274299134", "name": "Runhao Zeng"}, {"authorId": "2394089526", "name": "Zun Liu"}, {"authorId": "2394192290", "name": "Zhengjie Wang"}, {"authorId": "2327679204", "name": "Jianqiang Li"}], "abstract": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness."}
{"paperId": "8fbe44cf67faecf3de130b88e35f65395672f7bd", "url": "https://www.semanticscholar.org/paper/8fbe44cf67faecf3de130b88e35f65395672f7bd", "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-15", "authors": [{"authorId": "2398802768", "name": "Jebeom Chae"}, {"authorId": "2238150373", "name": "Junwoo Chang"}, {"authorId": "2398799542", "name": "Seungho Yeom"}, {"authorId": null, "name": "Yujin Kim"}, {"authorId": "2238150779", "name": "Jongeun Choi"}], "abstract": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency."}
{"paperId": "907fad5571619df00b30fb2ae2458d767556e786", "url": "https://www.semanticscholar.org/paper/907fad5571619df00b30fb2ae2458d767556e786", "title": "KIEPrompter: Leveraging Lightweight Models' Predictions for Cost-Effective Key Information Extraction using Vision LLMs", "venue": "International Conference on Information and Knowledge Management", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746252.3761416?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746252.3761416, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-11-10", "authors": [{"authorId": "30079913", "name": "Lorenzo Vaiani"}, {"authorId": "2166956722", "name": "Yihao Ding"}, {"authorId": "2287930712", "name": "Luca Cagliero"}, {"authorId": "2282598754", "name": "Jean Lee"}, {"authorId": "2292447799", "name": "Paolo Garza"}, {"authorId": "144179461", "name": "Josiah Poon"}, {"authorId": "2046142", "name": "S. Han"}], "abstract": "Key information extraction (KIE) from visually rich documents, such as receipts and forms, involves a deep understanding of textual, visual, and layout feature information. Transformers fine-tuned for KIE achieve state-of-the-art performance but lack generality and portability across different domains. In contrast, vision large language models (VLLMs) offer higher flexibility and zero-shot capability but fall short with domain-specific layout relations unless performing a resource-demanding supervised fine-tuning. To reach the best compromise solution between lightweight models and VLLMs, we propose KIEPrompter, a cost-effective LLM-based KIE approach that leverages the predictions of lightweight models as external knowledge injected into VLLM prompts. By incorporating these auxiliary predictions, VLLMs are guided to attend relevant multimodal content without ad hoc training. The accuracy results achieved by KIEPrompter in three benchmark document collections are superior to those of VLLMs in both zero-shot and layout-sensitive scenarios. We compare various strategies for incorporating lightweight model predictions, ranging from coarse-grained predictions without explicit confidence scores to fine-grained per-element network logits. We also demonstrate that our approach is robust to the absence of specific classes in trained lightweight models, as the VLLMs' pre-training compensates for the limited generality of lightweight models."}
{"paperId": "908b0627c4409ec8c96a4b23ee81867f53aba7e6", "url": "https://www.semanticscholar.org/paper/908b0627c4409ec8c96a4b23ee81867f53aba7e6", "title": "Smart Sign Language Interpreter for Text and Speech Conversion", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem52921?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem52921, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-09", "authors": [{"authorId": "2384908782", "name": "Dr.K .Malarvizhi"}, {"authorId": "2384889528", "name": "R. .Vijaykumar"}, {"authorId": "2384908799", "name": "B. K. Pavishnu"}, {"authorId": "2384914797", "name": "P. Hariharan"}, {"authorId": "2384908789", "name": "A. Kavin"}], "abstract": "Abstract - The communication obstacle for individuals with hearing impairments causes severe issues in education and workplaces. This research considers how artificial intelligence (AI), such as computer vision and language models, can be utilized to develop an intelligent sign language interpretation system. This system is able to rapidly translate gestures into text and speech. We surveyed recent progress from 2018 to 2025 in gesture recognition, deep learning, and natural language processing. We have structured the work into four broad categories: sensor-based motion capture, AI-based gesture recognition, text-to-speech translation frameworks, and real-time communication interfaces. Our review emphasizes the strengths and limitations of today's systems. Accuracy can go up to 95% in laboratory conditions, but real-time applications in ever-changing environments are still not possible. This project envisages a straightforward design that integrates vision-based gesture recognition, predictive AI interpretation, and speech generation to bridge the communication divide smoothly. The research also identifies areas where additional research is required, including support for multiple languages, context-sensitive interpretation, and low-latency real-time execution. This paper hopes to develop an inclusive and smart communication aid for the hearing-impaired community. Keywords: AI, Sign Language Recognition, Real-Time Translation, Gesture Detection, Speech Synthesis, Hearing-Impaired Communication, Accessibility Technology.\n\nKey Words: AI, hand gestures, classification, instant translation, sign language recognition, speech production, connection with the deaf community, assistive technology."}
{"paperId": "918eb67d20249d38129cdb3e768bb9cc72c47a0d", "url": "https://www.semanticscholar.org/paper/918eb67d20249d38129cdb3e768bb9cc72c47a0d", "title": "OVL-MAP: An Online Visual Language Map Approach for Vision-and-Language Navigation in Continuous Environments", "venue": "IEEE Robotics and Automation Letters", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LRA.2025.3540577?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LRA.2025.3540577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2345684484", "name": "Shuhuan Wen"}, {"authorId": "2325912952", "name": "Ziyuan Zhang"}, {"authorId": "2161366509", "name": "Yuxiang Sun"}, {"authorId": "2325557794", "name": "Zhiwen Wang"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate 3D environments based on visual observations and natural language instructions. Existing approaches, focused on topological and semantic maps, often face limitations in accurately understanding and adapting to complex or previously unseen environments, particularly due to static and offline map constructions. To address these challenges, this letter proposes OVL-MAP, an innovative algorithm comprising three key modules: an online vision-and-language map construction module, a waypoint prediction module, and an action decision module. The online map construction module leverages robust open-vocabulary semantic segmentation to dynamically enhance the agent's scene understanding. The waypoint prediction module processes natural language instructions to identify task-relevant regions, predict sub-goal locations, and guide trajectory planning. The action decision module utilizes the DD-PPO strategy for effective navigation. Evaluations on the Robo-VLN and R2R-CE datasets demonstrate that OVL-MAP significantly improves navigation performance and exhibits stronger generalization in unknown environments."}
{"paperId": "91f5ff0b7fb7fdb8a7a720808dbccaf75242d8ea", "url": "https://www.semanticscholar.org/paper/91f5ff0b7fb7fdb8a7a720808dbccaf75242d8ea", "title": "SpecBPP: A Self-Supervised Learning Approach for Hyperspectral Representation and Soil Organic Carbon Estimation", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.19781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-26", "authors": [{"authorId": "2268405922", "name": "Daniel La'ah Ayuba"}, {"authorId": "2518631", "name": "Jean-Yves Guillemaut"}, {"authorId": "2298514086", "name": "Belén Martí-Cardona"}, {"authorId": "2373609182", "name": "Oscar Mendez Maldonado"}], "abstract": "Self-supervised learning has revolutionized representation learning in vision and language, but remains underexplored for hyperspectral imagery (HSI), where the sequential structure of spectral bands offers unique opportunities. In this work, we propose Spectral Band Permutation Prediction (SpecBPP), a novel self-supervised learning framework that leverages the inherent spectral continuity in HSI. Instead of reconstructing masked bands, SpecBPP challenges a model to recover the correct order of shuffled spectral segments, encouraging global spectral understanding. We implement a curriculum-based training strategy that progressively increases permutation difficulty to manage the factorial complexity of the permutation space. Applied to Soil Organic Carbon (SOC) estimation using EnMAP satellite data, our method achieves state-of-the-art results, outperforming both masked autoencoder (MAE) and joint-embedding predictive (JEPA) baselines. Fine-tuned on limited labeled samples, our model yields an $R^2$ of 0.9456, RMSE of 1.1053%, and RPD of 4.19, significantly surpassing traditional and self-supervised benchmarks. Our results demonstrate that spectral order prediction is a powerful pretext task for hyperspectral understanding, opening new avenues for scientific representation learning in remote sensing and beyond."}
{"paperId": "92a73d0e6414631989898d551ce3fb7368b909b6", "url": "https://www.semanticscholar.org/paper/92a73d0e6414631989898d551ce3fb7368b909b6", "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.17406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-20", "authors": [{"authorId": "2329512755", "name": "Haochen Zhang"}, {"authorId": "2329370999", "name": "Nader Zantout"}, {"authorId": "2232238387", "name": "Pujith Kachana"}, {"authorId": "2329519808", "name": "Ji Zhang"}, {"authorId": "2296220021", "name": "Wenshan Wang"}], "abstract": "With the recent rise of large language models, vision-language models, and other general foundation models, there is growing potential for multimodal, multi-task robotics that can operate in diverse environments given natural language input. One such application is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the 3D spatial reasoning and semantic understanding required. Additionally, the language used may be imperfect or misaligned with the scene, further complicating the task. To address this challenge, we curate a benchmark dataset, IRef-VLA, for Interactive Referential Vision and Language-guided Action in 3D Scenes with imperfect references. IRef-VLA is the largest real-world dataset for the referential grounding task, consisting of over 11.5 K scanned 3D rooms from existing datasets, 7.6 M heuristically generated semantic relations, and 4.7 M referential statements. Our dataset also contains semantic object and room annotations, scene graphs, navigable free space annotations, and is augmented with statements where the language has imperfections or ambiguities. We verify the generalizability of our dataset by evaluating with state-of-the-art models to obtain a performance baseline and also develop a graphsearch baseline to demonstrate the performance bound and generation of alternatives using scene-graph knowledge. With this benchmark, we aim to provide a resource for 3D scene understanding that aids the development of robust, interactive navigation systems. The dataset and all source code is publicly released11https://github.com/HaochenZ11/IRef-VLA."}
{"paperId": "92d4107fd96608c3aa6fc1a1e0498fa3169938bf", "url": "https://www.semanticscholar.org/paper/92d4107fd96608c3aa6fc1a1e0498fa3169938bf", "title": "Vision and Language Integration for Domain Generalization", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-17", "authors": [{"authorId": "2130356228", "name": "Yanmei Wang"}, {"authorId": "2331001920", "name": "Xiyao Liu"}, {"authorId": "2211512709", "name": "Fupeng Chu"}, {"authorId": "2331462230", "name": "Zhi Han"}], "abstract": "Domain generalization aims at training on source domains to uncover a domain-invariant feature space, allowing the model to perform robust generalization ability on unknown target domains. However, due to domain gaps, it is hard to find reliable common image feature space, and the reason for that is the lack of suitable basic units for images. Different from image in vision space, language has comprehensive expression elements that can effectively convey semantics. Inspired by the semantic completeness of language and intuitiveness of image, we propose VLCA, which combine language space and vision space, and connect the multiple image domains by using semantic space as the bridge domain. Specifically, in language space, by taking advantage of the completeness of language basic units, we tend to capture the semantic representation of the relations between categories through word vector distance. Then, in vision space, by taking advantage of the intuitiveness of image features, the common pattern of sample features with the same class is explored through low-rank approximation. In the end, the language representation is aligned with the vision representation through the multimodal space of text and image. Experiments demonstrate the effectiveness of the proposed method."}
{"paperId": "930314720ee29ea6c12de392c8dde3f942b9e520", "url": "https://www.semanticscholar.org/paper/930314720ee29ea6c12de392c8dde3f942b9e520", "title": "History-Guided Prompt Generation for Vision-and-Language Navigation.", "venue": "IEEE Transactions on Cybernetics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCYB.2025.3613147?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCYB.2025.3613147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2241917230", "name": "Wen Guo"}, {"authorId": "2367428977", "name": "Zongmeng Wang"}, {"authorId": "2267739624", "name": "Yufan Hu"}, {"authorId": "46930271", "name": "Junyu Gao"}], "abstract": "Vision-and-language navigation (VLN) has garnered extensive attention in the field of embodied artificial intelligence. VLN involves time series information, where historical observations contain rich contextual knowledge and play a crucial role in navigation. However, current methods do not explicitly excavate the connection between rich contextual information in history and the current environment, and ignore adaptive learning of clues related to the current environment. Therefore, we explore a Prompt Learning-based strategy which adaptively mines information in history that is highly relevant to the current environment to enhance the agent's perception of the current environment and propose a history-guided prompt generation (HGPG) framework. Specifically, HGPG includes two parts, one is an entropy-based history acquisition module that assesses the uncertainty of the action probability distribution from the preceding step to determine whether historical information should be used at the current time step. The other part is the prompt generation module that transforms historical context into prompt vectors by sampling from an end-to-end learned token library. These prompt tokens serve as discrete, knowledge-rich representations that encode semantic cues from historical observations in a compact form, making them easier for the decision network to understand and utilize. In addition, we share the token library across various navigation tasks, mining common features between different tasks to improve generalization to unknown environments. Extensive experimental results on four mainstream VLN benchmarks (R2R, REVERIE, SOON, R2R-CE) demonstrate the effectiveness of our proposed method. Code is available at https://github.com/Wzmshdong/HGPG."}
{"paperId": "935de7ad31d6d43531816590df8e0b24e987ee67", "url": "https://www.semanticscholar.org/paper/935de7ad31d6d43531816590df8e0b24e987ee67", "title": "EGRTE: adversarially training a self-explaining smoothed classifier for certified robustness", "venue": "Cybersecurity", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s42400-025-00375-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s42400-025-00375-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-21", "authors": [{"authorId": "2276745960", "name": "Zijin Lin"}, {"authorId": "3259508", "name": "Jinwen He"}, {"authorId": "2276745296", "name": "Yue Zhao"}, {"authorId": "2387013121", "name": "Ruigang Liang"}, {"authorId": "2387108862", "name": "Hu Li"}, {"authorId": "2388093760", "name": "ZhenDong Wu"}], "abstract": null}
{"paperId": "936ea28bc19bcda8f181a8f5f3b6c5c9bf5938cf", "url": "https://www.semanticscholar.org/paper/936ea28bc19bcda8f181a8f5f3b6c5c9bf5938cf", "title": "Shortcut Learning Susceptibility in Vision Classifiers", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.09150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-13", "authors": [{"authorId": "2284680449", "name": "Pirzada Suhail"}, {"authorId": "2284680965", "name": "Amit Sethi"}], "abstract": "Shortcut learning, where machine learning models exploit spurious correlations in data instead of capturing meaningful features, poses a significant challenge to building robust and generalizable models. This phenomenon is prevalent across various machine learning applications, including vision, natural language processing, and speech recognition, where models may find unintended cues that minimize training loss but fail to capture the underlying structure of the data. Vision classifiers based on Convolutional Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers (ViTs) leverage distinct architectural principles to process spatial and structural information, making them differently susceptible to shortcut learning. In this study, we systematically evaluate these architectures by introducing deliberate shortcuts into the dataset that are correlated with class labels both positionally and via intensity, creating a controlled setup to assess whether models rely on these artificial cues or learn actual distinguishing features. We perform both quantitative evaluation by training on the shortcut-modified dataset and testing on two different test sets-one containing the same shortcuts and another without them-to determine the extent of reliance on shortcuts. Additionally, qualitative evaluation is performed using network inversion-based reconstruction techniques to analyze what the models internalize in their weights, aiming to reconstruct the training data as perceived by the classifiers. Further, we evaluate susceptibility to shortcut learning across different learning rates. Our analysis reveals that CNNs at lower learning rates tend to be more reserved against entirely picking up shortcut features, while ViTs, particularly those without positional encodings, almost entirely ignore the distinctive image features in the presence of shortcuts."}
{"paperId": "93c94cd5ab7bc7cbe9b28a07e95a29137a5d6ee9", "url": "https://www.semanticscholar.org/paper/93c94cd5ab7bc7cbe9b28a07e95a29137a5d6ee9", "title": "Enhancing Semi-supervised Learning with Zero-shot Pseudolabels", "venue": "", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-02-18", "authors": [{"authorId": "2346724257", "name": "Jichan Chung"}, {"authorId": "2345927094", "name": "Irene Y. Chen"}], "abstract": "The high cost of data labeling presents a major barrier to deploying machine learning systems at scale. Semi-supervised learning (SSL) mitigates this challenge by utilizing unlabeled data alongside limited labeled examples, while the emergence of foundation models (FMs) offers powerful zero-shot capabilities that can further reduce labeling cost. However, directly fine-tuning large FMs is often impractical in resource-constrained settings, and na\\\"ively using their pseudo-labels for unlabeled data can degrade performance due to its unreliablity or domain mismatch with target task. In this work, we introduce ZeroMatch, a novel SSL framework that integrates knowledge distillation with consistency-based learning to jointly leverage labeled data, unlabeled data, and pseudo-labels from FMs. ZeroMatch enables training compact student models using only FM inference, making it suitable for low-resource environments such as personal devices with limited compute. Experiments on six vision and language classification benchmarks show that ZeroMatch consistently outperforms standard SSL and zero-shot augmented methods, demonstrating its effectiveness and robustness across a range of foundation model qualities."}
{"paperId": "93d097fe3e4fe7de9f4e79800dff5c41b69df3e9", "url": "https://www.semanticscholar.org/paper/93d097fe3e4fe7de9f4e79800dff5c41b69df3e9", "title": "Improving vision-language alignment with graph spiking hybrid Networks", "venue": "Knowledge-Based Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.19069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-31", "authors": [{"authorId": "2267495455", "name": "Siyu Zhang"}, {"authorId": "2343600453", "name": "Heming Zheng"}, {"authorId": "2343592428", "name": "Yiming Wu"}, {"authorId": "2195014867", "name": "Ye-Ting Chen"}], "abstract": "To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks."}
{"paperId": "9457b629508f4a69b2b3f7dc124b20f2cb1a3a76", "url": "https://www.semanticscholar.org/paper/9457b629508f4a69b2b3f7dc124b20f2cb1a3a76", "title": "An Inclusive AI Docent System for Accessible and Interactive Art Appreciation using Vision and Language Models", "venue": "Journal of the Korea Society of Computer and Information", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.9708/jksci.2025.30.06.109?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.9708/jksci.2025.30.06.109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2369657929", "name": "Min-Su Kim"}, {"authorId": "2369906363", "name": "Min Kim"}, {"authorId": "2369661336", "name": "H. Kwak"}, {"authorId": "2369621437", "name": "Ye-jun Choi"}, {"authorId": "2369853206", "name": "Hyung-rok Lee"}, {"authorId": "2369654969", "name": "Chi-wook Ahn"}, {"authorId": "2370407257", "name": "Won-Joo Lee"}, {"authorId": "2369840413", "name": "Young-Bok Cho"}], "abstract": null}
{"paperId": "9460d52865163560e6dda166f67b53e8a25d6e4e", "url": "https://www.semanticscholar.org/paper/9460d52865163560e6dda166f67b53e8a25d6e4e", "title": "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.02405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-02", "authors": [{"authorId": "2273961060", "name": "Anoop Cherian"}, {"authorId": "2395766845", "name": "River Doyle"}, {"authorId": "2395766752", "name": "Eyal Ben-Dov"}, {"authorId": "1890366", "name": "Suhas Lohit"}, {"authorId": "2248068417", "name": "Kuan-Chuan Peng"}], "abstract": "Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents'solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations."}
{"paperId": "9484758a003459b1ccd2682c784f3d0e9d502bea", "url": "https://www.semanticscholar.org/paper/9484758a003459b1ccd2682c784f3d0e9d502bea", "title": "Deep-Bench: Deep Learning Benchmark Dataset for Code Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2322796800", "name": "Alireza Daghighfarsoodeh"}, {"authorId": "2284723402", "name": "Chung-Yu Wang"}, {"authorId": "2316563132", "name": "Hamed Taherkhani"}, {"authorId": "2347353730", "name": "Melika Sepidband"}, {"authorId": "2347356202", "name": "Mohammad Abdollahi"}, {"authorId": "2330399049", "name": "Hadi Hemmati"}, {"authorId": "2322780537", "name": "Hung Viet Pham"}], "abstract": "Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types. To address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text. GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code. Furthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain."}
{"paperId": "94ac3a97644c73a04ea9409dcdc089f5a24fc000", "url": "https://www.semanticscholar.org/paper/94ac3a97644c73a04ea9409dcdc089f5a24fc000", "title": "Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-14", "authors": [{"authorId": "1946923537", "name": "Ganlong Zhao"}, {"authorId": "2255423082", "name": "Guanbin Li"}, {"authorId": "2352301226", "name": "Jia Pan"}, {"authorId": "2226950987", "name": "Yizhou Yu"}], "abstract": "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned aerial vehicle agent to navigate aerial 3D environments following human instruction. Compared to ground-based VLN, aerial VLN requires the agent to decide the next action in both horizontal and vertical directions based on the first-person view observations. Previous methods struggle to perform well due to the longer navigation path, more complicated 3D scenes, and the neglect of the interplay between vertical and horizontal actions. In this paper, we propose a novel grid-based view selection framework that formulates aerial VLN action prediction as a grid-based view selection task, incorporating vertical action prediction in a manner that accounts for the coupling with horizontal actions, thereby enabling effective altitude adjustments. We further introduce a grid-based bird's eye view map for aerial space to fuse the visual information in the navigation history, provide contextual scene information, and mitigate the impact of obstacles. Finally, a cross-modal transformer is adopted to explicitly align the long navigation history with the instruction. We demonstrate the superiority of our method in extensive experiments."}
{"paperId": "94ae45252cbefbb1381d23c20ba3e053e70fdb96", "url": "https://www.semanticscholar.org/paper/94ae45252cbefbb1381d23c20ba3e053e70fdb96", "title": "Chitrarth: Bridging Vision and Language for a Billion People", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-21", "authors": [{"authorId": "2331709229", "name": "Shaharukh Khan"}, {"authorId": "2141034263", "name": "A. Tarun"}, {"authorId": "2331624065", "name": "Abhinav Ravi"}, {"authorId": "2331624512", "name": "Ali Faraz"}, {"authorId": "2345689854", "name": "Akshat Patidar"}, {"authorId": "2331623823", "name": "Praveen Pokala"}, {"authorId": "2346837797", "name": "Anagha Bhangare"}, {"authorId": "2346836523", "name": "Raja Kolla"}, {"authorId": "2331626735", "name": "Chandra Khatri"}, {"authorId": "144992211", "name": "Shubham Agarwal"}], "abstract": "Recent multimodal foundation models are primarily trained on English or high resource European language data, which limits their applicability to other medium and low-resource languages, such as the Indian languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various low resource languages, ultimately contributing to more diverse and effective AI systems. Our model presents SOTA results for benchmarks across Indian languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation for facilitating future advancements in this arena."}
{"paperId": "94f8b2698a2edff629f9cea28907d6f6a2d450e6", "url": "https://www.semanticscholar.org/paper/94f8b2698a2edff629f9cea28907d6f6a2d450e6", "title": "Leveraging computer vision, large language models, and multimodal machine learning for optimal decision-making in dairy farming.", "venue": "Journal of Dairy Science", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "https://doi.org/10.3168/jds.2024-25650", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3168/jds.2024-25650?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3168/jds.2024-25650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review", "JournalArticle"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2260607736", "name": "R. Ferreira"}, {"authorId": "2297440242", "name": "J. Dórea"}], "abstract": "This article explores various applications of artificial intelligence technologies in dairy farming, including the use of computer vision systems (CVS) for animal identification, body condition score (BCS) and body shape analysis, and potential uses of LLMs in the dairy industry. Among recent advancements in precision livestock farming (PLF) tools, CVS have gained popularity as powerful solutions for individual animal monitoring. These systems can capture phenotypes from multiple animals simultaneously using a single device in an automated and non-intrusive manner. To match animals with their corresponding predicted phenotypes, these systems require individual animal identification, which can be achieved through external identification systems or computer vision-based animal identification algorithms. Additionally, modern natural language processing techniques, such as large language models (LLMs), offer opportunities for advanced data integration, including unstructured textual data. Furthermore, we discuss the challenges associated with integrating data from different sources and modalities - such as images, text, and tabular data - into multimodal machine learning systems for phenotype prediction, which also represents a key area of artificial intelligence application. Digital technologies such as CVS and LLMs have the potential to transform dairy farming. CVS can provide individual and objective assessments of animal health, while LLMs can integrate diverse data sources for phenotype prediction. While there is much potential ahead, these technologies offer significant opportunities for advancing animal health monitoring, farm management, and individual phenotyping."}
{"paperId": "965a69bd7527f3177385e22d9fbff928948d0b31", "url": "https://www.semanticscholar.org/paper/965a69bd7527f3177385e22d9fbff928948d0b31", "title": "DSACap: Enhancing Visual-Semantic Alignment with Diffusion-based Framework for Image Captioning", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3755156?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3755156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2335595751", "name": "Liangyu Fu"}, {"authorId": "2335626353", "name": "Junbo Wang"}, {"authorId": "2368478576", "name": "Yuke Li"}, {"authorId": "2368449386", "name": "Qiangguo Jin"}, {"authorId": "2319443925", "name": "Hongsong Wang"}, {"authorId": "2387933806", "name": "Jing Ya"}, {"authorId": "2319651633", "name": "Linjiang Huang"}, {"authorId": "2390208794", "name": "Liang Yao"}, {"authorId": "2288631969", "name": "Jiangbin Zheng"}, {"authorId": "2261939375", "name": "Xuecheng Wu"}, {"authorId": "2180815992", "name": "Zhiyong Wang"}], "abstract": "Diffusion-based image captioning methods have been proposed to address the inherent issues of autoregressive models, such as slow inference speed, significant accumulative errors, and limited generative diversity. However, due to excessive reliance on textual data and constrained training objective, existing diffusion-based methods suffer from a semantic gap between vision and language, ultimately resulting in poor quality of generated captions. To address this issue, we propose a novel diffusion-based semantics aligned image captioning framework, namely DSACap. Specifically, DSACap deviates from existing methods which treat text as the target of noise-adding and denoising, instead directly applying these processes to the image, thus reducing the loss of visual-semantic alignment. In addition, we introduce a reinforcement learning-based training strategy to maximize the semantic alignment between image and text. We feed the generated textual descriptions into an image generation model to reconstruct the original image and use the cosine similarity between the generated image and the original image as the reward to train the image captioning model. Extensive experimental results on the MS COCO dataset demonstrate that DSACap achieves a CIDEr score of 128.8, clearly outperforming existing diffusion-based image captioning methods. Our code will be made publicly open soon."}
{"paperId": "96e3081ea3c8f234be9a84ae45393da27d794ceb", "url": "https://www.semanticscholar.org/paper/96e3081ea3c8f234be9a84ae45393da27d794ceb", "title": "Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval", "venue": "Neurocomputing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.16756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-22", "authors": [{"authorId": "2036585936", "name": "Hailong Ning"}, {"authorId": "2363177848", "name": "Siying Wang"}, {"authorId": "2349731437", "name": "Tao Lei"}, {"authorId": "2362918378", "name": "Xiaopeng Cao"}, {"authorId": "2362726612", "name": "Huanmin Dou"}, {"authorId": "2365404280", "name": "Bin Zhao"}, {"authorId": "2282977894", "name": "Asoke K. Nandi"}, {"authorId": "143601910", "name": "P. Radeva"}], "abstract": "Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in geographic information interpretation, disaster monitoring, and urban planning by establishing semantic associations between image and textual descriptions. Existing Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language Pre-training (VLP) models typically adopt symmetric adapter structures for exploring cross-modal correlations. However, the strong discriminative nature of text modality may dominate the optimization process and inhibits image representation learning. The nonnegligible imbalanced cross-modal optimization remains a bottleneck to enhancing the model performance. To address this issue, this study proposes a Representation Discrepancy Bridging (RDB) method for the RSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is designed to enable modality-specific optimization and improve feature alignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text Semantic Adapter (TSA). VEA mines fine-grained image features by Differential Attention (DA) mechanism, while TSA identifies key textual semantics through Hierarchical Attention (HA) mechanism. On the other hand, this study extends the traditional single-task retrieval framework to a dual-task optimization framework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves cross-modal alignment robustness through an adaptive weighted combination of cross-modal, classification, and exponential moving average consistency constraints. Experiments on RSICD and RSITMD datasets show that the proposed RDB method achieves a 6%-11% improvement in mR metrics compared to state-of-the-art PEFT methods and a 1.15%-2% improvement over the full fine-tuned GeoRSCLIP model."}
{"paperId": "9763c201e3c525bdc6babfc9113f2d6997059ab5", "url": "https://www.semanticscholar.org/paper/9763c201e3c525bdc6babfc9113f2d6997059ab5", "title": "Zero-Shot Object Counting With Vision-Language Prior Guidance Network", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 11, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2024.3488721?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2024.3488721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2141103268", "name": "Wenzheng Zhai"}, {"authorId": "2293432413", "name": "Xianglei Xing"}, {"authorId": "39810756", "name": "Mingliang Gao"}, {"authorId": "2108056391", "name": "Qilei Li"}], "abstract": "The majority of existing counting models are designed to operate on a singular object category, such as crowds or vehicles. The emergence of multi-modal foundational models, e.g., Contrastive Language-Image Pre-training (CLIP), has paved the way for class-agnostic counting. This approach facilitates the counting of objects across diverse classes within a single image based on textual indications. However, class-agnostic counting models based on CLIP confront two primary challenges. Firstly, the CLIP model exhibits limited sensitivity towards location information, which prioritizes global content over the precise localization of objects. Therefore, directly employing the CLIP model is regarded as suboptimal. Secondly, these models commonly employ frozen pre-trained vision and language encoders while disregarding potential misalignment within the constructed hypothesis space. In this paper, we propose a unified framework, named the Vision-Language Prior Guidance (VLPG) Network, to tackle these two challenges. The VLPG consists of three key components, namely the Grounding DINO module, Spatial Prior Calibration (SPC) module, and Object-Centric Alignment (OCA) module. The Grounding DINO module utilizes the spatial-awareness capability of extensive pre-trained object grounding models to incorporate the spatial position as an additional prior for a particular query class. This adaptation enables the network to concentrate more precisely on the exact location of the objects. Meanwhile, the SPC module is built to extract the long-range dependencies and local regions of the spatial position. Additionally, to align the feature space across different modalities, we design an OCA module that condenses textual information into an object query which serves as an instruction for cross-modality matching. Through the collaborative efforts of these three modules, multimodal representations are aligned while maintaining their discriminative nature. Comprehensive experiments conducted on various benchmarks validate the effectiveness of the proposed model."}
{"paperId": "97c2440f01cb56cff81ca7e43a0408e512d9cfc3", "url": "https://www.semanticscholar.org/paper/97c2440f01cb56cff81ca7e43a0408e512d9cfc3", "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.15232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-08-21", "authors": [{"authorId": "2324223496", "name": "Ruipu Wu"}, {"authorId": "2291052942", "name": "Yige Zhang"}, {"authorId": "2371144611", "name": "Jinyu Chen"}, {"authorId": "153562818", "name": "Linjiang Huang"}, {"authorId": "2333082718", "name": "Shifeng Zhang"}, {"authorId": "2332406292", "name": "Xu Zhou"}, {"authorId": "2376505438", "name": "Liang Wang"}, {"authorId": "2373989817", "name": "Si Liu"}], "abstract": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural language instructions and visual cues. However, due to the extended trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN performance is challenging and often requires human intervention or overly detailed instructions. To harness the advantages of UAVs' high mobility, which could provide multi-grained perspectives, while maintaining a manageable motion space for learning, we introduce a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct altitudes: a high-altitude UAV responsible for broad environmental reasoning, and a low-altitude UAV tasked with precise navigation. To support the training and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising 13,838 collaborative high-low UAV demonstration trajectories, each paired with target-oriented language instructions. This dataset includes both unseen maps and an unseen object validation set to systematically evaluate the model's generalization capabilities across novel environments and unfamiliar targets. To consolidate their complementary strengths, we propose a dual-UAV collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a multimodal large language model (Pilot-LLM) for target reasoning, while the low-altitude UAV employs a lightweight multi-stage policy for navigation and target grounding. The two UAVs work collaboratively and only exchange minimal coordinate information to ensure efficiency. Experimental results indicate that AeroDuo achieves an evident 9.71% improvement in success rates compared to existing single-UAV methods, demonstrating the effectiveness of dual-altitude collaboration in balancing environmental coverage, precision, and operational autonomy."}
{"paperId": "97e09762eebb023374daa9cfd66d69cc02758238", "url": "https://www.semanticscholar.org/paper/97e09762eebb023374daa9cfd66d69cc02758238", "title": "Bridging vision and language multi modal learning for improved image understanding", "venue": "International journal of research in humanities and social sciences", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63345/ijrhs.net.v13.i3.16?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63345/ijrhs.net.v13.i3.16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2351163625", "name": "Aatishkumar Dhami"}, {"authorId": "2351495972", "name": "Prof (Dr) Ajay Shriram Kushwaha"}], "abstract": "In this study, we introduce a novel multimodal framework that synergistically integrates visual and linguistic cues to enhance image understanding. By leveraging deep neural architectures tailored for both vision and language processing, our approach extracts rich semantic representations from images and complements them with contextual information from associated text. This integration not only improves the accuracy of image classification and caption generation but also enhances the interpretability and robustness of the overall system. Experimental evaluations on standard benchmarks demonstrate that our model outperforms traditional unimodal methods, underscoring the potential of bridging vision and language in achieving more comprehensive image analysis. The proposed framework lays the groundwork for future applications in areas such as visual question answering, content-based retrieval, and automated scene interpretation."}
{"paperId": "9812d33e3906bf8f872d0dcdc08d08dd9816535c", "url": "https://www.semanticscholar.org/paper/9812d33e3906bf8f872d0dcdc08d08dd9816535c", "title": "Semantic Alignment on Action for Image Captioning", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3631093?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3631093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2392028329", "name": "Da Huo"}, {"authorId": "31549937", "name": "Marc A. Kastner"}, {"authorId": "2639141", "name": "Takatsugu Hirayama"}, {"authorId": "2243999883", "name": "Takahiro Komamizu"}, {"authorId": "2326135368", "name": "Yasutomo Kawanishi"}, {"authorId": "2258944865", "name": "Ichiro Ide"}], "abstract": "Image captioning is a popular task in vision and language processing, which aims to generate textual descriptions for images. Previously, it simply used image and text as input with self-attention to capture global dependencies. Recent research further uses objects detected from the input image, so-called object tags, as anchor points to ease alignment between image and text with the attention mechanism. However, they only consider object information in images, while neglecting the actions and object interactions that also appear in the image, which causes actions not caught properly in image captioning. To tackle this previously underrepresented dimension of the semantic alignment, we take account of actions on the semantic level. Specifically, our work focuses on human actions and interactions, which ensures that more salient parts of the image get captioned. We introduce a new type of tag, called action tag, to anchor the action information. First, we provide a method for obtaining such action tags using an action detection model which predicts actions in the image. Next, we leverage these action tags into the captioning model. Experimental results indicate that the proposed action tags can help learn action semantics and catch the salient actions leading to perceived improvements in common performance. Experimental results on MS-COCO Karpathy test split show that the proposed model achieves good scores in BLEU-4 and CIDEr metrics, using action tags as anchors. Furthermore, the number of action tags (no more than 5) is smaller than that of object tags (commonly more than 20), which means there is a potential to reduce FLOPs by reducing the total sequence length. It indicates the potential for efficient reasoning and may be applied to daily activity scenes in the future."}
{"paperId": "984b9e060705735e9f0e78f47aee2eec9bf98b7a", "url": "https://www.semanticscholar.org/paper/984b9e060705735e9f0e78f47aee2eec9bf98b7a", "title": "Developing and Validating a Video-Based Measurement Instrument for Assessing Teachers’ Professional Vision of Language-Stimulation Interactions in the ECE Classroom", "venue": "Education sciences", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/educsci15020155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/educsci15020155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-26", "authors": [{"authorId": "2342624130", "name": "Lien Dorme"}, {"authorId": "2342548880", "name": "Anne-Lotte Stevens"}, {"authorId": "79580559", "name": "W. Vantieghem"}, {"authorId": "2342623800", "name": "Kris Van den Branden"}, {"authorId": "39877741", "name": "R. Vanderlinde"}], "abstract": "This study reports on the development and validation of a video-based instrument to assess early childhood education (ECE) teachers’ professional vision (PV) of language-stimulation (LS) interactions. PV refers to noticing and reasoning about key classroom interactions, a skill that can be trained and distinguishes experts from novices. The instrument targets the PV of three language-stimulation (LS) strategies: language input (LI), opportunities for language production (OLP), and feedback (FB). The instrument measures noticing through comparative judgement (CJ) and reasoning through multiple-choice items. Construct validity was assessed using the AERA framework, using three samples: a sample of professionals (n = 22), a pre-service teachers’ sample (n = 107), and a mixed sample with in- and pre-service teachers (n = 6). Reliability and validity were confirmed, with strong reliability scores for the CJ master rank orders (SRR: 0.827–0.866). Think-aloud procedures demonstrated that respondents’ decisions during CJ were mainly based on LS-relevant video features. Decisions unrelated to LS require further study. Multiple-choice reasoning items were developed from professionals’ open-ended feedback. Pre-service teacher reasoning scores showed no significant predictors. Using real classroom videos, this instrument provides an ecologically valid, scalable tool for assessing teachers’ professional vision of LS interactions. This validated instrument offers a foundation for professional development programs aimed at addressing the theory–practice gap in early language education."}
{"paperId": "985729e26c8d726642c3afe5d83ec1080f049149", "url": "https://www.semanticscholar.org/paper/985729e26c8d726642c3afe5d83ec1080f049149", "title": "Rethinking the Text-Vision Reasoning Imbalance in MLLMs through the Lens of Training Recipes", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.22836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-26", "authors": [{"authorId": "2387866991", "name": "Guanyu Yao"}, {"authorId": "2112250365", "name": "Qiucheng Wu"}, {"authorId": "2388309352", "name": "Yang Zhang"}, {"authorId": "2388566693", "name": "Zhaowen Wang"}, {"authorId": "2387986120", "name": "Handong Zhao"}, {"authorId": "2191955409", "name": "Shiyu Chang"}], "abstract": "Multimodal large language models (MLLMs) have demonstrated strong capabilities on vision-and-language tasks. However, recent findings reveal an imbalance in their reasoning capabilities across visual and textual modalities. Specifically, current MLLMs often over-rely on textual cues while under-attending to visual content, resulting in suboptimal performance on tasks that require genuine visual reasoning. We refer to this phenomenon as the \\textit{modality gap}, defined as the performance disparity between text-centric and vision-centric inputs. In this paper, we analyze the modality gap through the lens of training recipes. We first show that existing training recipes tend to amplify this gap. Then, we systematically explore strategies to bridge it from two complementary perspectives: data and loss design. Our findings provide insights into developing training recipes that mitigate the modality gap and promote more balanced multimodal reasoning. Our code is publicly available at https://github.com/UCSB-NLP-Chang/Bridging-Modality-Gap."}
{"paperId": "9886283a9aaa1553bd5860229ccefa1210eda88a", "url": "https://www.semanticscholar.org/paper/9886283a9aaa1553bd5860229ccefa1210eda88a", "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.16460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-19", "authors": [{"authorId": "32820056", "name": "John Abascal"}, {"authorId": "2367738705", "name": "Nicol'as Berrios"}, {"authorId": "2288696989", "name": "Alina Oprea"}, {"authorId": "2275606315", "name": "Jonathan R. Ullman"}, {"authorId": "2304904215", "name": "Adam Smith"}, {"authorId": "40844378", "name": "Matthew Jagielski"}], "abstract": "Multitask learning (MTL) has emerged as a powerful paradigm that leverages similarities among multiple learning tasks, each with insufficient samples to train a standalone model, to solve them simultaneously while minimizing data sharing across users and organizations. MTL typically accomplishes this goal by learning a shared representation that captures common structure among the tasks by embedding data from all tasks into a common feature space. Despite being designed to be the smallest unit of shared information necessary to effectively learn patterns across multiple tasks, these shared representations can inadvertently leak sensitive information about the particular tasks they were trained on. In this work, we investigate what information is revealed by the shared representations through the lens of inference attacks. Towards this, we propose a novel, black-box task-inference threat model where the adversary, given the embedding vectors produced by querying the shared representation on samples from a particular task, aims to determine whether that task was present when training the shared representation. We develop efficient, purely black-box attacks on machine learning models that exploit the dependencies between embeddings from the same task without requiring shadow models or labeled reference data. We evaluate our attacks across vision and language domains for multiple use cases of MTL and demonstrate that even with access only to fresh task samples rather than training data, a black-box adversary can successfully infer a task's inclusion in training. To complement our experiments, we provide theoretical analysis of a simplified learning setting and show a strict separation between adversaries with training samples and fresh samples from the target task's distribution."}
{"paperId": "98e0b9ea062af8817a26b108842556565819de46", "url": "https://www.semanticscholar.org/paper/98e0b9ea062af8817a26b108842556565819de46", "title": "In-Context Vision-Pattern-Language Model for Enhancing Vessel Activity Explanation", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3572714?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3572714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2115470851", "name": "Changha Lee"}, {"authorId": "117080238", "name": "Chan-Hyun Youn"}], "abstract": "Illegal vessel activities pose significant threats to marine resources and ecosystems worldwide, necessitating effective monitoring and detection methods. Current vessel monitoring systems struggle to accurately interpret vessel behaviors at a detailed level due to limitations in utilizing multi-modal data and regulatory frameworks. To overcome these challenges, we propose a new Vision-Pattern-Language (VPL) model designed to enhance the explanation and detection of illegal, unreported, and unregulated (IUU) vessel activities. To handle AIS-off in boundary waters, proposed model integrates satellite imagery with Automatic Identification System (AIS) trajectory data using a probabilistic fusion approach based on Maximum A Posteriori (MAP) estimation with Monte Carlo dropout. Additionally, the VPL model employs a CLIP-based zero-shot classifier to accurately identify vessel behaviors. To support law enforcement, the VPL model with in-context learning also generates faithful and contextually reasonable explanations grounded in the fused data and a legal-text database. Extensive experimental evaluations on the AIS and satellite imagery dataset demonstrate that the VPL model significantly improves trajectory prediction accuracy and classification performance than baselines. Moreover, VPL attains higher faithfulness and reasoning scores compared to Llama-3.3, highlighting its potential for robust and reliable maritime surveillance and contributing meaningfully to the detection and regulation of IUU vessel activities."}
{"paperId": "98e38fe938927306c0bf370fac285bf8cac7d46c", "url": "https://www.semanticscholar.org/paper/98e38fe938927306c0bf370fac285bf8cac7d46c", "title": "SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-27", "authors": [{"authorId": "2304516852", "name": "Jiayuan Du"}, {"authorId": "2395587704", "name": "Yiming Zhao"}, {"authorId": "2395869147", "name": "Zhenglong Guo"}, {"authorId": "2395550235", "name": "Yong Pan"}, {"authorId": "2395535496", "name": "Wenbo Hou"}, {"authorId": "2304876709", "name": "Zhihui Hao"}, {"authorId": "2395535432", "name": "Kun Zhan"}, {"authorId": "2395585697", "name": "Qijun Chen"}], "abstract": "This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning."}
{"paperId": "9902dc7a23a5d4a1fb9d74d5c918234ed9238093", "url": "https://www.semanticscholar.org/paper/9902dc7a23a5d4a1fb9d74d5c918234ed9238093", "title": "Language-guided Visual Tracking: Comprehensive and Effective Multimodal Information Fusion", "venue": "ACM Trans. Multim. Comput. Commun. Appl.", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3757322?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3757322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-05", "authors": [{"authorId": "2202880525", "name": "Jianbo Song"}, {"authorId": "2252215668", "name": "Hong Zhang"}, {"authorId": "2115389431", "name": "Yachun Feng"}, {"authorId": "2211421106", "name": "Hanyang Liu"}, {"authorId": "2108989104", "name": "Yifan Yang"}], "abstract": "Current vision-language trackers often struggle to fuse multimodal information comprehensively and effectively, leading to suboptimal performance in multimodal tasks. This study introduces LGTrack, a novel language-guided visual tracking framework designed to achieve a more comprehensive and efficient fusion of vision and language information. In the encoding stage, an Enhanced Multimodal Interaction Module is proposed to achieve full multimodal fusion, and it is used to construct Early Language Multilevel-guided Multimodal Encoding, which leverages deep semantic information for early and multilevel guidance of vision encoding. In the decoding stage, a multimodal decoding based on Joint Query is proposed, utilizing global features from both vision and language modalities, guiding the efficient operation of the decoding layers. These innovations achieve a more comprehensive fusion of multimodal information. Additionally, a contrastive learning strategy is introduced to align vision-language features in the semantic space, further enhancing the fusion effectiveness. Extensive experiments on multiple benchmarks such as LaSOT, \\(\\rm{LaSOT_{ext}}\\) , TNL2K, and OTB99-Lang demonstrate that our approach outperforms existing state-of-the-art trackers."}
{"paperId": "99a944509664d512dbd9b8fdbe4b43e1b191631f", "url": "https://www.semanticscholar.org/paper/99a944509664d512dbd9b8fdbe4b43e1b191631f", "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-29", "authors": [{"authorId": "2388960720", "name": "Jin Seong"}, {"authorId": "2389320894", "name": "Jiyun Park"}, {"authorId": "2152021361", "name": "Wencke Liermann"}, {"authorId": "2389299471", "name": "Hongseok Choi"}, {"authorId": "2389354541", "name": "Yoonji Nam"}, {"authorId": "2388962663", "name": "Hyun Kim"}, {"authorId": "2388968452", "name": "Soojong Lim"}, {"authorId": "2390457856", "name": "Namhoon Lee"}], "abstract": "The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs."}
{"paperId": "99f04c2d1d0ee1d22712de804ce6ec481e0082e0", "url": "https://www.semanticscholar.org/paper/99f04c2d1d0ee1d22712de804ce6ec481e0082e0", "title": "Analyzing Diagnostic Reasoning of Vision–Language Models via Zero-Shot Chain-of-Thought Prompting in Medical Visual Question Answering", "venue": "Mathematics", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math13142322?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math13142322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-21", "authors": [{"authorId": "2214751543", "name": "Fatema Tuj Johora Faria"}, {"authorId": "67047733", "name": "Laith H. Baniata"}, {"authorId": "2295225718", "name": "Ahyoung Choi"}, {"authorId": "2268289505", "name": "Sangwoo Kang"}], "abstract": "Medical Visual Question Answering (MedVQA) lies at the intersection of computer vision, natural language processing, and clinical decision-making, aiming to generate accurate responses from medical images paired with complex inquiries. Despite recent advances in vision–language models (VLMs), their use in healthcare remains limited by a lack of interpretability and a tendency to produce direct, unexplainable outputs. This opacity undermines their reliability in medical settings, where transparency and justification are critically important. To address this limitation, we propose a zero-shot chain-of-thought prompting framework that guides VLMs to perform multi-step reasoning before arriving at an answer. By encouraging the model to break down the problem, analyze both visual and contextual cues, and construct a stepwise explanation, the approach makes the reasoning process explicit and clinically meaningful. We evaluate the framework on the PMC-VQA benchmark, which includes authentic radiological images and expert-level prompts. In a comparative analysis of three leading VLMs, Gemini 2.5 Pro achieved the highest accuracy (72.48%), followed by Claude 3.5 Sonnet (69.00%) and GPT-4o Mini (67.33%). The results demonstrate that chain-of-thought prompting significantly improves both reasoning transparency and performance in MedVQA tasks."}
{"paperId": "99f3633ac6bbffd6d113d28270be96c49ed0f0b7", "url": "https://www.semanticscholar.org/paper/99f3633ac6bbffd6d113d28270be96c49ed0f0b7", "title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.06588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-08", "authors": [{"authorId": "2335677721", "name": "Zian Zhai"}, {"authorId": "2297246940", "name": "Fan Li"}, {"authorId": "2260858714", "name": "Xingyu Tan"}, {"authorId": "2243441133", "name": "Xiaoyang Wang"}, {"authorId": "2260826901", "name": "Wenjie Zhang"}], "abstract": "Vector Quantization (VQ) has recently emerged as a promising approach for learning discrete representations of graph-structured data. However, a fundamental challenge, i.e., codebook collapse, remains underexplored in the graph domain, significantly limiting the expressiveness and generalization of graph tokens.In this paper, we present the first empirical study showing that codebook collapse consistently occurs when applying VQ to graph data, even with mitigation strategies proposed in vision or language domains. To understand why graph VQ is particularly vulnerable to collapse, we provide a theoretical analysis and identify two key factors: early assignment imbalances caused by redundancy in graph features and structural patterns, and self-reinforcing optimization loops in deterministic VQ. To address these issues, we propose RGVQ, a novel framework that integrates graph topology and feature similarity as explicit regularization signals to enhance codebook utilization and promote token diversity. RGVQ introduces soft assignments via Gumbel-Softmax reparameterization, ensuring that all codewords receive gradient updates. In addition, RGVQ incorporates a structure-aware contrastive regularization to penalize the token co-assignments among dissimilar node pairs. Extensive experiments demonstrate that RGVQ substantially improves codebook utilization and consistently boosts the performance of state-of-the-art graph VQ backbones across multiple downstream tasks, enabling more expressive and transferable graph token representations."}
{"paperId": "9afa18cea10809fe94f99c5b8b287a5f83b4644d", "url": "https://www.semanticscholar.org/paper/9afa18cea10809fe94f99c5b8b287a5f83b4644d", "title": "Integrating Visual Context Into Language Models for Situated Social Conversation Starters", "venue": "IEEE Transactions on Affective Computing", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAFFC.2024.3428704?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAFFC.2024.3428704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2159680523", "name": "R. Janssens"}, {"authorId": "88728223", "name": "Pieter Wolfert"}, {"authorId": "2287896215", "name": "Thomas Demeester"}, {"authorId": "2268680255", "name": "Tony Belpaeme"}], "abstract": "Embodied conversational agents that interact socially with people in the physical world require multi-modal capabilities, such as appropriately responding to visual features of users. While existing vision-and-language models can generate language based on visual input, this language is not situated in a social interaction in the physical world. We present a novel task called Visual Conversation Starters, where an agent generates a conversation-starting question referring to features visible in an image of the user. We collect a dataset of 4000 images of people with 12000 crowdsourced conversation starters, compare various model architectures: fine-tuning smaller seq2seq or image-to-text models versus zero-shot prompting of GPT-3.5, using image captions versus end-to-end image input, training on human data versus synthetic questions generated by GPT-3.5. Models were used to generate friendly conversation starters which were evaluated on criteria including language fluency, visual grounding, interestingness, politeness. Results show that GPT-3.5 generates more interesting, polite questions than smaller models that are fine-tuned on crowdsourced data, but vision-to-language models are better at referencing visual features, they can mimick GPT-3.5's performance. This demonstrates the feasibility of deep visiolinguistic models for situated social agents, forming an important first stage in creating situated multimodal social interaction."}
{"paperId": "9b0bfa57789059ba8646784587694385037774c1", "url": "https://www.semanticscholar.org/paper/9b0bfa57789059ba8646784587694385037774c1", "title": "X-VMamba: Explainable Vision Mamba", "venue": "", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-16", "authors": [{"authorId": "2293395937", "name": "M. Mabrok"}, {"authorId": "2373259858", "name": "Yalda Zafari"}], "abstract": "State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication"}
{"paperId": "9b0cb437c8a10b03f7dd14e8cf96758f38505704", "url": "https://www.semanticscholar.org/paper/9b0cb437c8a10b03f7dd14e8cf96758f38505704", "title": "Does Combining Parameter‐Efficient Modules Improve Few‐Shot Transfer Accuracy?", "venue": "Artificial Intelligence for Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1049/aie2.12002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/aie2.12002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-11", "authors": [{"authorId": "2282542700", "name": "Nader Asadi"}, {"authorId": "2171758288", "name": "Mahdi Beitollahi"}, {"authorId": "2282538005", "name": "Yasser H. Khalil"}, {"authorId": "2286593306", "name": "Yinchuan Li"}, {"authorId": "2265579642", "name": "Guojun Zhang"}, {"authorId": "2257714474", "name": "Xi Chen"}], "abstract": "Parameter‐efficient fine‐tuning has become a key technique for adapting large language and vision models to diverse downstream tasks, including emerging applications in engineering, such as design optimisation, fault detection in complex systems, material property prediction and autonomous robotics control. Specifically, the efficiency of low‐rank adaptation has facilitated the creation and sharing of hundreds of custom low‐rank adaptation (LoRA) modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre‐trained modules enhances generalisation to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Through experiments on both vision and language models, we demonstrate that in few‐shot settings—common in many engineering workflows such as structural design, material discovery and system diagnostics—both composition methods consistently outperform full fine‐tuning and training LoRA from scratch. Moreover, in full‐shot settings, learned composition matches the performance of standard LoRA training while requiring significantly fewer trainable parameters. These results highlight the potential of compositional parameter‐efficient adaptation as a scalable and flexible approach for AI‐driven engineering tasks, enabling faster deployment, reduced data requirements and improved model reusability across diverse domains."}
{"paperId": "9ba23f971bd2273f3ffbf09d92dab9700bd8ced3", "url": "https://www.semanticscholar.org/paper/9ba23f971bd2273f3ffbf09d92dab9700bd8ced3", "title": "RandLoRA: Full-rank parameter-efficient fine-tuning of large models", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 17, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-03", "authors": [{"authorId": "2309482483", "name": "Paul Albert"}, {"authorId": "2309661271", "name": "Frederic Z. Zhang"}, {"authorId": "101834772", "name": "Hemanth Saratchandran"}, {"authorId": "145112187", "name": "Cristian Rodriguez-Opazo"}, {"authorId": "5546141", "name": "Anton van den Hengel"}, {"authorId": "8088602", "name": "Ehsan Abbasnejad"}], "abstract": "Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. The low-rank nature of the weight update inherently limits the representation power of fine-tuned models, however, thus potentially compromising performance on complex tasks. This raises a critical question: when a performance gap between LoRA and standard fine-tuning is observed, is it due to the reduced number of trainable parameters or the rank deficiency? This paper aims to answer this question by introducing RandLoRA, a parameter-efficient method that performs full-rank updates using a learned linear combinations of low-rank, non-trainable random matrices. Our method limits the number of trainable parameters by restricting optimization to diagonal scaling matrices applied to the fixed random matrices. This allows us to effectively overcome the low-rank limitations while maintaining parameter and memory efficiency during training. Through extensive experimentation across vision, language, and vision-language benchmarks, we systematically evaluate the limitations of LoRA and existing random basis methods. Our findings reveal that full-rank updates are beneficial across vision and language tasks individually, and even more so for vision-language tasks, where RandLoRA significantly reduces -- and sometimes eliminates -- the performance gap between standard fine-tuning and LoRA, demonstrating its efficacy."}
{"paperId": "9bd8b95e2106ba71e1bfefe6ba3f9ad25015204b", "url": "https://www.semanticscholar.org/paper/9bd8b95e2106ba71e1bfefe6ba3f9ad25015204b", "title": "DiffStyleTS: Diffusion Model for Style Transfer in Time Series", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.11335, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-13", "authors": [{"authorId": "1412683126", "name": "Mayank Nagda"}, {"authorId": "2105018040", "name": "Phil Ostheimer"}, {"authorId": "2225982850", "name": "Justus Arweiler"}, {"authorId": "2385477250", "name": "Indra Jungjohann"}, {"authorId": "2367414747", "name": "Jennifer Werner"}, {"authorId": "2211338674", "name": "Dennis Wagner"}, {"authorId": "3432403", "name": "Aparna Muraleedharan"}, {"authorId": "2385477733", "name": "Pouya Jafari"}, {"authorId": "2291140762", "name": "Jochen Schmid"}, {"authorId": "92981587", "name": "Fabian Jirasek"}, {"authorId": "71757539", "name": "J. Burger"}, {"authorId": "145664450", "name": "M. Bortz"}, {"authorId": "2239414431", "name": "Hans Hasse"}, {"authorId": "2258707737", "name": "Stephan Mandt"}, {"authorId": "2265771154", "name": "Marius Kloft"}, {"authorId": "2161485495", "name": "Sophie Fellenz"}], "abstract": "Style transfer combines the content of one signal with the style of another. It supports applications such as data augmentation and scenario simulation, helping machine learning models generalize in data-scarce domains. While well developed in vision and language, style transfer methods for time series data remain limited. We introduce DiffTSST, a diffusion-based framework that disentangles a time series into content and style representations via convolutional encoders and recombines them through a self-supervised attention-based diffusion process. At inference, encoders extract content and style from two distinct series, enabling conditional generation of novel samples to achieve style transfer. We demonstrate both qualitatively and quantitatively that DiffTSST achieves effective style transfer. We further validate its real-world utility by showing that data augmentation with DiffTSST improves anomaly detection in data-scarce regimes."}
{"paperId": "9beafcd2820627174fe9742935c2ea438eedb056", "url": "https://www.semanticscholar.org/paper/9beafcd2820627174fe9742935c2ea438eedb056", "title": "TS-OOD: Evaluating Time-Series Out-of-Distribution Detection and Prospective Directions for Progress", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-21", "authors": [{"authorId": "121803963", "name": "Onat Güngör"}, {"authorId": "2334881107", "name": "Amanda Rios"}, {"authorId": "2346984698", "name": "Nilesh Ahuja"}, {"authorId": "2338276247", "name": "Tajana Simunic"}], "abstract": "Detecting out-of-distribution (OOD) data is a fundamental challenge in the deployment of machine learning models. From a security standpoint, this is particularly important because OOD test data can result in misleadingly confident yet erroneous predictions, which undermine the reliability of the deployed model. Although numerous models for OOD detection have been developed in computer vision and language, their adaptability to the time-series data domain remains limited and under-explored. Yet, time-series data is ubiquitous across manufacturing and security applications for which OOD is essential. This paper seeks to address this research gap by conducting a comprehensive analysis of modality-agnostic OOD detection algorithms. We evaluate over several multivariate time-series datasets, deep learning architectures, time-series specific data augmentations, and loss functions. Our results demonstrate that: 1) the majority of state-of-the-art OOD methods exhibit limited performance on time-series data, and 2) OOD methods based on deep feature modeling may offer greater advantages for time-series OOD detection, highlighting a promising direction for future time-series OOD detection algorithm development."}
{"paperId": "9c1c299f49d29919572c4456cc90c3047adb3c90", "url": "https://www.semanticscholar.org/paper/9c1c299f49d29919572c4456cc90c3047adb3c90", "title": "Can Hallucination Correction Improve Video-Language Alignment?", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-20", "authors": [{"authorId": "2261693792", "name": "Lingjun Zhao"}, {"authorId": "2347711060", "name": "Mingyang Xie"}, {"authorId": "2346834677", "name": "Paola Cascante-Bonilla"}, {"authorId": "2344615259", "name": "Hal Daum'e"}, {"authorId": "2311411529", "name": "Kwonjoon Lee"}], "abstract": "Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model's ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment."}
{"paperId": "9c28bba30a74c1620f280ee5864a77d27a80f139", "url": "https://www.semanticscholar.org/paper/9c28bba30a74c1620f280ee5864a77d27a80f139", "title": "A tutorial note on collecting simulated data for vision-language-action models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.06547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-06", "authors": [{"authorId": "2375392440", "name": "Heran Wu"}, {"authorId": "2375865005", "name": "Zirun Zhou"}, {"authorId": "2375395616", "name": "Jingfeng Zhang"}], "abstract": "Traditional robotic systems typically decompose intelligence into independent modules for computer vision, natural language processing, and motion control. Vision-Language-Action (VLA) models fundamentally transform this approach by employing a single neural network that can simultaneously process visual observations, understand human instructions, and directly output robot actions -- all within a unified framework. However, these systems are highly dependent on high-quality training datasets that can capture the complex relationships between visual observations, language instructions, and robotic actions. This tutorial reviews three representative systems: the PyBullet simulation framework for flexible customized data generation, the LIBERO benchmark suite for standardized task definition and evaluation, and the RT-X dataset collection for large-scale multi-robot data acquisition. We demonstrated dataset generation approaches in PyBullet simulation and customized data collection within LIBERO, and provide an overview of the characteristics and roles of the RT-X dataset for large-scale multi-robot data acquisition."}
{"paperId": "9c3d7690050f3cff1098596f2c51b60338b708bb", "url": "https://www.semanticscholar.org/paper/9c3d7690050f3cff1098596f2c51b60338b708bb", "title": "Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.19266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2387114366", "name": "Penghao Wang"}, {"authorId": "2350431958", "name": "Yuhao Zhou"}, {"authorId": "2303461457", "name": "Mengxuan Wu"}, {"authorId": "2240565618", "name": "Panpan Zhang"}, {"authorId": "2333835086", "name": "Zhangyang Wang"}, {"authorId": "2258699561", "name": "Kai Wang"}], "abstract": "State-space models (SSMs) have emerged as efficient alternatives to Transformers for sequence modeling, offering superior scalability through recurrent structures. However, their training remains costly and the ecosystem around them is far less mature than that of Transformers. Moreover, the structural heterogeneity between SSMs and Transformers makes it challenging to efficiently distill knowledge from pretrained attention models. In this work, we propose Cross-architecture distillation via Attention Bridge (CAB), a novel data-efficient distillation framework that efficiently transfers attention knowledge from Transformer teachers to state-space student models. Unlike conventional knowledge distillation that transfers knowledge only at the output level, CAB enables token-level supervision via a lightweight bridge and flexible layer-wise alignment, improving both efficiency and transferability. We further introduce flexible layer-wise alignment strategies to accommodate architectural discrepancies between teacher and student. Extensive experiments across vision and language domains demonstrate that our method consistently improves the performance of state-space models, even under limited training data, outperforming both standard and cross-architecture distillation methods. Our findings suggest that attention-based knowledge can be efficiently transferred to recurrent models, enabling rapid utilization of Transformer expertise for building a stronger SSM community."}
{"paperId": "9c6b6c86ddd4bca3d5f6ef728665fb6efd25e8da", "url": "https://www.semanticscholar.org/paper/9c6b6c86ddd4bca3d5f6ef728665fb6efd25e8da", "title": "AI-Based Performance Analysis in TV Sports Programs", "venue": "International Conference on Multimodal Interaction", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICMI65310.2025.11141120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICMI65310.2025.11141120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-04-05", "authors": [{"authorId": "2379857844", "name": "Kadem Eidhan Shadid"}, {"authorId": "2379858097", "name": "Suhad Hamid Jassim"}], "abstract": "In this study, we examine the use of artificial intelligence (AI) technologies in televised sports programmers to facilitate real-time performance analysis and improve audience engagement. Sport broadcasting is mainly dependent on the way of providing manual comments and post-game analyses, where results are frequently without any immediacy as well as impartiality. To ameliorate this project proposes a unique multimodal AI driven methodology that utilizes computer vision, natural language generation, and audio-visual sync to dissect key performance metrics such as player movements, tempo, and emotional signals of players. “Leveraging over 150 hours of annotated datasets across football, basketball, and tennis, this study presents a lowerror convolutional neural networks to detect events, track players from sports broadcasts and investigate the sentiment of commentary using attention-based bidirectional LSTM models. The experimental outcomes show high accuracy for event detection with 96 % for goals, as well as strong contextual agreement between AI-generated insights and commentary provided by humans. The findings highlight how AI can help automate and enhance the live sports viewing experience, providing another leap in intelligent sports media production."}
{"paperId": "9cb49898b30ce36617b2655e27351660b7da6c79", "url": "https://www.semanticscholar.org/paper/9cb49898b30ce36617b2655e27351660b7da6c79", "title": "DynamicEarth: How Far are We from Open-Vocabulary Change Detection?", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.12931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-22", "authors": [{"authorId": "2269474675", "name": "Kaiyu Li"}, {"authorId": "2295746912", "name": "Xiangyong Cao"}, {"authorId": "2300521352", "name": "Yupeng Deng"}, {"authorId": "2341523381", "name": "Chao Pang"}, {"authorId": "2341528255", "name": "Zepeng Xin"}, {"authorId": "2286899116", "name": "Deyu Meng"}, {"authorId": "2312340604", "name": "Zhi Wang"}], "abstract": "Monitoring Earth's evolving land covers requires methods capable of detecting changes across a wide range of categories and contexts. Existing change detection methods are hindered by their dependency on predefined classes, reducing their effectiveness in open-world applications. To address this issue, we introduce open-vocabulary change detection (OVCD), a novel task that bridges vision and language to detect changes across any category. Considering the lack of high-quality data and annotation, we propose two training-free frameworks, M-C-I and I-M-C, which leverage and integrate off-the-shelf foundation models for the OVCD task. The insight behind the M-C-I framework is to discover all potential changes and then classify these changes, while the insight of I-M-C framework is to identify all targets of interest and then determine whether their states have changed. Based on these two frameworks, we instantiate to obtain several methods, e.g., SAM-DINOv2-SegEarth-OV, Grounding-DINO-SAM2-DINO, etc. Extensive evaluations on 5 benchmark datasets demonstrate the superior generalization and robustness of our OVCD methods over existing supervised and unsupervised methods. To support continued exploration, we release DynamicEarth, a dedicated codebase designed to advance research and application of OVCD. https://likyoo.github.io/DynamicEarth"}
{"paperId": "9e604abe35b10b0c6289806c51ec6255b79c0420", "url": "https://www.semanticscholar.org/paper/9e604abe35b10b0c6289806c51ec6255b79c0420", "title": "Investigating the role of modality and training objective on representational alignment between transformers and the brain", "venue": "UniReps", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2376055658", "name": "Hyewon Willow Han"}, {"authorId": "2311888638", "name": "Ruchira Dhar"}, {"authorId": "2373690517", "name": "Qingqing Yang"}, {"authorId": "2376336064", "name": "Maryam Hoseini Behbahani"}, {"authorId": "2376337786", "name": "María Alejandra Martínez Ortiz"}, {"authorId": "2323951586", "name": "T. S. Oladele"}, {"authorId": "2323148155", "name": "Diana C. Dima"}, {"authorId": "2376360131", "name": "Hsin-Hung Li"}, {"authorId": "2288155393", "name": "Anders Søgaard"}, {"authorId": "2265760748", "name": "Yalda Mohsenzadeh"}], "abstract": null}
{"paperId": "9e632d19a47d90bc811a00be4efa53d07f49467c", "url": "https://www.semanticscholar.org/paper/9e632d19a47d90bc811a00be4efa53d07f49467c", "title": "Multimodal Pretrained Knowledge for Real-world Object Navigation", "venue": "Machine Intelligence Research", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11633-024-1537-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11633-024-1537-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-26", "authors": [{"authorId": "2370771457", "name": "Hui Yuan"}, {"authorId": "2369163799", "name": "Yan Huang"}, {"authorId": "2242656814", "name": "Naigong Yu"}, {"authorId": "2296680364", "name": "Dongbo Zhang"}, {"authorId": "2335167086", "name": "Zetao Du"}, {"authorId": "2376170389", "name": "Ziqi Liu"}, {"authorId": "2369542233", "name": "Kun Zhang"}], "abstract": null}
{"paperId": "9ea4be11e261605d9aa463621adb427bbaff1100", "url": "https://www.semanticscholar.org/paper/9ea4be11e261605d9aa463621adb427bbaff1100", "title": "Attention-Only Transformers via Unrolled Subspace Denoising", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.03790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-04", "authors": [{"authorId": "2306059983", "name": "Peng Wang"}, {"authorId": "2337543715", "name": "Yifu Lu"}, {"authorId": "2142173500", "name": "Yaodong Yu"}, {"authorId": "2167026315", "name": "Druv Pai"}, {"authorId": "2304551969", "name": "Qing Qu"}, {"authorId": "2319861048", "name": "Yi Ma"}], "abstract": "Despite the popularity of transformers in practice, their architectures are empirically designed and neither mathematically justified nor interpretable. Moreover, as indicated by many empirical studies, some components of transformer architectures may be redundant. To derive a fully interpretable transformer architecture with only necessary components, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. To compress these noisy token representations, an associated denoising operation naturally takes the form of a multi-head (subspace) self-attention. By unrolling such iterative denoising operations into a deep network, we arrive at a highly compact architecture that consists of \\textit{only} self-attention operators with skip connections at each layer. Moreover, we show that each layer performs highly efficient denoising: it improves the signal-to-noise ratio of token representations \\textit{at a linear rate} with respect to the number of layers. Despite its simplicity, extensive experiments on vision and language tasks demonstrate that such a transformer achieves performance close to that of standard transformer architectures such as GPT-2 and CRATE."}
{"paperId": "9ea9edb04cc07dd877df6848ff612f02f8b8bc69", "url": "https://www.semanticscholar.org/paper/9ea9edb04cc07dd877df6848ff612f02f8b8bc69", "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2116899991", "name": "Juan A. Rodriguez"}, {"authorId": "2315848755", "name": "Haotian Zhang"}, {"authorId": "2310436656", "name": "Abhay Puri"}, {"authorId": "1962954677", "name": "Aarash Feizi"}, {"authorId": "2139439332", "name": "Rishav Pramanik"}, {"authorId": "2363570017", "name": "Pascal Wichmann"}, {"authorId": "2363570265", "name": "Arnab Mondal"}, {"authorId": "88739822", "name": "Mohammad Reza Samsami"}, {"authorId": "66736108", "name": "Rabiul Awal"}, {"authorId": "1784150", "name": "Perouz Taslakian"}, {"authorId": "2921001", "name": "Spandana Gella"}, {"authorId": "1818842", "name": "Sai Rajeswar"}, {"authorId": "2275588082", "name": "David Vázquez"}, {"authorId": "2275240361", "name": "Christopher Pal"}, {"authorId": "2352279684", "name": "Marco Pedersoli"}], "abstract": "Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF (Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization."}
{"paperId": "9f28c30a78742942b5b4e7f4330286ab1ed5969e", "url": "https://www.semanticscholar.org/paper/9f28c30a78742942b5b4e7f4330286ab1ed5969e", "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-06", "authors": [{"authorId": "2259387950", "name": "Haonan Wang"}, {"authorId": "8359343", "name": "Jiaji Mao"}, {"authorId": "2243356280", "name": "Lehan Wang"}, {"authorId": "2112199915", "name": "Qixiang Zhang"}, {"authorId": "2158113368", "name": "Marawan Elbatel"}, {"authorId": "2281415764", "name": "Yi Qin"}, {"authorId": "2359257426", "name": "Huijun Hu"}, {"authorId": "2272495133", "name": "Baoxun Li"}, {"authorId": "2357285299", "name": "Wenhui Deng"}, {"authorId": "2332561763", "name": "Weifeng Qin"}, {"authorId": "2359264967", "name": "Hongrui Li"}, {"authorId": "2359399030", "name": "Jialin Liang"}, {"authorId": "2322619807", "name": "Jun Shen"}, {"authorId": "2259623185", "name": "Xiaomeng Li"}], "abstract": "Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare."}
{"paperId": "9f57afae632bf9b64fae3847881399b2d097a65b", "url": "https://www.semanticscholar.org/paper/9f57afae632bf9b64fae3847881399b2d097a65b", "title": "Evaluation of Multimodal Image and Text Processing Models from an Uncertainty Perspective", "venue": "Pattern Recognition and Image Analysis", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1134/S1054661825700166?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1134/S1054661825700166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2386752770", "name": "V. V. Kostyumov"}, {"authorId": "2215722629", "name": "B. M. Nutfullin"}, {"authorId": "2215722519", "name": "O. G. Pilipenko"}], "abstract": null}
{"paperId": "9fdacdeeed1100184ae4ab77c4c02ac082589430", "url": "https://www.semanticscholar.org/paper/9fdacdeeed1100184ae4ab77c4c02ac082589430", "title": "Closed-Form Robustness Bounds for Second-Order Pruning of Neural Controller Policies", "venue": "Proceedings of the Institute of Applied Mathematics and Mechanics NAS of Ukraine", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.02953, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-29", "authors": [{"authorId": "2302559629", "name": "Maksym Shamrai"}], "abstract": "Deep neural policies have unlocked agile flight for quadcopters, adaptive grasping for manipulators, and reliable navigation for ground robots, yet their millions of weights conflict with the tight memory and real-time constraints of embedded microcontrollers.\nSecond-order pruning methods -- \\emph{Optimal Brain Damage} (OBD) and its\nvariants, including \\emph{Optimal Brain Surgeon} (OBS) and the recent\n\\textsc{SparseGPT} -- compress networks in a single pass by leveraging the\nlocal Hessian, achieving far higher sparsity than magnitude\nthresholding. Despite their success in vision and language, the\nconsequences of such weight removal on \\emph{closed-loop} stability,\ntracking accuracy, and safety have remained unclear.\nWe present the first mathematically rigorous robustness analysis of\nsecond-order pruning in nonlinear discrete-time control.\nThe system evolves under a continuous transition map\n\\(f:X\\times U\\!\\to\\!X\\), while the controller is an\n\\(L\\)-layer multilayer perceptron with ReLU-type activations that are\n\\emph{globally} \\(1\\)-Lipschitz. Pruning the weight matrix of layer\n\\(k\\) replaces \\(W_k\\) with \\(W_k+\\delta W_k\\), producing the perturbed\nparameter vector \\(\\widehat{\\Theta}=\\Theta+\\delta\\Theta\\) and the pruned\npolicy \\(\\pi(\\cdot;\\widehat{\\Theta})\\).\nFor every input state \\(s\\in X\\) we derive the closed-form inequality\n\\(\n\\|\\pi(s;\\Theta)-\\pi(s;\\widehat{\\Theta})\\|_2\n\\le\nC_k(s)\\,\\|\\delta W_k\\|_2,\n\\) where the constant \\(C_k(s)\\) depends \\emph{only} on unpruned spectral norms and biases, and can be\nevaluated in closed form from a single forward pass.\nWhen several layers \\(S\\subseteq\\{1,\\dots,L\\}\\) are pruned we show the\ndeviations add \\emph{linearly},\n\\(\n\\textstyle \\sum_{k\\in S} C_k(s)\\|\\delta W_k\\|_2,\n\\)\nyielding an explicit state-dependent or worst-case robustness estimation.\nAll constants are computed \\emph{offline}. Therefore, environment roll-outs, back-propagation, or hyper-parameter tuning are \\emph{not} necessary, what makes the framework directly usable inside compression pipelines.\nThe derived bounds specify, prior to field deployment, the maximal admissible pruning magnitude compatible with a prescribed control-error threshold.\nBy linking second-order network compression with closed-loop performance guarantees, our work narrows a crucial gap between modern deep-learning tooling and the robustness demands of safety-critical autonomous systems."}
{"paperId": "a024d96393eb5179824f2867c559769529535299", "url": "https://www.semanticscholar.org/paper/a024d96393eb5179824f2867c559769529535299", "title": "Counterfactual Segmentation Reasoning: Diagnosing and Mitigating Pixel-Grounding Hallucination", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.21546, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-06-26", "authors": [{"authorId": "2336119887", "name": "Xinzhuo Li"}, {"authorId": "51935721", "name": "Adheesh Juvekar"}, {"authorId": "2371104306", "name": "Xingyou Liu"}, {"authorId": "2004145852", "name": "Muntasir Wahed"}, {"authorId": "2299298657", "name": "Kiet A. Nguyen"}, {"authorId": "2099420", "name": "Ismini Lourentzou"}], "abstract": "Segmentation Vision-Language Models (VLMs) have significantly advanced grounded visual understanding, yet they remain prone to pixel-grounding hallucinations, producing masks for incorrect objects or for objects that are entirely absent. Existing evaluations rely almost entirely on text- or label-based perturbations, which check only whether the predicted mask matches the queried label. Such evaluations overlook the spatial footprint and severity of hallucination and therefore fail to reveal vision-driven hallucinations, which are more challenging and more prevalent. To address this gap, we formalize the task of Counterfactual Segmentation Reasoning (CSR), where a model must segment the referenced object in the factual image and abstain in its counterfactual counterpart. To support this task, we curate HalluSegBench, the first large-scale benchmark to diagnose referring and reasoning expression segmentation hallucinations using controlled visual counterfactuals, alongside new evaluation metrics that measure hallucination severity and disentangle vision- and language-driven failure modes. We further introduce RobustSeg, a segmentation VLM trained with counterfactual fine-tuning (CFT) to learn when to segment and when to abstain. Experimental results confirm RobustSeg reduces hallucinations by 30%, while improving segmentation performance on FP-RefCOCO(+/g)."}
{"paperId": "a15c6cca4bd6a3a433962442918831117fafab98", "url": "https://www.semanticscholar.org/paper/a15c6cca4bd6a3a433962442918831117fafab98", "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-11", "authors": [{"authorId": "2047122977", "name": "Ron Yosef"}, {"authorId": "2270672017", "name": "Moran Yanuka"}, {"authorId": "1938499056", "name": "Yonatan Bitton"}, {"authorId": "2263229794", "name": "Dani Lischinski"}], "abstract": "Text-guided image editing, fueled by recent advancements in generative AI, is becoming increasingly widespread. This trend highlights the need for a comprehensive framework to verify text-guided edits and assess their quality. To address this need, we introduce EditInspector, a novel benchmark for evaluation of text-guided image edits, based on human annotations collected using an extensive template for edit verification. We leverage EditInspector to evaluate the performance of state-of-the-art (SoTA) vision and language models in assessing edits across various dimensions, including accuracy, artifact detection, visual quality, seamless integration with the image scene, adherence to common sense, and the ability to describe edit-induced changes. Our findings indicate that current models struggle to evaluate edits comprehensively and frequently hallucinate when describing the changes. To address these challenges, we propose two novel methods that outperform SoTA models in both artifact detection and difference caption generation."}
{"paperId": "a2884dc4208c2fba6be2430175cb7db070ebd6a6", "url": "https://www.semanticscholar.org/paper/a2884dc4208c2fba6be2430175cb7db070ebd6a6", "title": "Dynamic Reflections: Probing Video Representations with Text Alignment", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02767, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-04", "authors": [{"authorId": "2392602473", "name": "Tyler Zhu"}, {"authorId": "22237490", "name": "Tengda Han"}, {"authorId": "2390492910", "name": "Leonidas Guibas"}, {"authorId": "1756112", "name": "Viorica Patraucean"}, {"authorId": "1690177", "name": "M. Ovsjanikov"}], "abstract": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/"}
{"paperId": "a29fe8d2070560fef50cdaff5273d8b0c96f86a8", "url": "https://www.semanticscholar.org/paper/a29fe8d2070560fef50cdaff5273d8b0c96f86a8", "title": "Clinical Dementia Rating Classification Using Integrated Vision and Language Information", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3624215?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3624215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2228119719", "name": "Joonhyeok Yoon"}, {"authorId": "2381067131", "name": "Hangyeol Park"}, {"authorId": "2117956113", "name": "Minjun Kim"}, {"authorId": "1804955582", "name": "Hwihun Jeong"}, {"authorId": "2353269499", "name": "Se Young Chun"}, {"authorId": "116758650", "name": "Sooyeon Ji"}, {"authorId": "2251751411", "name": "Jongho Lee"}], "abstract": "The diagnosis and severity assessment of Alzheimer’s disease (AD) is a complex procedure that requires clinicians to perform a comprehensive review of multiple factors, including biomarkers, physical and neurological examinations, and other relevant data. This suggests that the performance of machine learning algorithms for AD evaluation might exhibit limited performance if they rely on single or limited information. Recently, with advancements in deep learning, several studies have been conducted to assess AD severity by combining MRI and a few tabular-form data. However, no study tried to integrate imaging data with language-form of information. Recent progresses in large language models and multimodal approaches have opened a new window for integrated data processing of vision and language information. In this study, we experimented the use of these multimodal capabilities for the evaluation of AD severity via clinical dementia rating (CDR). For the inputs of the multimodal neural network, both sentences containing various clinical information (e.g., neurological exam results, demographic and diagnostic information) and 3D MRI image were utilized. The text information was generated from tabular data using GPT-4o to mimic the condition in which natural language information is available. The results demonstrated that information embedded in sentences was effectively integrated with MRI information, producing statistically significant performance gains (9 out of 19 input conditions) when compared to text-only input. This work demonstrated potentials of natural language information and imaging data as the synergistic inputs for disease evaluation."}
{"paperId": "a2a3a7425f711456b0e923d8c40db51463d9b1d2", "url": "https://www.semanticscholar.org/paper/a2a3a7425f711456b0e923d8c40db51463d9b1d2", "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.05478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-07", "authors": [{"authorId": "2367272255", "name": "Malak Mansour"}, {"authorId": "2339666771", "name": "Ahmed Aly"}, {"authorId": "2288736422", "name": "Bahey Tharwat"}, {"authorId": "2339667075", "name": "Sarim Hashmi"}, {"authorId": "2339668329", "name": "Dong An"}, {"authorId": "2335448567", "name": "Ian Reid"}], "abstract": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications."}
{"paperId": "a2d8e9dd7b35cd2cca243335635e29ee17b66578", "url": "https://www.semanticscholar.org/paper/a2d8e9dd7b35cd2cca243335635e29ee17b66578", "title": "Integrating vision and language: a novel approach to translation for low-resource Indic languages", "venue": "Asia Pacific Translation and Intercultural Studies", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/23306343.2025.2585418?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/23306343.2025.2585418, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-03", "authors": [{"authorId": "153769552", "name": "S. Chauhan"}, {"authorId": "2058369155", "name": "Shefali Saxena"}, {"authorId": "2116971410", "name": "Shweta Jain"}], "abstract": null}
{"paperId": "a3227c861f52168a94bbb307d815ec410e37d230", "url": "https://www.semanticscholar.org/paper/a3227c861f52168a94bbb307d815ec410e37d230", "title": "Ultrafast neuromorphic computing with nanophotonic optical parametric oscillators", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.16604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-01-28", "authors": [{"authorId": "13933779", "name": "M. Parto"}, {"authorId": "2303406984", "name": "G. Li"}, {"authorId": "2132696872", "name": "Ryoto Sekine"}, {"authorId": "79648232", "name": "R. Gray"}, {"authorId": "145986119", "name": "Luis Ledezma"}, {"authorId": "2265943933", "name": "James A. Williams"}, {"authorId": "40846320", "name": "Arkadev Roy"}, {"authorId": "2238098200", "name": "Alireza Marandi"}], "abstract": "Over the past decade, artificial intelligence (AI) has led to disruptive advancements in fundamental sciences and everyday technologies. Among various machine learning algorithms, deep neural networks have become instrumental in revealing complex patterns in large datasets with key applications in computer vision, natural language processing, and predictive analytics. On-chip photonic neural networks offer a promising platform that leverage high bandwidths and low propagation losses associated with optical signals to perform analog computations for deep learning. However, nanophotonic circuits are yet to achieve the required linear and nonlinear operations simultaneously in an all-optical and ultrafast fashion. Here, we report an ultrafast nanophotonic neuromorphic processor using an optical parametric oscillator (OPO) fabricated on thin-film lithium niobate (TFLN). The input data is used to modulate the optical pulses synchronously pumping the OPO. The consequent signal pulses generated by the OPO are coupled to one another via the nonlinear delayed dynamics of the OPO, thus forming the internal nodes of a deep recurrent neural network. We use such a nonlinearly coupled OPO network for chaotic time series prediction, nonlinear error correction in a noisy communication channel, as well as noisy waveform classification and achieve accuracies exceeding 93% at an operating clock rate of ~ 10 GHz. Our OPO network is capable of achieving sub-nanosecond latencies, a timescale comparable to a single clock cycle in state-of-the-art digital electronic processors. By circumventing the need for optical-electronic-optical (OEO) conversions, our ultrafast nanophotonic neural network paves the way for the next generation of compact all-optical neuromorphic processors with ultralow latencies and high energy efficiencies."}
{"paperId": "a322d335aa373106f9266dc0c3b5a6028f90a3cf", "url": "https://www.semanticscholar.org/paper/a322d335aa373106f9266dc0c3b5a6028f90a3cf", "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "venue": "International Conference on Natural Language and Speech Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.02917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-04", "authors": [{"authorId": "2380388988", "name": "Vebjørn Haug Kåsene"}, {"authorId": "2261081553", "name": "Pierre Lison"}], "abstract": "Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as\"turn left\"or\"move forward\"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task."}
{"paperId": "a329efe099c347b38033581c9c88b329e380ef39", "url": "https://www.semanticscholar.org/paper/a329efe099c347b38033581c9c88b329e380ef39", "title": "Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2298298632", "name": "Kartik Prabhu"}, {"authorId": "2298400938", "name": "Jeffrey Yu"}, {"authorId": "2387895834", "name": "Xinyuan Allen Pan"}, {"authorId": "2318259075", "name": "Zhouhua Xie"}, {"authorId": "2381061178", "name": "Abigail Aleshire"}, {"authorId": "2381349864", "name": "Zihan Chen"}, {"authorId": "2232834356", "name": "A. A. Ratnani"}, {"authorId": "2301018188", "name": "Priyanka Raina"}], "abstract": "While deep neural networks (DNNs) have achieved state-of-the-art performance in fields from computer vision to natural language processing, efficiently running these computationally demanding models requires hardware accelerators. However, designing these accelerators is a time-consuming, labor-intensive process that does not scale well. While prior efforts have sought to automate DNN accelerator generation, they offer limited parameterization, cannot produce high-performance, tapeout-ready designs, provide limited support for datatypes and quantization schemes, and lack an integrated, end-to-end software compiler. This work proposes Voyager, a high-level synthesis (HLS)-based framework for design space exploration (DSE) and generation of DNN accelerators. Voyager overcomes the limitations of prior work by offering extensive configurability across technology nodes, clock frequencies, and scales, with customizable parameters such as number of processing elements, on-chip buffer sizes, and external memory bandwidth. Voyager supports a wider variety of datatypes and quantization schemes versus prior work, including both built-in floating-point, posit and integer formats, as well as user-defined formats with both per-tensor scaling and microscaling quantization. Voyager's PyTorch-based compiler efficiently maps networks end-to-end on the generated hardware, with support for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art vision and language models. Voyager enables fast DSE with full-dataset accuracy evaluation for datatypes and quantization schemes. Generated designs achieve a high utilization across models and scales, up to 99.8%, and outperform prior generators with up to 61% lower latency and 56% lower area. Compared to hand-optimized accelerators, Voyager achieves comparable performance, while offering much greater automation in design and workload mapping."}
{"paperId": "a34de33c691ff8dc20c20545248f381cf0ae0a11", "url": "https://www.semanticscholar.org/paper/a34de33c691ff8dc20c20545248f381cf0ae0a11", "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-10", "authors": [{"authorId": "2109667563", "name": "Zhaoxian Wu"}, {"authorId": "2256992663", "name": "Quan Xiao"}, {"authorId": "2255591487", "name": "Tayfun Gokmen"}, {"authorId": "1689553362", "name": "Omobayode Fagbohungbe"}, {"authorId": "2307264973", "name": "Tianyi Chen"}], "abstract": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights."}
{"paperId": "a4c8b80d74ffd178d08d2f7e16300e8d5db14016", "url": "https://www.semanticscholar.org/paper/a4c8b80d74ffd178d08d2f7e16300e8d5db14016", "title": "Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-20", "authors": [{"authorId": "113705250", "name": "Aiswarya Baby"}, {"authorId": "2346327756", "name": "Tintu Thankom Koshy"}], "abstract": "Visual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer vision and natural language processing, requiring models to understand and reason about visual content in response to natural language questions. Analyzing VQA datasets is essential for developing robust models that can handle the complexities of multimodal reasoning. Several approaches have been developed to examine these datasets, each offering distinct perspectives on question diversity, answer distribution, and visual-textual correlations. Despite significant progress, existing VQA models face challenges related to dataset bias, limited model complexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real world scenarios. This paper offers a detailed study of the original VQA dataset, baseline models and methods along with a comparative study of five advanced VQA models, ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, each employing distinct methods to address these ongoing challenges."}
{"paperId": "a565b2a3a93476b399d86f44ee16bd744524c8ab", "url": "https://www.semanticscholar.org/paper/a565b2a3a93476b399d86f44ee16bd744524c8ab", "title": "Advancing Bridge Infrastructure Management through Artificial Intelligence: A Comprehensive Review", "venue": "International Journal of Bridge Engineering, Management and Research", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.70465/ber.v2i3.45?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.70465/ber.v2i3.45, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-10", "authors": [{"authorId": "2372351713", "name": "Deepak Kumar"}, {"authorId": "2372354704", "name": "Anil K. Agrawal"}], "abstract": "Bridge infrastructure serves as a vital component of global transportation systems, yet its aging condition and exposure to increasing environmental and operational stressors necessitate smarter, faster, and more objective approaches to inspection, deterioration modeling, and maintenance management. Traditional methods often suffer from subjectivity, inefficiency, and data limitations. This comprehensive review explores how recent advancements in Artificial Intelligence (AI), including computer vision, natural language processing, deep learning, predictive modeling, robotics, and large language models (LLMs), are revolutionizing the entire bridge management lifecycle. AI-based systems are examined for automated condition detection and rating, data-driven deterioration forecasting, and maintenance prioritization using multi-modal data inputs. Special emphasis is placed on the LLMs for extracting actionable insights from unstructured inspection records and facilitating automated decision support. In addition, the review covers AI-driven training and quality assurance tools for inspectors and demonstrates the potential of LLM-powered bots for real-time bridge condition communication. By benchmarking these innovations against traditional practices, this paper identifies current capabilities, integration challenges, and future research directions essential for realizing intelligent, sustainable, and scalable bridge infrastructure management."}
{"paperId": "a5a57224c2adf98b334e95abd7259d4af05b1f72", "url": "https://www.semanticscholar.org/paper/a5a57224c2adf98b334e95abd7259d4af05b1f72", "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.01357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-03", "authors": [{"authorId": "24921532", "name": "Qiangguo Jin"}, {"authorId": "2390594420", "name": "Xianyao Zheng"}, {"authorId": "2254262205", "name": "Hui Cui"}, {"authorId": "2255321824", "name": "Changming Sun"}, {"authorId": "2372210280", "name": "Yuqi Fang"}, {"authorId": "2365774629", "name": "Cong Cong"}, {"authorId": "2254308815", "name": "Ran Su"}, {"authorId": "2254904782", "name": "Leyi Wei"}, {"authorId": "2066135805", "name": "Ping Xuan"}, {"authorId": "2365970328", "name": "Junbo Wang"}], "abstract": "Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model's capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at https://github.com/BioMedIA-repo/CMI-MTL."}
{"paperId": "a5e2cbe38dc55b6877e1c30fc7d833c48201a16c", "url": "https://www.semanticscholar.org/paper/a5e2cbe38dc55b6877e1c30fc7d833c48201a16c", "title": "On the Integration of Social Context for Enhanced Fake News Detection Using Multimodal Fusion Attention Mechanism", "venue": "Applied Informatics", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6040078?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6040078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2314649984", "name": "Hachemi Nabil Dellys"}, {"authorId": "2355016169", "name": "Halima Mokeddem"}, {"authorId": "3147672", "name": "Layth Sliman"}], "abstract": "Detecting fake news has become a critical challenge in today’s information-dense society. Existing research on fake news detection predominantly emphasizes multi-modal approaches, focusing primarily on textual and visual features. However, despite its clear importance, the integration of social context has received limited attention in the literature. To address this gap, this study proposes a novel three-dimensional multimodal fusion framework that integrates textual, visual, and social context features for effective fake news detection on social media platforms. The proposed methodology leverages an advanced Vision-and-Language Bidirectional Encoder Representations from Transformers multi-task model to extract fused attention features from text and images concurrently, capturing intricate inter-modal correlations. Comprehensive experiments validate the efficacy of the proposed approach. The results demonstrate that the proposed solution achieves the highest balanced accuracy of 77%, surpassing other baseline models. Furthermore, the incorporation of social context features significantly enhances model performance. The proposed multimodal architecture also outperforms state-of-the-art approaches, providing a robust and scalable framework for fake news detection using artificial intelligence. This study contributes to advancing the field by offering a comprehensive and practical engineering solution for combating fake news."}
{"paperId": "a60aae2bcd4e86fe09696a3be092b2976b0f1763", "url": "https://www.semanticscholar.org/paper/a60aae2bcd4e86fe09696a3be092b2976b0f1763", "title": "Representation Invariance and Allocation: When Subgroup Balance Matters", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.09496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-10", "authors": [{"authorId": "2167911380", "name": "Anissa Alloula"}, {"authorId": "2398911988", "name": "Charles Jones"}, {"authorId": "2397619054", "name": "Zuzanna Wakefield-Skorniewska"}, {"authorId": "2397618569", "name": "Francesco Quinzan"}, {"authorId": "2363571118", "name": "Bartlomiej W. Papie.z"}], "abstract": "Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions."}
{"paperId": "a630e2200d5e9a42b83be086a08262852d112218", "url": "https://www.semanticscholar.org/paper/a630e2200d5e9a42b83be086a08262852d112218", "title": "VLAG: Graph-Based Planning for Vision-Language-Action Models in Long Horizon Manipulation Tasks", "venue": "Design Automation Conference", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1115/detc2025-169527?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1115/detc2025-169527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-17", "authors": [{"authorId": "2384135480", "name": "Ardalan Aryashad"}, {"authorId": "2388920837", "name": "Yan Jin"}], "abstract": "\n We present a novel, modular Graph based Vision-Language-Action (VLAG) framework designed for long-horizon robotic manipulation tasks. Our approach integrates a graph-based planner with dedicated vision, language, and action modules, enabling robust and efficient task planning and execution. The graph planner serves as a high-level decision-making entity that interprets visual observations and language instructions to select appropriate task sequences. Specifically, our framework leverages a vision model with a multi-layer perceptron to extract key environmental features from both RGB and depth images. The language model is fine-tuned from a pre-trained model to enhance instruction-to-task pairing accuracy, thus having reliable and robust task recognition. The action model is built on the Action Chunking with Transformers (ACT) architecture, modified to accommodate the vision and language modalities. The graph planner is crucial to the framework’s functionality as it allows the combination of the strengths of the vision, language, and action modules, leading to a system that is both adaptable and computationally efficient. Overall, VLAG’s modular design enables the flexible integration of its components, providing a scalable solution for robotic manipulation tasks in both seen and unseen environments."}
{"paperId": "a667c46d4a58eefe30cb7eefdd8115f2da30ced2", "url": "https://www.semanticscholar.org/paper/a667c46d4a58eefe30cb7eefdd8115f2da30ced2", "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.19391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-22", "authors": [{"authorId": "2381833215", "name": "Axel Marmoret"}, {"authorId": "2280142949", "name": "Reda Bensaid"}, {"authorId": "2381834242", "name": "Jonathan Lys"}, {"authorId": "2281746434", "name": "Vincent Gripon"}, {"authorId": "1470826088", "name": "Franccois Leduc-Primeau"}], "abstract": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts."}
{"paperId": "a6e6c72f33a7fcd1ac91ba65f9fd40f7f6655cd0", "url": "https://www.semanticscholar.org/paper/a6e6c72f33a7fcd1ac91ba65f9fd40f7f6655cd0", "title": "From Image Captioning to Visual Storytelling", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.14045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-31", "authors": [{"authorId": "2149928155", "name": "Admitos-Rafael Passadakis"}, {"authorId": "2315948339", "name": "Yingjin Song"}, {"authorId": "2315933729", "name": "Albert Gatt"}], "abstract": "Visual Storytelling is a challenging multimodal task between Vision&Language, where the purpose is to generate a story for a stream of images. Its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent. The aim of this work is to balance between these aspects, by treating Visual Storytelling as a superset of Image Captioning, an approach quite different compared to most of prior relevant studies. This means that we firstly employ a vision-to-language model for obtaining captions of the input images, and then, these captions are transformed into coherent narratives using language-to-language methods. Our multifarious evaluation shows that integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. In addition, compared to numerous previous studies, this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested. Lastly, we propose a new metric/tool, named ideality, that can be used to simulate how far some results are from an oracle model, and we apply it to emulate human-likeness in visual storytelling."}
{"paperId": "a792f7cc0e6b8a97149e58fc7412188ca3a804a3", "url": "https://www.semanticscholar.org/paper/a792f7cc0e6b8a97149e58fc7412188ca3a804a3", "title": "Fake Image Detection in Fake News Using Convolutional Neural Network (CNN)", "venue": "International conference Applied Internet and Information Technologies", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIIT63112.2025.11082891?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIIT63112.2025.11082891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-05-07", "authors": [{"authorId": "2216631389", "name": "Linah Alqurashi"}, {"authorId": "2372795545", "name": "Sara AlMuraytib"}, {"authorId": "2372786404", "name": "Rehab K. Qarout"}, {"authorId": "37370925", "name": "Nuha Zamzami"}], "abstract": "’Fake news’ refers to false, inaccurate, or misleading information that spreads as real news. Fake news primarily aims to affect societies and individuals by spreading false or misleading information. Fake news comes in text, images, videos, and audio formats. Detecting fake news is a research challenge that requires a promising solution to assure the accuracy of the information. Recently, promising results have been generated using artificial intelligence to detect fake news in multimedia. Computer vision, natural language processing techniques, audio processing, and machine learning algorithms are all techniques used in artificial intelligence to help accurately differentiate accurate news from fake news. However, The paper proposed a Convolutional Neural Networks (CNNs) model that can detect fake images in the context of fake news. The paper also utilized various preprocessing techniques, including error level analysis (ELA), gradient, and noise analysis. These techniques are compared to discern their effectiveness in enhancing the model’s performance. The result indicates that the proposed CNN model achieved an overall accuracy of 91.38% on the CASIA V2.0 dataset with error level analysis (ELA) as the preprocessing technique. The results indicate that ELA is an effective preprocessing technique for extracting features that can be used to identify fake images."}
{"paperId": "a7d57d471898fa6c5d7b3bc5fc60388e45884532", "url": "https://www.semanticscholar.org/paper/a7d57d471898fa6c5d7b3bc5fc60388e45884532", "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.13742, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-14", "authors": [{"authorId": null, "name": "Md. Najib Hasan"}, {"authorId": null, "name": "Imran Ahmad"}, {"authorId": "2302868188", "name": "S. Shuvo"}, {"authorId": "2381421849", "name": "Mahadi Hasan Ankon"}, {"authorId": null, "name": "Sunanda Das"}, {"authorId": "2398905716", "name": "Nazmul Siddique"}, {"authorId": "2398902347", "name": "Hui Wang Wichita State University"}, {"authorId": "2072133510", "name": "Usa"}, {"authorId": "2398905310", "name": "Khulna University of Engineering"}, {"authorId": "2321564519", "name": "Technology"}, {"authorId": "2187927725", "name": "Bangladesh"}, {"authorId": "102266791", "name": "U. Arkansas"}, {"authorId": "2398905718", "name": "Ulster University"}, {"authorId": "2258971425", "name": "Uk"}, {"authorId": "97162471", "name": "Queen's University of Belfast"}], "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M."}
{"paperId": "a7e1b0409b9173ef325f5de0fce81cf0a08cf2fd", "url": "https://www.semanticscholar.org/paper/a7e1b0409b9173ef325f5de0fce81cf0a08cf2fd", "title": "Bayesian Optimization over Bounded Domains with the Beta Product Kernel", "venue": "Conference on Uncertainty in Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.16316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-19", "authors": [{"authorId": "144691487", "name": "Huy Hoang Nguyen"}, {"authorId": "2257037757", "name": "Han Zhou"}, {"authorId": "2272487345", "name": "M. Blaschko"}, {"authorId": "8796283", "name": "A. Tiulpin"}], "abstract": "Bayesian optimization with Gaussian processes (GP) is commonly used to optimize black-box functions. The Mat\\'ern and the Radial Basis Function (RBF) covariance functions are used frequently, but they do not make any assumptions about the domain of the function, which may limit their applicability in bounded domains. To address the limitation, we introduce the Beta kernel, a non-stationary kernel induced by a product of Beta distribution density functions. Such a formulation allows our kernel to naturally model functions on bounded domains. We present statistical evidence supporting the hypothesis that the kernel exhibits an exponential eigendecay rate, based on empirical analyses of its spectral properties across different settings. Our experimental results demonstrate the robustness of the Beta kernel in modeling functions with optima located near the faces or vertices of the unit hypercube. The experiments show that our kernel consistently outperforms a wide range of kernels, including the well-known Mat\\'ern and RBF, in different problems, including synthetic function optimization and the compression of vision and language models."}
{"paperId": "a89bf1cdcda5488873642a4abef2ccd1b9565582", "url": "https://www.semanticscholar.org/paper/a89bf1cdcda5488873642a4abef2ccd1b9565582", "title": "Language-Conditioned Waypoint Predictor for Continuous Vision-and-Language Navigation", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICME59968.2025.11209085?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICME59968.2025.11209085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2282505189", "name": "Zeyu Wang"}, {"authorId": "2257919485", "name": "Yuankai Qi"}, {"authorId": "2389526209", "name": "Dong An"}, {"authorId": "2383532594", "name": "Xu Yang"}, {"authorId": "2289504170", "name": "Hongxin Li"}, {"authorId": "2391474835", "name": "Zhaoxiang Zhang"}], "abstract": "Waypoint prediction is a popular technique for Vision-and-Language Navigation in Continuous Environments (VLN-CE), which abstracts navigable locations as waypoints to ease the subsequent action prediction. Nevertheless, we found current waypoint predictors are not always accurate, limiting navigation’s overall performance. One possible reason may be the lack of language context, leading to the failure to generate corresponding waypoints for critical locations mentioned in the instructions. To that end, we propose a novel framework to enable the training of the language-conditioned waypoint predictor. First, as the VLN-CE agents ground instructions with the environment when navigating, we employ a pre-trained agent to encode language for the waypoint predictor. Second, the language-conditioned waypoint predictor is trained with the data collected using the same agent. Third, we train the new VLN-CE navigation agent with the proposed waypoint predictor. Fourth, the disparity between the language encoder agent and the navigation agent drives us to devise a cycle training scheme to alternately train the agent and the waypoint predictor, further enhancing the performance of both the waypoint predictor and navigation agent. Experimental results show that our waypoint predictor’s performance surpasses all existing ones. With better waypoints, the gap between waypoint-based methods and their upper bound narrows by about 60%."}
{"paperId": "aa573824ec5f0ce81d96f7cc8edd22149ee56c54", "url": "https://www.semanticscholar.org/paper/aa573824ec5f0ce81d96f7cc8edd22149ee56c54", "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.03958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-03", "authors": [{"authorId": "2349269580", "name": "Xiaobei Zhao"}, {"authorId": "2375388873", "name": "Xingqi Lyu"}, {"authorId": null, "name": "Xin Chen"}, {"authorId": "2349772700", "name": "Xiang Li"}], "abstract": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extended Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on multimodal reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN."}
{"paperId": "aa5856b484f9f3fb33b13a0a9aa93ce5f28b9b93", "url": "https://www.semanticscholar.org/paper/aa5856b484f9f3fb33b13a0a9aa93ce5f28b9b93", "title": "CDIP-ChatGLM3: A dual-model approach integrating computer vision and language modeling for crop disease identification and prescription", "venue": "Computers and Electronics in Agriculture", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.compag.2025.110442?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.compag.2025.110442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2312307003", "name": "Changqing Yan"}, {"authorId": "2292901368", "name": "Zeyun Liang"}, {"authorId": "2317340689", "name": "Han Cheng"}, {"authorId": "2357677461", "name": "Shuyang Li"}, {"authorId": "2355071114", "name": "Guangpeng Yang"}, {"authorId": "2357519892", "name": "Zhiwei Li"}, {"authorId": "2312439846", "name": "Ling Yin"}, {"authorId": "5359905", "name": "Junjie Qu"}, {"authorId": "2357715679", "name": "Jing Wang"}, {"authorId": "1576181562", "name": "Genghong Wu"}, {"authorId": "2312192741", "name": "Qi Tian"}, {"authorId": "2355038834", "name": "Qiang Yu"}, {"authorId": "2312337704", "name": "Gang Zhao"}], "abstract": null}
{"paperId": "aaa6851a0ee315f1eb74d1c1e0c06446458efabd", "url": "https://www.semanticscholar.org/paper/aaa6851a0ee315f1eb74d1c1e0c06446458efabd", "title": "Causal learning with uncertainty-aware transformer for vision-and-language navigation", "venue": "Neurocomputing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neucom.2025.132196?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neucom.2025.132196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-01", "authors": [{"authorId": "2395442150", "name": "Kaijie Zhang"}, {"authorId": "2293777202", "name": "Wanru Xu"}, {"authorId": "2297559416", "name": "Zhenjiang Miao"}, {"authorId": "2254910871", "name": "Yi Tian"}, {"authorId": "2255575201", "name": "Yigang Cen"}, {"authorId": "2395348857", "name": "Yutao Liu"}, {"authorId": "2391122875", "name": "Wangsheng He"}], "abstract": null}
{"paperId": "aaf01bb665d6d0e32c39e30ddaff521eedc7174e", "url": "https://www.semanticscholar.org/paper/aaf01bb665d6d0e32c39e30ddaff521eedc7174e", "title": "Unifying Vision-Language Latents for Zero-label Image Caption Enhancement", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12931, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-14", "authors": [{"authorId": "2329101843", "name": "Sanghyun Byun"}, {"authorId": "2385565657", "name": "J. Guack"}, {"authorId": "37446444", "name": "Mohanad Odema"}, {"authorId": "2385599845", "name": "Baisub Lee"}, {"authorId": "2329134064", "name": "Jacob Song"}, {"authorId": "2329101664", "name": "Woo Seong Chung"}], "abstract": "Vision-language models (VLMs) achieve remarkable performance through large-scale image-text pretraining. However, their reliance on labeled image datasets limits scalability and leaves vast amounts of unlabeled image data underutilized. To address this, we propose Unified Vision-Language Alignment for Zero-Label Enhancement (ViZer), an enhancement training framework that enables zero-label learning in image captioning, providing a practical starting point for broader zero-label adaptation in vision-language tasks. Unlike prior approaches that rely on human or synthetically annotated datasets, ViZer actively aligns vision and language representation features during training, enabling existing VLMs to generate improved captions without requiring text labels or full retraining. We demonstrate ViZer's advantage in qualitative evaluation, as automated caption metrics such as CIDEr and BERTScore often penalize details that are absent in reference captions. Applying ViZer on SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements, producing captions that are more grounded and descriptive than their baseline."}
{"paperId": "ab0a4637d9b7c102863e19a1a644ecbdbeffb2a8", "url": "https://www.semanticscholar.org/paper/ab0a4637d9b7c102863e19a1a644ecbdbeffb2a8", "title": "Towards Distributed Neural Architectures", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.22389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-27", "authors": [{"authorId": "2290016986", "name": "Aditya Cowsik"}, {"authorId": "2257038013", "name": "Tianyu He"}, {"authorId": "2257007969", "name": "Andrey Gromov"}], "abstract": "We introduce and train distributed neural architectures (DNA) in vision and language domains. DNAs are initialized with a proto-architecture that consists of (transformer, MLP, attention, etc.) modules and routers. Any token (or patch) can traverse any series of modules in any order. DNAs are a natural generalization of the sparse methods such as Mixture-of-Experts, Mixture-of-Depths, parameter sharing, etc. Computation and communication patterns of DNA modules are learnt end-to-end during training and depend on the content and context of each token (or patch). These patterns can be shaped by further requirements added to the optimization objective such as compute/memory efficiency or load balancing. We empirically show that (i) trained DNAs are competitive with the dense baselines in both domains and (ii) compute efficiency/parameter sharing can be learnt from data. Next, we analyze the emergent connectivity and computation patterns in the trained DNAs. We find that the paths that tokens take through the models are themselves distributed according to a power-law. We show that some paths (or, equivalently, groups of modules) show emergent specialization. Finally, we demonstrate that models learn to allocate compute and active parameters in an interpretable way."}
{"paperId": "ac62232ea892ea26bf6753d19fe8a1d1d044d0fa", "url": "https://www.semanticscholar.org/paper/ac62232ea892ea26bf6753d19fe8a1d1d044d0fa", "title": "Green AI: Balancing Model Complexity and Energy Footprint in Deep Learning", "venue": "2025 3rd International Conference on Sustainable Computing and Data Communication Systems (ICSCDS)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSCDS65426.2025.11167425?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSCDS65426.2025.11167425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference", "Review"], "publicationDate": "2025-08-06", "authors": [{"authorId": "50999672", "name": "Deepshikha Aggarwal"}, {"authorId": "2256332367", "name": "Deepti Sharma"}, {"authorId": "49984209", "name": "Archana B. Saxena"}], "abstract": "The rapid advancement of deep learning has led to significant breakthroughs in various domains, including computer vision, natural language processing, and autonomous systems. However, the growing complexity and size of these models come with a significant environmental cost. Training state-of-the-art models consumes enormous computational power, contributing to high energy consumption and carbon emissions. This paper explores the paradigm of Green AI, emphasizing the need to balance model complexity and performance with environmental sustainability. We analyse the environmental impact of deep learning models, review recent strategies for energy-efficient AI, and propose a research framework to guide sustainable model development. The study also presents research objectives, a methodology for empirical evaluation, and a discussion of findings aimed at guiding researchers toward greener practices."}
{"paperId": "acd706d9aefc4c5d958cad4c45c5c33c33c97209", "url": "https://www.semanticscholar.org/paper/acd706d9aefc4c5d958cad4c45c5c33c33c97209", "title": "[Artificial intelligence accelerates bridging the gap in intelligent surgery through medical-engineering-mathematical integration].", "venue": "Zhonghua wai ke za zhi [Chinese journal of surgery]", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.3760/cma.j.cn112139-20250814-00403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3760/cma.j.cn112139-20250814-00403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-26", "authors": [{"authorId": "2325490449", "name": "X. W. Wu"}, {"authorId": "2176525642", "name": "C. Fang"}], "abstract": null}
{"paperId": "ace33eec6b610d7ec8ba6562dad75bb82d632b96", "url": "https://www.semanticscholar.org/paper/ace33eec6b610d7ec8ba6562dad75bb82d632b96", "title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-11", "authors": [{"authorId": "2297095867", "name": "Navid Rajabi"}, {"authorId": "2288626103", "name": "J. Kosecka"}], "abstract": "In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance."}
{"paperId": "ad16b50033ec278983b6287b118351758a5ece9f", "url": "https://www.semanticscholar.org/paper/ad16b50033ec278983b6287b118351758a5ece9f", "title": "Large Language Models in Medical Image Analysis: A Systematic Survey and Future Directions", "venue": "Bioengineering", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12383295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-29", "authors": [{"authorId": "2377804169", "name": "Nanyin Zhang"}, {"authorId": "2377772468", "name": "Richard Bayford"}, {"authorId": "2378897869", "name": "Sikandar Ali"}, {"authorId": "2377942058", "name": "Ali Hussain"}, {"authorId": "2160793597", "name": "Bushra Urooj"}, {"authorId": "2287234834", "name": "Muhammad Fayaz"}, {"authorId": "2378819440", "name": "Shafqat Ali"}, {"authorId": "40547902", "name": "L. Dang"}, {"authorId": "2149235495", "name": "K. Kim"}], "abstract": "The integration of vision and language processing into a cohesive system has already shown promise with the application of large language models (LLMs) in medical image analysis. Their capabilities encompass the generation of medical reports, disease classification, visual question answering, and segmentation, providing yet another approach to interpreting multimodal data. This survey aims to compile all known applications of LLMs in the medical image analysis field, spotlighting their promises alongside critical challenges and future avenues. We introduce the concept of X-stage tuning which serves as a framework for LLMs fine-tuning across multiple stages: zero stage, one stage, and multi-stage, wherein each stage corresponds to task complexity and available data. The survey describes issues like sparsity of data, hallucination in outputs, privacy issues, and the requirement for dynamic knowledge updating. Alongside these, we cover prospective features including integration of LLMs with decision support systems, multimodal learning, and federated learning for privacy-preserving model training. The goal of this work is to provide structured guidance to the targeted audience, demystifying the prospects of LLMs in medical image analysis."}
{"paperId": "ad3d2d5036724a642974ef8eec74410914b1a921", "url": "https://www.semanticscholar.org/paper/ad3d2d5036724a642974ef8eec74410914b1a921", "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.10454, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-12", "authors": [{"authorId": "2326442520", "name": "Hang Yin"}, {"authorId": "2380468739", "name": "Haoyu Wei"}, {"authorId": "2158440998", "name": "Xiuwei Xu"}, {"authorId": "2271589673", "name": "Wenxuan Guo"}, {"authorId": "2256790031", "name": "Jie Zhou"}, {"authorId": "2257098405", "name": "Jiwen Lu"}], "abstract": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework."}
{"paperId": "ada8e7e5b8adddb2914f4e88ba92d20f82be1e7e", "url": "https://www.semanticscholar.org/paper/ada8e7e5b8adddb2914f4e88ba92d20f82be1e7e", "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety", "venue": "arXiv.org", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08663, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-11", "authors": [{"authorId": "3142556", "name": "P. Sermanet"}, {"authorId": "2349542174", "name": "Anirudha Majumdar"}, {"authorId": "17818078", "name": "A. Irpan"}, {"authorId": "48313860", "name": "Dmitry Kalashnikov"}, {"authorId": "1808676", "name": "Vikas Sindhwani"}], "abstract": "Until recently, robotics safety research was predominantly about collision avoidance and hazard reduction in the immediate vicinity of a robot. Since the advent of large vision and language models (VLMs), robots are now also capable of higher-level semantic scene understanding and natural language interactions with humans. Despite their known vulnerabilities (e.g. hallucinations or jail-breaking), VLMs are being handed control of robots capable of physical contact with the real world. This can lead to dangerous behaviors, making semantic safety for robots a matter of immediate concern. Our contributions in this paper are two fold: first, to address these emerging risks, we release the ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for evaluating and improving semantic safety of foundation models serving as robot brains. Our data generation recipe is highly scalable: by leveraging text and image generation techniques, we generate undesirable situations from real-world visual scenes and human injury reports from hospitals. Secondly, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot's behavior using Constitutional AI mechanisms. We propose a novel auto-amending process that is able to introduce nuances in written rules of behavior; this can lead to increased alignment with human preferences on behavior desirability and safety. We explore trade-offs between generality and specificity across a diverse set of constitutions of different lengths, and demonstrate that a robot is able to effectively reject unconstitutional actions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. Data is available at asimov-benchmark.github.io"}
{"paperId": "adc33357727f8c9f47614ac0081125806cfb1678", "url": "https://www.semanticscholar.org/paper/adc33357727f8c9f47614ac0081125806cfb1678", "title": "Deep Learning Applications in Imaging of Acute Ischemic Stroke: A Systematic Review and Narrative Summary.", "venue": "Radiology", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.240775?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.240775, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2345439719", "name": "Bin Jiang"}, {"authorId": "2354217822", "name": "Nancy Pham"}, {"authorId": "1557716912", "name": "Eric K van Staalduinen"}, {"authorId": "1604961097", "name": "Yongkai Liu"}, {"authorId": "1398174093", "name": "S. Nazari-Farsani"}, {"authorId": "2251370364", "name": "A. Sanaat"}, {"authorId": "2343153633", "name": "Henk van Voorst"}, {"authorId": "2225991376", "name": "Ates Fettahoglu"}, {"authorId": "2331461514", "name": "Donghoon Kim"}, {"authorId": "35566862", "name": "J. Ouyang"}, {"authorId": "2331795885", "name": "Ashwin Kumar"}, {"authorId": "2283289566", "name": "Aditya Srivatsan"}, {"authorId": "2354212485", "name": "Ramy Hussein"}, {"authorId": "2092499855", "name": "Maarten G. Lansberg"}, {"authorId": "2354217703", "name": "Fernando Boada"}, {"authorId": "2274406020", "name": "G. Zaharchuk"}], "abstract": "Background Acute ischemic stroke (AIS) is a major cause of morbidity and mortality, requiring swift and precise clinical decisions based on neuroimaging. Recent advances in deep learning-based computer vision and language artificial intelligence (AI) models have demonstrated transformative performance for several stroke-related applications. Purpose To evaluate deep learning applications for imaging in AIS in adult patients, providing a comprehensive overview of the current state of the technology and identifying opportunities for advancement. Materials and Methods A systematic literature review was conducted following Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. A comprehensive search of four databases from January 2016 to January 2024 was performed, targeting deep learning applications for imaging of AIS, including automated detection of large vessel occlusion and measurement of Alberta Stroke Program Early CT Score. Articles were selected based on predefined inclusion and exclusion criteria, focusing on convolutional neural networks and transformers. The top-represented areas were addressed, and the relevant information was extracted and summarized. Results Of 380 studies included, 171 (45.0%) focused on stroke lesion segmentation, 129 (33.9%) on classification and triage, 31 (8.2%) on outcome prediction, 15 (3.9%) on generative AI and large language models, and 11 (2.9%) on rapid or low-dose imaging specific to stroke applications. Detailed data extraction was performed for 68 studies. Public AIS datasets are also highlighted, for researchers developing AI models for stroke imaging. Conclusion Deep learning applications have permeated AIS imaging, particularly for stroke lesion segmentation. However, challenges remain, including the need for standardized protocols and test sets, larger public datasets, and performance validation in real-world settings. © RSNA, 2025 Supplemental material is available for this article."}
{"paperId": "ae05d074a764ccdddea8522db15df0754d7bb48d", "url": "https://www.semanticscholar.org/paper/ae05d074a764ccdddea8522db15df0754d7bb48d", "title": "Recursive bidirectional cross-modal reasoning network for vision-and-language navigation", "venue": "Expert systems with applications", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2025.126442?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2025.126442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2280419588", "name": "Jie Wu"}, {"authorId": "46740305", "name": "Chunlei Wu"}, {"authorId": "2201062816", "name": "Xiuxuan Shen"}, {"authorId": "2340107772", "name": "Fengjiang Wu"}, {"authorId": "2250564", "name": "Leiquan Wang"}], "abstract": null}
{"paperId": "ae43452fe8f94559fc4a8a8e61725b22c1366408", "url": "https://www.semanticscholar.org/paper/ae43452fe8f94559fc4a8a8e61725b22c1366408", "title": "TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.00813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-01", "authors": [{"authorId": "2192363522", "name": "Jiaqi Luo"}, {"authorId": "2312437791", "name": "Yuan Yuan"}, {"authorId": "2257007273", "name": "Shixin Xu"}], "abstract": "Tabular-image multimodal learning, which integrates structured tabular data with imaging data, holds great promise for a variety of tasks, especially in medical applications. Yet, two key challenges remain: (1) the lack of a standardized, pretrained representation for tabular data, as is commonly available in vision and language domains; and (2) the difficulty of handling missing values in the tabular modality, which are common in real-world medical datasets. To address these issues, we propose the TabPFN-Integrated Multimodal Engine (TIME), a novel multimodal framework that builds on the recently introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen tabular encoder to generate robust, strong embeddings that are naturally resilient to missing data, and combines them with image features from pretrained vision backbones. We explore a range of fusion strategies and tabular encoders, and evaluate our approach on both natural and medical datasets. Extensive experiments demonstrate that TIME consistently outperforms competitive baselines across both complete and incomplete tabular inputs, underscoring its practical value in real-world multimodal learning scenarios."}
{"paperId": "ae4ba9427654e39d93c32ecf5d3b5162d54e8e17", "url": "https://www.semanticscholar.org/paper/ae4ba9427654e39d93c32ecf5d3b5162d54e8e17", "title": "GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.24340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-30", "authors": [{"authorId": "2279541301", "name": "G. Hacheme"}, {"authorId": "51228744", "name": "G. Tadesse"}, {"authorId": "2273344088", "name": "Caleb Robinson"}, {"authorId": "2279540875", "name": "Akram Zaytar"}, {"authorId": "2266459553", "name": "R. Dodhia"}, {"authorId": "4258401", "name": "J. Ferres"}], "abstract": "Classifying geospatial imagery remains a major bottleneck for applications such as disaster response and land-use monitoring-particularly in regions where annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that claim zero-shot classification capabilities for satellite imagery nonetheless rely on task-specific pretraining and adaptation to reach competitive performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot classification framework: a vision Large Language Model (vLLM) generates rich, human-readable image descriptions, which are then mapped to user-defined classes by a conventional Large Language Model (LLM). This modular, and interpretable pipeline enables flexible image classification for a large range of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced, and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class classification tasks (UC Merced, RESISC45), we implemented a recursive LLM-driven clustering to form meta-classes at successive depths, followed by hierarchical classification-first resolving coarse groups, then finer distinctions-to deliver competitive zero-shot performance. GVL is open-sourced at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in real-world geospatial workflows."}
{"paperId": "ae88fb1e821d647e0e3083269842480a9d41db68", "url": "https://www.semanticscholar.org/paper/ae88fb1e821d647e0e3083269842480a9d41db68", "title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-09", "authors": [{"authorId": "2108568288", "name": "Yijun Yang"}, {"authorId": "2344788655", "name": "Lichao Wang"}, {"authorId": "2290009408", "name": "Xiao Yang"}, {"authorId": "2347348975", "name": "Lanqing Hong"}, {"authorId": "2270894724", "name": "Jun Zhu"}], "abstract": "Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of generating unsafe responses. In response, leading companies have implemented Multi-Layered safety defenses, including alignment training, safety system prompts, and content moderation. However, their effectiveness against sophisticated adversarial attacks remains largely unexplored. In this paper, we propose MultiFaceted Attack, a novel attack framework designed to systematically bypass Multi-Layered Defenses in VLLMs. It comprises three complementary attack facets: Visual Attack that exploits the multimodal nature of VLLMs to inject toxic system prompts through images; Alignment Breaking Attack that manipulates the model's alignment mechanism to prioritize the generation of contrasting responses; and Adversarial Signature that deceives content moderators by strategically placing misleading information at the end of the response. Extensive evaluations on eight commercial VLLMs in a black-box setting demonstrate that MultiFaceted Attack achieves a 61.56% attack success rate, surpassing state-of-the-art methods by at least 42.18%."}
{"paperId": "aeac346a83d9ffcf697728a1aa9cdc3900c93ef2", "url": "https://www.semanticscholar.org/paper/aeac346a83d9ffcf697728a1aa9cdc3900c93ef2", "title": "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-04", "authors": [{"authorId": "2324070581", "name": "Zixuan Liu"}, {"authorId": "2634806", "name": "S. Khajavi"}, {"authorId": "2374296356", "name": "Guangkai Jiang"}], "abstract": "Recent advances in multi-modal models have demonstrated strong performance in tasks such as image generation and reasoning. However, applying these models to the fire domain remains challenging due to the lack of publicly available datasets with high-quality fire domain annotations. To address this gap, we introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k high-resolution fire-related images and 2.5k real-world fire-related videos covering a wide range of fire types, environments, and risk levels. The data are annotated with both traditional computer vision labels (e.g., bounding boxes) and detailed textual prompts describing the scene, enabling applications such as synthetic data generation and fire risk reasoning. DetectiumFire offers clear advantages over existing benchmarks in scale, diversity, and data quality, significantly reducing redundancy and enhancing coverage of real-world scenarios. We validate the utility of DetectiumFire across multiple tasks, including object detection, diffusion-based image generation, and vision-language reasoning. Our results highlight the potential of this dataset to advance fire-related research and support the development of intelligent safety systems. We release DetectiumFire to promote broader exploration of fire understanding in the AI community. The dataset is available at https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890"}
{"paperId": "af0211c1df93bf6bbc5d59bde0f5ffe62a204582", "url": "https://www.semanticscholar.org/paper/af0211c1df93bf6bbc5d59bde0f5ffe62a204582", "title": "Parameter-free Video Segmentation for Vision and Language Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-03", "authors": [{"authorId": "2290077047", "name": "Louis Mahon"}, {"authorId": "1747893", "name": "Mirella Lapata"}], "abstract": "The proliferation of creative video content has driven demand for adapting language models to handle video input and enable multimodal understanding. However, end-to-end models struggle to process long videos due to their size and complexity. An effective alternative is to divide them into smaller chunks to be processed separately, and this motivates a method for choosing where the chunk boundaries should be. In this paper, we propose an algorithm for segmenting videos into contiguous chunks, based on the minimum description length principle, coupled with a dynamic programming search. The algorithm is entirely parameter-free, given feature vectors, not requiring a set threshold or the number or size of chunks to be specified. We show empirically that the breakpoints it produces more accurately approximate scene boundaries in long videos, compared with existing methods for scene detection, even when such methods have access to the true number of scenes. We then showcase this algorithm in two tasks: long video summarization, and retrieval-augmented video question answering. In both cases, scene breaks produced by our algorithm lead to better downstream performance than existing methods for video segmentation."}
{"paperId": "af091c35761345aed2882cd62b48f42a04fe0922", "url": "https://www.semanticscholar.org/paper/af091c35761345aed2882cd62b48f42a04fe0922", "title": "Automated, Interpretable, and Scalable Scientific Machine Learning", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i27.35103", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i27.35103?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i27.35103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2355229923", "name": "Wuyang Chen"}], "abstract": "Although Artificial Intelligence (AI) has transformed vision and language modeling, Scientific Machine Learning (SciML) complements data-driven AI via a knowledge-driven approach, enhancing our understanding of the physical world. My work focuses on: 1) automating scientific reasoning with language models, 2) improving geometric interpretation, 3) developing foundation models for multiphysics."}
{"paperId": "af1a5b447628fb24a0ce9986bd78c4536ff0ddc1", "url": "https://www.semanticscholar.org/paper/af1a5b447628fb24a0ce9986bd78c4536ff0ddc1", "title": "Vision and Language Reference Prompt into SAM for Few-shot Segmentation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-02", "authors": [{"authorId": "2321872154", "name": "Kosuke Sakurai"}, {"authorId": "2053965325", "name": "Ryotaro Shimizu"}, {"authorId": "2257964700", "name": "Masayuki Goto"}], "abstract": "Segment Anything Model (SAM) represents a large-scale segmentation model that enables powerful zero-shot capabilities with flexible prompts. While SAM can segment any object in zero-shot, it requires user-provided prompts for each target image and does not attach any label information to masks. Few-shot segmentation models addressed these issues by inputting annotated reference images as prompts to SAM and can segment specific objects in target images without user-provided prompts. Previous SAM-based few-shot segmentation models only use annotated reference images as prompts, resulting in limited accuracy due to a lack of reference information. In this paper, we propose a novel few-shot segmentation model, Vision and Language reference Prompt into SAM (VLP-SAM), that utilizes the visual information of the reference images and the semantic information of the text labels by inputting not only images but also language as reference information. In particular, VLP-SAM is a simple and scalable structure with minimal learnable parameters, which inputs prompt embeddings with vision-language information into SAM using a multimodal vision-language model. To demonstrate the effectiveness of VLP-SAM, we conducted experiments on the PASCAL-5i and COCO-20i datasets, and achieved high performance in the few-shot segmentation task, outperforming the previous state-of-the-art model by a large margin (6.3% and 9.5% in mIoU, respectively). Furthermore, VLP-SAM demonstrates its generality in unseen objects that are not included in the training data. Our code is available at https://github.com/kosukesakurai1/VLP-SAM."}
{"paperId": "afaf8afee1ff9fcb75c1f3d696af095a342e662f", "url": "https://www.semanticscholar.org/paper/afaf8afee1ff9fcb75c1f3d696af095a342e662f", "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding", "venue": "Knowledge Discovery and Data Mining", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.14526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2025-04-20", "authors": [{"authorId": "2339774898", "name": "Tong Zeng"}, {"authorId": "2356572157", "name": "Longfeng Wu"}, {"authorId": "2186737215", "name": "Liang Shi"}, {"authorId": "2356800731", "name": "Dawei Zhou"}, {"authorId": "2321758436", "name": "Feng Guo"}], "abstract": "Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench-a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 state-of-the-art VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose vision-language models and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git."}
{"paperId": "afb181e3d3e091aa57fb1b8eafd2103a17678372", "url": "https://www.semanticscholar.org/paper/afb181e3d3e091aa57fb1b8eafd2103a17678372", "title": "It’s a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2149870620", "name": "Dominik Schnaus"}, {"authorId": "1942495", "name": "Nikita Araslanov"}, {"authorId": "2269841267", "name": "Daniel Cremers"}], "abstract": "The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or \"blind\", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation."}
{"paperId": "afcc90828c2e5f0df074f4393d17bed89c47af30", "url": "https://www.semanticscholar.org/paper/afcc90828c2e5f0df074f4393d17bed89c47af30", "title": "Computational Budget Should Be Considered in Data Selection", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16806, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-19", "authors": [{"authorId": "2287930439", "name": "Weilin Wan"}, {"authorId": "2267276696", "name": "Weizhong Zhang"}, {"authorId": "2267253979", "name": "Cheng Jin"}], "abstract": "Data selection improves computational efficiency by choosing informative subsets of training samples. However, existing methods ignore the compute budget, treating data selection and importance evaluation independently of compute budget constraints. Yet empirical studies show no algorithm can consistently outperform others (or even random selection) across varying budgets. We therefore argue that compute budget must be integral to data-selection strategies, since different budgets impose distinct requirements on data quantity, quality, and distribution for effective training. To this end, we propose a novel Computational budget-Aware Data Selection (CADS) method and naturally formulate it into a bilevel optimization framework, where the inner loop trains the model within the constraints of the computational budget on some selected subset of training data, while the outer loop optimizes data selection based on model evaluation. Our technical contributions lie in addressing two main challenges in solving this bilevel optimization problem: the expensive Hessian matrix estimation for outer-loop gradients and the computational burden of achieving inner-loop optimality during iterations. To solve the first issue, we propose a probabilistic reparameterization strategy and compute the gradient using a Hessian-free policy gradient estimator. To address the second challenge, we transform the inner optimization problem into a penalty term in the outer objective, further discovering that we only need to estimate the minimum of a one-dimensional loss to calculate the gradient, significantly improving efficiency. Extensive experiments show that our method achieves performance gains of up to 14.42% over baselines in vision and language benchmarks."}
{"paperId": "b072159975ef7fb3b69277d678449f50da11e957", "url": "https://www.semanticscholar.org/paper/b072159975ef7fb3b69277d678449f50da11e957", "title": "Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-20", "authors": [{"authorId": "2226257177", "name": "Abhiroop Chatterjee"}, {"authorId": "2268430567", "name": "Susmita Ghosh"}], "abstract": "As data requirements continue to grow, efficient learning increasingly depends on the curation and distillation of high-value data rather than brute-force scaling of model sizes. In the case of a hyperspectral image (HSI), the challenge is amplified by the high-dimensional 3D voxel structure, where each spatial location is associated with hundreds of contiguous spectral channels. While vision and language models have been optimized effectively for natural image or text tasks, their cross-modal alignment in the hyperspectral domain remains an open and underexplored problem. In this article, we make an attempt to optimize a Vision-Language Model (VLM) for hyperspectral scene understanding by exploiting a CLIP-style contrastive training framework. Our framework maps voxel-level embeddings from a vision backbone onto the latent space of a frozen large embedding model (LEM), where a trainable probe aligns vision features with the model's textual token representations. The two modalities are aligned via a contrastive loss restricted to a curated set of hard (closest wrong classes) and semi-hard (random distractors) negatives, along with positive pairs. To further enhance alignment, descriptive prompts that encode class semantics are introduced and act as structured anchors for the HSI embeddings. It is seen that the proposed method updates only 0.07 percent of the total parameters, yet yields state-of-the-art performance. For example, on Indian Pines (IP) the model produces better results over unimodal and multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa ($\\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA and +0.90 $\\kappa$. Moreover, this is achieved with the set of parameters, nearly 50$\\times$ smaller than DCTN and 90$\\times$ smaller than SS-TMNet."}
{"paperId": "b0ac57bcde1a138d20ff2b5ce599cdaa394315fc", "url": "https://www.semanticscholar.org/paper/b0ac57bcde1a138d20ff2b5ce599cdaa394315fc", "title": "Image Description and Aspect-Aware Denoising for Aspect-Based Multimodal Sentiment Analysis", "venue": "International Conference on Multimedia Retrieval", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3731715.3733364?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3731715.3733364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2292467903", "name": "Jiachang Sun"}, {"authorId": "2279266423", "name": "Xiuhong Li"}], "abstract": null}
{"paperId": "b0b5e476918ce89c91753385a4c1377c3b3391b2", "url": "https://www.semanticscholar.org/paper/b0b5e476918ce89c91753385a4c1377c3b3391b2", "title": "Benchmarking Vision LLMs in fetal ultrasound interpretation: a five-point expert evaluation of standard vs. custom prompts.", "venue": "BioMedical Engineering OnLine", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s12938-025-01486-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s12938-025-01486-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-09", "authors": [{"authorId": "2235098848", "name": "Wala Elsharif"}, {"authorId": "27066815", "name": "M. Alzubaidi"}, {"authorId": "2197530460", "name": "Muhammad Tukur"}, {"authorId": "2212125620", "name": "Abdullatif Magram"}, {"authorId": "2261025404", "name": "Fatima Anver"}, {"authorId": "2397601780", "name": "Aysha Hamza"}, {"authorId": "2397591552", "name": "Safia Said"}, {"authorId": null, "name": "Rukhayya Khan"}, {"authorId": "2031828128", "name": "Mowafa J Househ"}, {"authorId": "2261027323", "name": "Marco Agus"}], "abstract": null}
{"paperId": "b120f42f8daef31366613238ee16b1be5267b6aa", "url": "https://www.semanticscholar.org/paper/b120f42f8daef31366613238ee16b1be5267b6aa", "title": "CVLN-Think: Causal Inference with Counterfactual Style Adaptation for Continuous Vision-and-Language Navigation", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS60139.2025.11247004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS60139.2025.11247004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-10-19", "authors": [{"authorId": "2304360884", "name": "Ruonan Liu"}, {"authorId": "2326643957", "name": "Shuai Wu"}, {"authorId": "2395723237", "name": "Di Lin"}, {"authorId": "2308918422", "name": "Weidong Zhang"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) presents challenges due to environmental variations and domain shifts, making it difficult for agents to generalize beyond seen environments. Most existing methods rely on learning correlations between observations and actions from training data, which leads to spurious dependencies on environmental biases. To address this, we propose CVLN-Think (CVT), a novel navigation model that incorporates causal inference to enhance robustness and adaptability. Specifically, Style Causal Adjuster (SCA) generates counterfactual style observations, enabling agents to learn invariant spatial structures rather than overfitting to dataset-specific visual patterns. Furthermore, Thinking Cause Navigation Engine (TCNE) applies causal intervention to adjust navigation decisions by identifying and mitigating biases from prior experience. Unlike conventional approaches that passively learn from data distributions, our model actively thinks along the \"observation-action\" chain to make more reliable navigation predictions. Experimental results demonstrate that our approach achieves satisfactory performance on VLN-CE tasks. Further analysis indicates that our method possesses stronger generalization capabilities, highlighting the superiority of our proposed approach."}
{"paperId": "b138a61ad73f00c66adc8ef4c82c5c68aa6fe4e9", "url": "https://www.semanticscholar.org/paper/b138a61ad73f00c66adc8ef4c82c5c68aa6fe4e9", "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-17", "authors": [{"authorId": "2392908012", "name": "Chenyang Li"}, {"authorId": "2310188774", "name": "Wen Tang"}, {"authorId": "2142367654", "name": "Yihao Huang"}, {"authorId": "2392880110", "name": "Sinong Simon Zhan"}, {"authorId": "2306762461", "name": "Ming Hu"}, {"authorId": "2275762052", "name": "Xiaojun Jia"}, {"authorId": "2260197081", "name": "Yang Liu"}], "abstract": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations."}
{"paperId": "b207ae47cadfa364c35f6b6604d63516dfb30720", "url": "https://www.semanticscholar.org/paper/b207ae47cadfa364c35f6b6604d63516dfb30720", "title": "Remodeling Semantic Relationships in Vision-Language Fine-Tuning", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.08238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-11", "authors": [{"authorId": "2393008526", "name": "Xiangyang Wu"}, {"authorId": "2366165158", "name": "Liu Liu"}, {"authorId": "2329694303", "name": "Baosheng Yu"}, {"authorId": "2300855342", "name": "Jiayan Qiu"}, {"authorId": "2391838964", "name": "Zhenwei Shi"}], "abstract": "Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and relationships.Specifically, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods."}
{"paperId": "b215e8f6f9cdbcd3098826aa98ba6732a2afc7e8", "url": "https://www.semanticscholar.org/paper/b215e8f6f9cdbcd3098826aa98ba6732a2afc7e8", "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.00576, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-01", "authors": [{"authorId": "2350502501", "name": "Zhanliang Wang"}, {"authorId": "2351254035", "name": "Kai Wang"}], "abstract": "Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their\"black-box\"nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples -\"why the model makes a specific prediction on this input\", and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples -\"how the model integrates information across modalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models."}
{"paperId": "b2189a93c75bc687d2326bef4eb3f4148e250d9d", "url": "https://www.semanticscholar.org/paper/b2189a93c75bc687d2326bef4eb3f4148e250d9d", "title": "Imagining Vision From Language for Few-Shot Class-Incremental Learning", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3754841?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3754841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2281607295", "name": "Shuo Li"}, {"authorId": "2391173573", "name": "Xingchen Liu"}, {"authorId": "2261732890", "name": "Fang Liu"}, {"authorId": "2287033797", "name": "Licheng Jiao"}, {"authorId": "2214542036", "name": "Jiahao Wang"}, {"authorId": "2215862040", "name": "Xinyan Huang"}, {"authorId": "1557288808", "name": "Yanbiao Ma"}, {"authorId": "3394474", "name": "Puhua Chen"}, {"authorId": "47681424", "name": "Lingling Li"}, {"authorId": "2110952179", "name": "Xu Liu"}, {"authorId": "2212697511", "name": "Xue Gou"}], "abstract": "Few-Shot Class-Incremental Learning (FSCIL) aims to enable the model to continuously learn new categories with only limited samples and maintain the recognition ability of old categories. However, this task faces two core challenges: catastrophic forgetting and overfitting due to data scarcity. Existing methods mostly rely on fine-tuning strategies or the transfer ability of visual language models, which is difficult to fundamentally solve the problem of insufficient image samples. To this end, we propose an incremental learning method called Imagining Vision From Language (IVFL), which consists of a base session and multiple incremental sessions. In the base session, the model generates imagined visual features through language descriptions, learns the mapping relationship from language to vision, and estimates the feature distribution of each category; in the incremental session, the model jointly uses old-class pseudo-image features, new-class images, and language descriptions to guide the model to perform effective category expansion while maintaining old-class knowledge. Our IVFL achieves an effective balance between new and old knowledge and achieves significant performance improvements on miniImageNet, CIFAR100, and CUB200 datasets, verifying its superiority in scenarios with data scarcity and knowledge retention."}
{"paperId": "b2406c0b01bd643f6d7dc942b32951848bcb7942", "url": "https://www.semanticscholar.org/paper/b2406c0b01bd643f6d7dc942b32951848bcb7942", "title": "Learning Vision and Language Concepts for Controllable Image Generation", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "25106675", "name": "Shaoan Xie"}, {"authorId": "2324899900", "name": "Lingjing Kong"}, {"authorId": "2309464222", "name": "Yujia Zheng"}, {"authorId": "2125563094", "name": "Zeyu Tang"}, {"authorId": "2262446774", "name": "Eric P. Xing"}, {"authorId": "2155315836", "name": "Guan-Hong Chen"}, {"authorId": "2372757363", "name": "Kun Zhang"}], "abstract": null}
{"paperId": "b24657aa68556b3b91d62d3651535cfbd44fa6c5", "url": "https://www.semanticscholar.org/paper/b24657aa68556b3b91d62d3651535cfbd44fa6c5", "title": "A trans-disciplinary forensic study of Lil Miquela’s virtual identity performance in Instagram", "venue": "Ai & Society", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "https://doi.org/10.1007/s00146-025-02219-8", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00146-025-02219-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00146-025-02219-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-10", "authors": [{"authorId": "2281255919", "name": "Nashwa Elyamany"}, {"authorId": "2332555585", "name": "Yasser Omar Youssef"}, {"authorId": "2349526593", "name": "Nehal El-karef"}], "abstract": "Virtual Influencers (VIs) have become the most prolific research subjects in human–computer interaction and mass media and communication studies from a plethora of perspectives. Developed to integrate social traits and anthropomorphic minds in their social media posts, human-like VIs engage with followers via visually authentic personae, emotionally captivating multimodal storytelling, and semio-pragmatic labor-intensive strategies in conformity with the expectations (and pressures) of the contemporary influencer culture. Informed by Belk’s revisited model of and timely scholarly works on the extended self, we introduce a new conceptualization of the virtual self that performs identity in platformized spaces. To examine virtual personae’s identity performance, we adopt a trans-disciplinary mixed-method forensic netnographic research design, synergizing computer vision, natural language processing, and semio-pragmatic analytical tools. A convenient sample of 334 (sponsored) posts, retrieved from the official Instagram account of the quintessential virtual agent Lil Miquela, is scrutinized taking into consideration her posts’ images and accompanying captions. The paper carries out the tripartite analysis in serious attempt to unravel: (a) how humanoid her synthesized images appear to the naked eye in quest of authenticity building; (b) the techno-affects that contribute to her identity performance; and (c) the semio-pragmatic affordances appropriated and deployed in Instagrammable spaces, showcasing how the three serve the performance of her digital identity. Valuable insights reveal that her agency draws heavily on algorithmization and semiotic immateriality to produce action. The study’s findings contribute to the existing body of literature on VIs and the extended self within the context of artificial intelligence."}
{"paperId": "b24a72d80919ed891b569eda7c0b9ce10d9c99c4", "url": "https://www.semanticscholar.org/paper/b24a72d80919ed891b569eda7c0b9ce10d9c99c4", "title": "Unlocking Value From Historical Documents: Automated Raster Log Digitization Using Visual Language Models And Computer Vision", "venue": "ADIPEC", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2118/229257-ms?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2118/229257-ms, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-03", "authors": [{"authorId": "2254042078", "name": "Karim Rekik"}, {"authorId": "2390707752", "name": "Ayoub Diouri"}, {"authorId": "1644155227", "name": "Sohaïb Ouzineb"}, {"authorId": "2254044275", "name": "Olfa Zened"}], "abstract": "\n Thousands of historical well logs exist only in raster image or scanned PDF formats, limiting their usability in modern digital workflows. Manually digitizing these logs is time-consuming and error-prone, hindering field evaluation and reservoir characterization efforts. This project introduces an AI-based raster log digitization solution combining computer vision segmentation techniques with the extraction of key metadata from raster logs using Generative AI and Visual Language Models, enabling scalable deep learning digitization with minimal human intervention.\n The proposed method leverages cutting-edge multimodal AI models that integrate vision and language understanding to process well log images holistically. Image segmentation techniques are first employed to localize visual components like tracks, headers, and curve line styles. Visual Language Models are then optimized to detect and extract crucial metadata such as log names, units, curve limits, and depth scales by interpreting both text and graphical elements from the segmentation outputs. These combined steps form the foundation for automated log curve digitization using a pre-trained deep learning digitization model.\n The system provides accurate and stable results throughout the various stages of the digitization pipeline and significantly reduces the need for manual labeling or user input. By combining generative AI with vision-language learning, the solution improves robustness in handling a wide range of raster log styles, low-quality or handwritten scans. When deployed in cloud environments, this approach enables high-volume, concurrent processing of raster logs—completing in minutes tasks that would traditionally take weeks—and accelerating the ingestion of legacy data into digital ecosystems like OSDU.\n Unlike conventional OCR or rule-based methods, this approach understands the context and structure of log images, extracting both visual and textual metadata with precision, as well as understanding complex raster logs structures such as wrapping or single-track multi-logs display. It represents a transformative step in subsurface data management, bringing previously inaccessible historical data into modern interpretation platforms through scalable, intelligent automation."}
{"paperId": "b25f7e913bc523bf77f3f9c6734fbba0a137d58a", "url": "https://www.semanticscholar.org/paper/b25f7e913bc523bf77f3f9c6734fbba0a137d58a", "title": "Make \"V\" and \"Q\" Inseparable: Deliberately Dual-Channel Adversarial Learning for Robust Visual Question Answering", "venue": "IEEE International Joint Conference on Neural Network", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN64981.2025.11228973?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN64981.2025.11228973, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2370450607", "name": "Hanxiao Wu"}, {"authorId": "2392721283", "name": "Zhaowen Li"}, {"authorId": "2392992486", "name": "Feilong Chen"}, {"authorId": "2382504134", "name": "Zhiyu Wang"}, {"authorId": "2392469163", "name": "Jiali Xu"}, {"authorId": "2392469060", "name": "Liquan Hu"}, {"authorId": "2393005087", "name": "Huaixuan Cao"}, {"authorId": "2392464354", "name": "Yin Li"}, {"authorId": "2392445690", "name": "Jinqiao Wang"}, {"authorId": "2349371331", "name": "Jianlong Chang"}], "abstract": "Visual Question Answering (VQA) is a challenging task due to the vision-language biases which restrict the model to sufficiently learn the multi-modal knowledge from visual image and natural language simultaneously. Several recent works attempt to alleviate this problem via weakening language prior but ignore vision prior, hindering further performance improvement. In this paper, we propose a novel Deliberately Dual-Channel Adversarial Learning (DCAL) to make \"V\" and \"Q\" inseparable, which aims to weaken prior from both vision and language. Specifically, DCAL introduces in-batch random negative sampling to force the model to be wrong when given the wrong questions or images. DCAL maximizes the likelihood of correct answers for the original question-image pairs and minimizes it for random negative samples. In order to solve the problem of false negatives, DCAL exploits a deliberate training strategy to utilize the sampled question-image pairs. The proposed DCAL is model-agnostic and can be applied to various VQA models. Experiments demonstrate that our proposed DCAL method improves the performance of existing robust VQA models on the sensitive VQA-CP dataset while performing robustly on the balanced VQA v2 dataset."}
{"paperId": "b2651736ec800e9104c11df9ecc2c6ea3a8f64e2", "url": "https://www.semanticscholar.org/paper/b2651736ec800e9104c11df9ecc2c6ea3a8f64e2", "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models", "venue": "arXiv.org", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-06", "authors": [{"authorId": "2366064842", "name": "Yuhan Hao"}, {"authorId": "2367076342", "name": "Zhengning Li"}, {"authorId": "2365998324", "name": "Lei Sun"}, {"authorId": "2366058028", "name": "Weilong Wang"}, {"authorId": "2366011114", "name": "Naixin Yi"}, {"authorId": "2366158758", "name": "Sheng Song"}, {"authorId": "2366013295", "name": "Caihong Qin"}, {"authorId": "2366003746", "name": "Mofan Zhou"}, {"authorId": "2333235303", "name": "Yifei Zhan"}, {"authorId": "2353954278", "name": "Peng Jia"}, {"authorId": "2277446558", "name": "Xianpeng Lang"}], "abstract": "Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by drivers of autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from drivers'actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving."}
{"paperId": "b356413a98d0ffa790442744aa8eac1b20213c81", "url": "https://www.semanticscholar.org/paper/b356413a98d0ffa790442744aa8eac1b20213c81", "title": "WP-CMA: Waypoint Prediction for Cross-modal Alignment of Vision-and-Language Navigation in Continuous Environments", "venue": "Proceedings of the 7th ACM International Conference on Multimedia in Asia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3743093.3770935?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3743093.3770935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-12-06", "authors": [{"authorId": null, "name": "Siyang Fu"}, {"authorId": "2345469749", "name": "Yifei Wu"}, {"authorId": "2338687339", "name": "Ting Yu"}], "abstract": "Vision-and-Language Navigation (VLN) tasks challenge an embodied agent to follow natural language instructions and reach a specified goal in visually rich environments. In continuous navigation settings, predicting accurate waypoints becomes particularly difficult due to the complex spatial dynamics and the semantic gap between language and vision modalities. In this work, we introduce WP-CMA, a novel cross-modal alignment framework designed for waypoint prediction in continuous VLN scenarios. Our framework consists of two primary modules: a Depth Space Reasoning Module (DSRM) and a Vision-Language Fusion Module (VLFM). The DSRM leverages 3D information from depth imagery to perform spatial reasoning and predict optimal waypoints. Concurrently, the VLFM integrates visual features with linguistic instructions to generate a unified representation, which provides essential context to the DSRM. Experiments on the R2R-CE dataset show that our proposed method significantly improves navigation efficiency."}
{"paperId": "b39d8049fde7afdd941450ab8d3b7375f40f70df", "url": "https://www.semanticscholar.org/paper/b39d8049fde7afdd941450ab8d3b7375f40f70df", "title": "A Review of Visual-Language Pre-training Paradigms: From CLIP to the Future", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.ld29147?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.ld29147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-11-05", "authors": [{"authorId": "2392206456", "name": "Yi Su"}], "abstract": "With the rapid advancement of general artificial intelligence, researchers have moved beyond monomodal approaches to vigorously pursue multimodal studies. As vision and language represent humanity's primary channels for information acquisition, vision-language pre-training (VLP) techniques focused on these two modalities have emerged. However, early VLP paradigms heavily relied on manually annotated datasets tailored for specific downstream tasks (such as visual question answering and image captioning), suffering from drawbacks like high costs and weak generalization capabilities. The emergence of contrastive learning models like Contrastive Language-Image Pre-training (CLIP), trained on vast image-text pairs using contrastive learning objectives, demonstrated exceptional zero-shot prediction capabilities, sparking a paradigm shift in the field. Following the development of joint training paradigms for understanding and generation tasks, subsequent advancements have increasingly focused on deep integration with large language models (LLMs). This review systematically traces the evolution from CLIP's contrastive learning paradigm to the current mainstream of generative multimodal models driven by large pre-trained models. It analyzes core models and key technologies, summarizes their application value across domains, and provides a reference for subsequent researchers."}
{"paperId": "b43b345d7be1408bd8635c5964c5183bdf0b4ec9", "url": "https://www.semanticscholar.org/paper/b43b345d7be1408bd8635c5964c5183bdf0b4ec9", "title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Document Understanding", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-02-03", "authors": [{"authorId": "100832735", "name": "Ahmed Masry"}, {"authorId": "2116899991", "name": "Juan A. Rodriguez"}, {"authorId": "2305815558", "name": "Tianyu Zhang"}, {"authorId": "2355999292", "name": "Suyuchen Wang"}, {"authorId": "2344189229", "name": "Chao Wang"}, {"authorId": "1962954677", "name": "Aarash Feizi"}, {"authorId": "88971031", "name": "Akshay Kalkunte Suresh"}, {"authorId": "2310436656", "name": "Abhay Puri"}, {"authorId": "2334357492", "name": "Xiangru Jian"}, {"authorId": "1387900763", "name": "Pierre-Andr'e Noel"}, {"authorId": "100519532", "name": "Sathwik Tejaswi Madhusudhan"}, {"authorId": "2352279684", "name": "Marco Pedersoli"}, {"authorId": "2241485472", "name": "Bang Liu"}, {"authorId": "2368445285", "name": "Nicolas Chapados"}, {"authorId": "2211024206", "name": "Y. Bengio"}, {"authorId": "2274022429", "name": "Enamul Hoque"}, {"authorId": "2275240361", "name": "Christopher Pal"}, {"authorId": "3266173", "name": "I. Laradji"}, {"authorId": "2351057662", "name": "David Vázquez"}, {"authorId": "1784150", "name": "Perouz Taslakian"}, {"authorId": "2921001", "name": "Spandana Gella"}, {"authorId": "2253453798", "name": "Sai Rajeswar"}], "abstract": "Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), lack inductive bias to constrain visual features within the linguistic structure of the LLM's embedding space, making them data-hungry and prone to cross-modal misalignment. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where visual and textual modalities are highly correlated. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods, with larger gains on document understanding tasks and under low-resource setups. We provide further analysis demonstrating its efficiency and robustness to noise."}
{"paperId": "b4415d04a1bcd9f55761487d3f1a32f4aab17a9e", "url": "https://www.semanticscholar.org/paper/b4415d04a1bcd9f55761487d3f1a32f4aab17a9e", "title": "Discovering Meaningful Units with Visually Grounded Semantics from Image Captions", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-14", "authors": [{"authorId": "1411451148", "name": "Melika Behjati"}, {"authorId": "2261965004", "name": "James Henderson"}], "abstract": "Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively."}
{"paperId": "b443c7eadb20d7d37fa786899fc92d9bc4d34eae", "url": "https://www.semanticscholar.org/paper/b443c7eadb20d7d37fa786899fc92d9bc4d34eae", "title": "DeepCon: Improving Distributed Deep Learning Model Consistency in Edge-Cloud Environments via Distillation", "venue": "IEEE Transactions on Cognitive Communications and Networking", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCCN.2025.3547049?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCCN.2025.3547049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-01", "authors": [{"authorId": "101038657", "name": "Bin Qian"}, {"authorId": "2220508332", "name": "Jiaxu Qian"}, {"authorId": "2323758471", "name": "Zhenyu Wen"}, {"authorId": "2300501742", "name": "Di Wu"}, {"authorId": "2324799795", "name": "Shibo He"}, {"authorId": "2138800088", "name": "Jiming Chen"}, {"authorId": "115452174", "name": "R. Ranjan"}], "abstract": "In a typical distributed Deep Learning (DL) based application, models are configured differently to meet the requirements of resource constraints. For instance, a large ResNet56 model is deployed on the cloud server while a small lightweight MobileNet model is more suitable for the end-user device with fewer computation resources. However, the heterogeneity of the model architectures and configurations may bring a systemic problem - models may produce different outputs when given the same input. This inconsistency problem may cause severe system failure of prediction agreement inside the application. Current research has not studied the systemic design for efficiently detecting and reducing the inconsistency among models in distributed DL applications. With the increasing scale of distributed DL applications, the challenges of inconsistency mitigation should consider both algorithm and system design. To this end, we design and implement DeepCon, an adaptive deployment system across the edge-cloud layer with over-the-air model updates. We implement ASRS sampling for efficiently sampling data to reveal the real data distribution as well as model prediction inconsistency. Then, we implement DMML-Par, an asynchronous parallel training algorithm for quickly updating the models and reducing inconsistency. DeepCon implements over-the-air updates with a set of APIS to enable seamless inconsistency detection and reduction in such deep learning applications. Our experiment results on both vision and language tasks demonstrate that DMML could improve the model consistency up to 4%, 7%, and 13% at CIFAR10/100 and IMDB datasets without sacrificing the accuracy of individual models. We also show that the ASRS sampling can save 90% network bandwidth of data transmission and that DMML-Par is up to 60% faster compared to simple synchronous parallel training."}
{"paperId": "b4b4c2fce59d401daab81fbf97f9d9da6e843475", "url": "https://www.semanticscholar.org/paper/b4b4c2fce59d401daab81fbf97f9d9da6e843475", "title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-16", "authors": [{"authorId": "2332527018", "name": "Zihan Wang"}, {"authorId": "2345871965", "name": "Yaohui Zhu"}, {"authorId": "2332479307", "name": "Gim Hee Lee"}, {"authorId": "2345843841", "name": "Yachun Fan"}], "abstract": "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models."}
{"paperId": "b4cfa945df05c92671a2def8e0705c59c5c757a1", "url": "https://www.semanticscholar.org/paper/b4cfa945df05c92671a2def8e0705c59c5c757a1", "title": "Quantum Relational Knowledge Distillation", "venue": "", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.13054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-08-18", "authors": [{"authorId": "2265863857", "name": "Chen-Yu Liu"}, {"authorId": "2265758042", "name": "Kuan-Cheng Chen"}, {"authorId": "2376198885", "name": "Keisuke Murota"}, {"authorId": "2319599119", "name": "Samuel Yen-Chi Chen"}, {"authorId": "2376197493", "name": "Enrico Rinaldi"}], "abstract": "Knowledge distillation (KD) is a widely adopted technique for compressing large models into smaller, more efficient student models that can be deployed on devices with limited computational resources. Among various KD methods, Relational Knowledge Distillation (RKD) improves student performance by aligning relational structures in the feature space, such as pairwise distances and angles. In this work, we propose Quantum Relational Knowledge Distillation (QRKD), which extends RKD by incorporating quantum relational information. Specifically, we map classical features into a Hilbert space, interpret them as quantum states, and compute quantum kernel values to capture richer inter-sample relationships. These quantum-informed relations are then used to guide the distillation process. We evaluate QRKD on both vision and language tasks, including CNNs on MNIST and CIFAR-10, and GPT-2 on WikiText-2, Penn Treebank, and IMDB. Across all benchmarks, QRKD consistently improves student model performance compared to classical RKD. Importantly, both teacher and student models remain classical and deployable on standard hardware, with quantum computation required only during training. This work presents the first demonstration of quantum-enhanced knowledge distillation in a fully classical deployment setting."}
{"paperId": "b4f7c9ae4f37be900da3be7c71b4692cd9eba437", "url": "https://www.semanticscholar.org/paper/b4f7c9ae4f37be900da3be7c71b4692cd9eba437", "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.10606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-12", "authors": [{"authorId": "2334922159", "name": "Yuqi Liu"}, {"authorId": "2321480473", "name": "Liangyu Chen"}, {"authorId": "2385485669", "name": "Jiazhen Liu"}, {"authorId": "2312344497", "name": "Mingkang Zhu"}, {"authorId": "2294147935", "name": "Zhisheng Zhong"}, {"authorId": "2349958453", "name": "Bei Yu"}, {"authorId": "2291196160", "name": "Jiaya Jia"}], "abstract": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs) include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). SFT leverages external guidance to inject new knowledge, whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities and overall performance. However, our analysis reveals that SFT often leads to sub-optimal performance, while RLVR struggles with tasks that exceed the model's internal knowledge base. To address these limitations, we propose ViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement \\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the strengths of both SFT and RLVR within a single stage. We analyze the derivation of the SFT and RLVR objectives to establish the ViSurf objective, providing a unified perspective on these two paradigms. The core of ViSurf involves injecting ground-truth labels into the RLVR rollouts, thereby providing simultaneous external supervision and internal reinforcement. Furthermore, we introduce three novel reward control strategies to stabilize and optimize the training process. Extensive experiments across several diverse benchmarks demonstrate the effectiveness of ViSurf, outperforming both individual SFT, RLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates these findings, validating the derivation and design principles of ViSurf."}
{"paperId": "b5ec878a4ef0afe31486981afdaac3ae4b01ff62", "url": "https://www.semanticscholar.org/paper/b5ec878a4ef0afe31486981afdaac3ae4b01ff62", "title": "Abstract C036: From pixels to prognosis: Attention-based chain of thought reasoning for automated spine cancer image analysis", "venue": "Cancer Epidemiology, Biomarkers &amp; Prevention", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1158/1538-7755.disp25-c036?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1158/1538-7755.disp25-c036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2381491556", "name": "Vedant Malik"}, {"authorId": "2381378365", "name": "Krish Malik"}], "abstract": "\n \n \n Current vision– Large Language Models (V-LLMs) for spinal oncology imaging use a black-box approach towards generating caption from an MRI/CT scan image. This conflicts with what a real radiologist goes through when interpreting the same images. These models take the pixels, traverse a billion plus parameter latent space, and generate a caption in one shot. This “all-at-once” approach ignores the multi-pass workflow that expert radiologists follow—checking image quality and vertebral levels, mapping baseline anatomy, surveying for disease, measuring epidural tumor extension and spinal-canal compromise and then integrating these findings into a structured report that drives surgical or radiotherapy decisions. Because the vision model’s intermediate reasoning remains hidden, clinicians cannot confirm that the AI model has examined every clinically critical cue, they can also not trace the origin of potential errors when the AI model misses a small sacral lesion or overstates the degree of canal stenosis. This opacity limits trust, complicates regulatory review, and ultimately slows the adoption of AI in oncologic imaging. It also slows down clinical adoption at scale. A total of 1978 expert-captioned studies—radiographs, CT, and MRI—were collected from the public ROCO-v2 corpus which also included images for spine. The proposed solution is an inference-time five-step Chain-of-Thought (CoT) reasoning that uses a fine tuned 7-billion-parameter vision–language model that guides the model to generate the following:Image-quality & modality check, Systematic vertebral and soft-tissue mapping, Disease detection, Tumor-specific feature analysis, Clinical synthesis. After following these five steps, the system generates a structured clinical report while 32 layer-wise attention mechanism expose where in the image the AI model looked at every stage of image processing. This allows a clinician visibility into why of what the model generated as a caption to explain what’s happening in the image. This type of model scaffolding provides insights into model’s reasoning ability, why it generated a specific language, in addition to that, it lifts caption fidelity and provides an auditable trail that oncologists and regulators can inspect—all without the cost and delay of re-training the model for each new cancer-imaging task. The chain of thought reasoning improved caption accuracy by nearly 3x when compared to just the fine tuned AI vision model. Corpus BLEU-4 increased from 0.005 to 0.013 (+161%) and image level BLEU-4 score rose from 0.005 ± 0.002 to 0.014 ± 0.003 (+194 %). During testing, all 8 COT runs for spine related cancer completed without error. Patch-level attention captured from all 32 vision layers of the AI vision model showed early layers attending diffusely across vertebrae and later layers concentrating on tumor margins, corroborating the step-wise reasoning flow. The five-tage, attention based CoT scaffold transforms a 7 B-parameter V-LLM from a black box to a radiologist-like, auditable spine-cancer reader.\n \n \n \n Vedant Malik, Krish Malik. From pixels to prognosis: Attention-based chain of thought reasoning for automated spine cancer image analysis [abstract]. In: Proceedings of the 18th AACR Conference on the Science of Cancer Health Disparities; 2025 Sep 18-21; Baltimore, MD. Philadelphia (PA): AACR; Cancer Epidemiol Biomarkers Prev 2025;34(9 Suppl):Abstract nr C036.\n"}
{"paperId": "b601a39d1ba5743d36e1db0b9ef3c5519a5f01f6", "url": "https://www.semanticscholar.org/paper/b601a39d1ba5743d36e1db0b9ef3c5519a5f01f6", "title": "Research on Teaching Reform of Deep Learning Course to Enhance Practical Ability Based on the Integration of Industry and Education", "venue": "Journal of Contemporary Educational Research", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.26689/jcer.v9i7.11383?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26689/jcer.v9i7.11383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-31", "authors": [{"authorId": "2365457148", "name": "Lifeng Yin"}], "abstract": "With the rapid development of artificial intelligence technology, deep learning, as one of its core technologies, occupies an important position in the cultivation of applied talents. Based on the concept of integration of industry and education, this paper proposes a systematic teaching reform plan to address the issues of disconnection between theory and practice, single teaching methods, and insufficient practical resources in the deep learning courses for professional master’s students at our university. Through deep cooperation with Huawei Cloud Technologies Co., Ltd., we introduce cutting-edge theoretical content (such as GoogleNet, ResNet, Transformer, BERT, etc.), update practical cases (covering computer vision, natural language processing, and smart manufacturing), and adopt a case-led comprehensive teaching method combined with the online and offline hybrid practical platform ModelArts to promote the close integration of theory and practice. Simultaneously, a diversified evaluation system with practice as the core is constructed to comprehensively assess students’ practical abilities and project execution levels. The research in this paper provides a valuable reference for the innovation of teaching modes and the cultivation of practical abilities in deep learning courses in higher education institutions."}
{"paperId": "b609fa36d23437233262d103239db2a6326d1ad9", "url": "https://www.semanticscholar.org/paper/b609fa36d23437233262d103239db2a6326d1ad9", "title": "AI-Powered Assistive Technologies for Visual Impairment", "venue": "arXiv.org", "year": 2025, "citationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.15494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-14", "authors": [{"authorId": "2345573993", "name": "Prudhvi Naayini"}, {"authorId": "2332100398", "name": "Praveen Kumar Myakala"}, {"authorId": "2332100344", "name": "Chiranjeevi Bura"}, {"authorId": "2336805523", "name": "Anil Kumar Jonnalagadda"}, {"authorId": "2345457632", "name": "Srikanth Kamatala"}], "abstract": "Artificial Intelligence (AI) is revolutionizing assistive technologies. It offers innovative solutions to enhance the quality of life for individuals with visual impairments. This review examines the development, applications, and impact of AI-powered tools in key domains, such as computer vision, natural language processing (NLP), and wearable devices. Specific advancements include object recognition for identifying everyday items, scene description for understanding surroundings, and NLP-driven text-to-speech systems for accessing digital information. Assistive technologies like smart glasses, smartphone applications, and AI-enabled navigation aids are discussed, demonstrating their ability to support independent travel, facilitate social interaction, and increase access to education and employment opportunities. The integration of deep learning models, multimodal interfaces, and real-time data processing has transformed the functionality and usability of these tools, fostering inclusivity and empowerment. This article also addresses critical challenges, including ethical considerations, affordability, and adaptability in diverse environments. Future directions highlight the need for interdisciplinary collaboration to refine these technologies, ensuring equitable access and sustainable innovation. By providing a comprehensive overview, this review underscores AI's transformative potential in promoting independence, enhancing accessibility, and fostering social inclusion for visually impaired individuals."}
{"paperId": "b6f14a2a31044d5e850d8490bc60da17c700ee45", "url": "https://www.semanticscholar.org/paper/b6f14a2a31044d5e850d8490bc60da17c700ee45", "title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-02", "authors": [{"authorId": "2348362749", "name": "Ziyang Zhang"}, {"authorId": "2348771719", "name": "Yang Yu"}, {"authorId": "2348271411", "name": "Yucheng Chen"}, {"authorId": "2376264976", "name": "Xulei Yang"}, {"authorId": "2348281539", "name": "Si Yong Yeo"}], "abstract": "Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the model’s ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learning. In this work, we propose MedUnifier, a unified VLP framework tailored for medical data. MedUnifier seamlessly integrates text-grounded image generation capabilities with multi-modal learning strategies, including image-text contrastive alignment, image-text matching and image-grounded text generation. Unlike traditional methods that reply on continuous visual representations, our approach employs visual vector quantization, which not only facilitates a more cohesive learning strategy for cross-modal understanding but also enhances multi-modal generation quality by effectively leveraging discrete representations. Our framework’s effectiveness is evidenced by the experiments on established benchmarks, including uni-modal tasks, cross-modal tasks, and multi-modal tasks, where it achieves state-of-the-art performance across various tasks. MedUnifier also offers a highly adaptable tool for a wide range of language and vision tasks in healthcare, marking advancement toward the development of a generalizable AI model for medical applications."}
{"paperId": "b6f1d35aa33d5f019070d191f0c9160b35b1060f", "url": "https://www.semanticscholar.org/paper/b6f1d35aa33d5f019070d191f0c9160b35b1060f", "title": "VISTA: Generative Visual Imagination for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-09", "authors": [{"authorId": "2351000265", "name": "Yanjia Huang"}, {"authorId": "2360939852", "name": "Mingyang Wu"}, {"authorId": "2351000281", "name": "Renjie Li"}, {"authorId": "2336092412", "name": "Zhengzhong Tu"}], "abstract": "Vision-and-Language Navigation (VLN) tasks agents with locating specific objects in unseen environments using natural language instructions and visual cues. Many existing VLN approaches typically follow an 'observe-and-reason' schema, that is, agents observe the environment and decide on the next action to take based on the visual observations of their surroundings. They often face challenges in long-horizon scenarios due to limitations in immediate observation and vision-language modality gaps. To overcome this, we present VISTA, a novel framework that employs an 'imagine-and-align' navigation strategy. Specifically, we leverage the generative prior of pre-trained diffusion models for dynamic visual imagination conditioned on both local observations and high-level language instructions. A Perceptual Alignment Filter module then grounds these goal imaginations against current observations, guiding an interpretable and structured reasoning process for action selection. Experiments show that VISTA sets new state-of-the-art results on Room-to-Room (R2R) and RoboTHOR benchmarks, e.g.,+3.6% increase in Success Rate on R2R. Extensive ablation analysis underscores the value of integrating forward-looking imagination, perceptual alignment, and structured reasoning for robust navigation in long-horizon environments."}
{"paperId": "b7ac48285cb68d263f058e7a833d7e3dc8ea0564", "url": "https://www.semanticscholar.org/paper/b7ac48285cb68d263f058e7a833d7e3dc8ea0564", "title": "DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation", "venue": "International Conference on Multimedia Retrieval", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.00743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2025-04-30", "authors": [{"authorId": "2359108621", "name": "Yinfeng Yu"}, {"authorId": "2359156440", "name": "Dongsheng Yang"}], "abstract": "Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach."}
{"paperId": "b8325c80576e5720bbf4b316f63bc3166f6122a0", "url": "https://www.semanticscholar.org/paper/b8325c80576e5720bbf4b316f63bc3166f6122a0", "title": "Modality-Agnostic Decoding of Vision and Language from fMRI", "venue": "bioRxiv", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2025.06.08.658221?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2025.06.08.658221, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-10-17", "authors": [{"authorId": "1387994359", "name": "Mitja Nikolaus"}, {"authorId": "9660519", "name": "Milad Mozafari"}, {"authorId": "2344838490", "name": "Isabelle Berry"}, {"authorId": "2292197399", "name": "Nicholas Asher"}, {"authorId": "2292157266", "name": "Leila Reddy"}, {"authorId": "2366110891", "name": "Rullen VanRullen"}], "abstract": null}
{"paperId": "b85bd635e6d49c2307f6e6ded79e44233d281265", "url": "https://www.semanticscholar.org/paper/b85bd635e6d49c2307f6e6ded79e44233d281265", "title": "The 4th Workshop on Ethical Artificial Intelligence: Methods and Applications (EAI)", "venue": "Knowledge Discovery and Data Mining", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3711896.3737859?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3711896.3737859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book", "Conference", "Review"], "publicationDate": "2025-08-03", "authors": [{"authorId": "48505082", "name": "Chen Zhao"}, {"authorId": "2317033161", "name": "Feng Chen"}, {"authorId": "2282586484", "name": "Xintao Wu"}, {"authorId": "2317089239", "name": "Haifeng Chen"}], "abstract": "As computers increasingly make decisions about who gets a loan, a job, or even bail, the expansion of AI algorithms has provoked public concern about ethical issues, and the need to understand what constitutes AI algorithms and how they make decisions becomes ever more pressing. For example, an increasing number of high-profile news reports that widely-used algorithms have unfairly discriminated against some groups of people (e.g., by gender and race) in parole decisions and other major life events. Focusing more attention on ethical bias in learning algorithms is key to unlocking the potential of automated decision systems while ensuring fairness and accountability so that everyone can advance equally in society. Ethical AI has become increasingly important and it has been attracting attention from academia and industry, due to its increased popularity in real-world applications with fairness concerns. It also places fundamental importance on ethical considerations in determining legitimate and illegitimate uses of AI. Organizations that apply ethical AI have clearly stated well-defined review processes to ensure adherence to legal guidelines. Therefore, the wave of research at the intersection of ethical AI in data mining and machine learning has also influenced other fields of science, including computer vision, natural language processing, reinforcement learning, and social science."}
{"paperId": "b87fe90417fdcc130d84ef215b7d58e6d8bc6bce", "url": "https://www.semanticscholar.org/paper/b87fe90417fdcc130d84ef215b7d58e6d8bc6bce", "title": "Artificial Intelligence in Orthopaedic and Trauma Surgery Education: Applications, Ethics, and Future Perspectives", "venue": "Journal of the American Academy of Orthopaedic Surgeons. Global research & reviews", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12425090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review", "JournalArticle"], "publicationDate": "2025-09-01", "authors": [{"authorId": "2380095191", "name": "Jaime Andrés Leal"}], "abstract": "Artificial intelligence (AI) is redefining surgical education by enabling personalized, data-driven learning environments. In orthopaedic trauma surgery, a specialty defined by diagnostic complexity, time-sensitive decision making, and procedural precision, AI tools are uniquely positioned to enhance resident training. This narrative review explores the role of AI subfields—machine learning (machine learning), deep learning, computer vision, natural language processing, and generative AI—in orthopaedic education. Each technology supports distinct educational functions, from real-time performance tracking and image interpretation to examination simulation and feedback automation. We describe how machine learning and deep learning models can assess technical competence and predict skill progression, whereas computer vision and augmented reality technologies provide immersive simulation and motion analysis. Natural language processing enables documentation analysis and scenario-based teaching, and large language models like ChatGPT support interactive, case-based learning. Ethical concerns such as algorithmic bias, data governance, transparency, and cognitive over-reliance are also discussed. A systems-based framework is proposed to integrate these technologies into a closed-loop educational cycle, emphasizing adaptive learning and professional growth. AI is not a substitute for surgical mentorship, but a powerful amplifier of educational quality. Its thoughtful implementation can foster equity, efficiency, and innovation in orthopaedic trauma training—transforming how surgical competence is acquired, assessed, and advanced."}
{"paperId": "b9174c70e34fdc354f1c786805fb715a11113971", "url": "https://www.semanticscholar.org/paper/b9174c70e34fdc354f1c786805fb715a11113971", "title": "Toward practical human-interpretable explanations", "venue": "Machine-mediated learning", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10994-025-06852-8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10994-025-06852-8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-19", "authors": [{"authorId": "2192607081", "name": "Alon Malach"}, {"authorId": "1486396826", "name": "Amiel Meiseles"}, {"authorId": "50616846", "name": "Ron Bitton"}, {"authorId": "2257295804", "name": "Satoru Momiyama"}, {"authorId": "2336777869", "name": "Toshinori Araki"}, {"authorId": "2168317210", "name": "Jun Furukawa"}, {"authorId": "1724372", "name": "Y. Elovici"}, {"authorId": "1720589", "name": "A. Shabtai"}], "abstract": null}
{"paperId": "b9a7e5e889f062a3c57873fa06b4503e9249ef06", "url": "https://www.semanticscholar.org/paper/b9a7e5e889f062a3c57873fa06b4503e9249ef06", "title": "D-Attn: Decomposed Attention for Large Vision-and-Language Models", "venue": "", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-02-04", "authors": [{"authorId": "2300871261", "name": "Chia-Wen Kuo"}, {"authorId": "9385903", "name": "Sijie Zhu"}, {"authorId": "2293705783", "name": "Fan Chen"}, {"authorId": "2343824924", "name": "Xiaohui Shen"}, {"authorId": "2244622326", "name": "Longyin Wen"}], "abstract": "Large vision-and-language models (LVLMs) have traditionally integrated visual and textual tokens by concatenating them into a single homogeneous input for large language models (LLMs), thereby maximally preserving the pre-trained language capabilities. However, this constrained architecture for visual and textual tokens restricts the design space for processing visual tokens, potentially leading to suboptimal performance and efficiency. In this paper, we propose Decomposed Attention (D-Attn), a more flexible attention architecture for LVLMs, which enables modification of visual token operations without affecting textual-to-textual attention. D-Attn decomposes the 1-D causal self-attention of LVLMs into visual-to-visual, textual-to-visual, and textual-to-textual attentions, and the visual and textual output tokens from the decomposed attentions are merged with a carefully derived weighting strategy, namely $\\alpha$-weighting. Taking advantage of the flexibility, we are able to introduce two critical improvements in visual token processing while maintaining the capacity of pre-trained LLMs: 1) We rectify the biased positional encoding in textual-to-visual attention to boost visual understanding performance. 2) We diagonalize visual-to-visual attention to reduce computation complexity from $O(|V|^2)$ to $O(|V|)$ for $|V|$ visual tokens without compromising performance. Extensive experiments and analysis validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs (\\eg, $5\\times$ faster). Code will be available at https://github.com/bytedance/DecomposedAttention."}
{"paperId": "b9a7fb7623bc6d722ac87e3fe3420f2f0727dcc8", "url": "https://www.semanticscholar.org/paper/b9a7fb7623bc6d722ac87e3fe3420f2f0727dcc8", "title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.11512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-14", "authors": [{"authorId": "2312659239", "name": "Yiyun Zhou"}, {"authorId": "2393369875", "name": "Mingjing Xu"}, {"authorId": "2393911743", "name": "Jingwei Shi"}, {"authorId": "2393964119", "name": "Quanjiang Li"}, {"authorId": "2392755694", "name": "Jingyuan Chen"}], "abstract": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation."}
{"paperId": "b9c05ac2d0488ea1924714ba6c19a803c4ff099f", "url": "https://www.semanticscholar.org/paper/b9c05ac2d0488ea1924714ba6c19a803c4ff099f", "title": "RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-08", "authors": [{"authorId": "2332090320", "name": "Hongzhi Zang"}, {"authorId": "2385344299", "name": "Mingjie Wei"}, {"authorId": "2303433046", "name": "Si Xu"}, {"authorId": "2381994329", "name": "Yongji Wu"}, {"authorId": "2373325170", "name": "Zhen Guo"}, {"authorId": "2352563942", "name": "Yuanqing Wang"}, {"authorId": "2372664232", "name": "Hao Lin"}, {"authorId": "2385363332", "name": "Liangzhi Shi"}, {"authorId": "2276622116", "name": "Yuqing Xie"}, {"authorId": "2364947689", "name": "Zhexuan Xu"}, {"authorId": "2381646406", "name": "Zhihao Liu"}, {"authorId": "2382574462", "name": "Kang Chen"}, {"authorId": "2323010636", "name": "Wenhao Tang"}, {"authorId": "2315889693", "name": "Quanlu Zhang"}, {"authorId": "2384696907", "name": "Weinan Zhang"}, {"authorId": "2306089515", "name": "Chaoyang Yu"}, {"authorId": "2357733528", "name": "Yu Wang"}], "abstract": "Recent progress in vision and language foundation models has significantly advanced multimodal understanding, reasoning, and generation, inspiring a surge of interest in extending such capabilities to embodied settings through vision-language-action (VLA) models. Yet, most VLA models are still trained with supervised fine-tuning (SFT), which struggles to generalize under distribution shifts due to error accumulation. Reinforcement learning (RL) offers a promising alternative by directly optimizing task performance through interaction, but existing attempts remain fragmented and lack a unified platform for fair and systematic comparison across model architectures and algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and efficient framework for scalable RL training of VLA models. The system adopts a highly flexible resource allocation design that addresses the challenge of integrating rendering, training, and inference in RL+VLA training. In particular, for GPU-parallelized simulators, RLinf-VLA implements a novel hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup in training. Through a unified interface, RLinf-VLA seamlessly supports diverse VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g., PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a unified model achieves 98.11\\% across 130 LIBERO tasks and 97.66\\% across 25 ManiSkill tasks. Beyond empirical performance, our study distills a set of best practices for applying RL to VLA training and sheds light on emerging patterns in this integration. Furthermore, we present preliminary deployment on a real-world Franka robot, where RL-trained policies exhibit stronger generalization than those trained with SFT. We envision RLinf-VLA as a foundation to accelerate and standardize research on embodied intelligence."}
{"paperId": "b9c35fd767f3450920215393bfafd6ced97d62d5", "url": "https://www.semanticscholar.org/paper/b9c35fd767f3450920215393bfafd6ced97d62d5", "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.09262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-12", "authors": [{"authorId": "2375962897", "name": "Dongwoo Kang"}, {"authorId": "9282980", "name": "Akhil Perincherry"}, {"authorId": "2264183027", "name": "Zachary Coalson"}, {"authorId": "2375819889", "name": "Aiden Gabriel"}, {"authorId": "2351101990", "name": "Stefan Lee"}, {"authorId": "2264459787", "name": "Sanghyun Hong"}], "abstract": "An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."}
{"paperId": "b9f0d92c5ec0d00d8d2c5c89d32ea2d481b81417", "url": "https://www.semanticscholar.org/paper/b9f0d92c5ec0d00d8d2c5c89d32ea2d481b81417", "title": "Fragmentation and multithreading of experience in the default-mode network", "venue": "bioRxiv", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12462485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-06", "authors": [{"authorId": "1913080186", "name": "Fahd Yazin"}, {"authorId": "2075705079", "name": "Gargi Majumdar"}, {"authorId": "2327868453", "name": "Neil Bramley"}, {"authorId": "2237152026", "name": "Paul Hoffman"}], "abstract": "Reliance on internal predictive models of the world is central to many theories of human cognition. Yet it is unknown whether humans acquired multiple separate internal models, each evolved for a specific domain, or maintain a globally unified representation. Using fMRI during naturalistic experiences (movie watching and narrative listening), we show that three topographically distinct midline prefrontal cortical regions perform distinct predictive operations. The ventromedial PFC updates contextual predictions (States), the anteromedial PFC governs reference frame shifts for social predictions (Agents), and the dorsomedial PFC predicts transitions across the abstract state spaces (Actions). Prediction-error-driven neural transitions in these regions, indicative of model updates, coincided with subjective belief changes in a domain-specific manner. We find these parallel top-down predictions are unified and selectively integrated with sensory streams in the Precuneus, shaping participants’ ongoing experience. Results generalized across sensory modalities and content, suggesting humans recruit abstract, modular predictive models for both vision and language. Our results highlight a key feature of human world modeling: fragmenting information into abstract domains before global integration."}
{"paperId": "ba229dbebda837117d5a6cea26ccdfb0b4c4971d", "url": "https://www.semanticscholar.org/paper/ba229dbebda837117d5a6cea26ccdfb0b4c4971d", "title": "Towards deployment-centric multimodal AI beyond vision and language", "venue": "Nature Machine Intelligence", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-04", "authors": [{"authorId": "2237586245", "name": "Xianyuan Liu"}, {"authorId": "2348249979", "name": "Jiayang Zhang"}, {"authorId": "2149164085", "name": "Shuo Zhou"}, {"authorId": "2207681860", "name": "T. L. V. D. Plas"}, {"authorId": "2220758263", "name": "Avish Vijayaraghavan"}, {"authorId": "2353952106", "name": "Anastasiia Grishina"}, {"authorId": "2353955610", "name": "Mengdie Zhuang"}, {"authorId": "145560694", "name": "Daniel Schofield"}, {"authorId": "2353954821", "name": "Christopher Tomlinson"}, {"authorId": "2267157647", "name": "Yuhan Wang"}, {"authorId": "2321728152", "name": "Ruizhe Li"}, {"authorId": "2279023082", "name": "Louisa van Zeeland"}, {"authorId": "2295669471", "name": "Sina Tabakhi"}, {"authorId": "2353955112", "name": "Cyndie Demeocq"}, {"authorId": "2354127585", "name": "Xiang Li"}, {"authorId": "2354050947", "name": "Arunav Das"}, {"authorId": "2353955283", "name": "Orlando Timmerman"}, {"authorId": "2268404111", "name": "Thomas Baldwin-McDonald"}, {"authorId": "2182815661", "name": "Jinge Wu"}, {"authorId": "65985908", "name": "Peizhen Bai"}, {"authorId": "2175555681", "name": "Zahraa Al Sahili"}, {"authorId": "2218087361", "name": "Omnia Alwazzan"}, {"authorId": "2161120152", "name": "T. N. Do"}, {"authorId": "2350753558", "name": "M. N. Suvon"}, {"authorId": "2354638704", "name": "Angeline Wang"}, {"authorId": "2304554001", "name": "Lucia Cipolina-Kun"}, {"authorId": "2353951600", "name": "Luigi A. Moretti"}, {"authorId": "2212025441", "name": "Lucas Farndale"}, {"authorId": "2353953131", "name": "Nitisha Jain"}, {"authorId": "2353955035", "name": "Natalia Efremova"}, {"authorId": "46960079", "name": "Yan Ge"}, {"authorId": "2353954527", "name": "Marta Varela"}, {"authorId": "2243472111", "name": "Hak-Keung Lam"}, {"authorId": "1386958722", "name": "Oya Çeliktutan"}, {"authorId": "2353956065", "name": "Ben R. Evans"}, {"authorId": "1410786664", "name": "Alejandro Coca-Castro"}, {"authorId": "2241781737", "name": "Honghan Wu"}, {"authorId": "3323364", "name": "Z. Abdallah"}, {"authorId": "2297434703", "name": "Chen Chen"}, {"authorId": "3382327", "name": "V. Danchev"}, {"authorId": "2353951553", "name": "Nataliya Tkachenko"}, {"authorId": "2116000717", "name": "Lei Lu"}, {"authorId": "2274201171", "name": "Tingting Zhu"}, {"authorId": "2239180011", "name": "Gregory G. Slabaugh"}, {"authorId": "2354191900", "name": "Roger K. Moore"}, {"authorId": "2353955214", "name": "William K. Cheung"}, {"authorId": "2140276738", "name": "Peter H. Charlton"}, {"authorId": "2349372750", "name": "Haiping Lu"}], "abstract": "Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction and decision-making across disciplines such as healthcare, science and engineering. However, most multimodal AI advances focus on models for vision and language data, and their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early on to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasize deeper integration across multiple levels of multimodality through stakeholder engagement and interdisciplinary collaboration to broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability and finance. By fostering interdisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact. Multimodal AI combines different types of data to improve decision-making in fields such as healthcare and engineering, but work so far has focused on vision and language models. To make these systems more usable in the real world, Liu et al. discuss the need to develop approaches with deployment in mind from the start, working closely with experts across relevant disciplines."}
{"paperId": "ba953a7be7197a825d06215742ca8a294066016b", "url": "https://www.semanticscholar.org/paper/ba953a7be7197a825d06215742ca8a294066016b", "title": "Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.08460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-14", "authors": [{"authorId": "25199716", "name": "Mihai Masala"}, {"authorId": "2243189217", "name": "Marius Leordeanu"}], "abstract": "In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach."}
{"paperId": "bac5da68388fae33c9a7fc804c0f47e7ce0e2ca6", "url": "https://www.semanticscholar.org/paper/bac5da68388fae33c9a7fc804c0f47e7ce0e2ca6", "title": "SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.06464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-08", "authors": [{"authorId": "2288239335", "name": "Hanyang Peng"}, {"authorId": "2220536379", "name": "Shuang Qin"}, {"authorId": "2333404208", "name": "Yue Yu"}, {"authorId": "2364698521", "name": "Fangqing Jiang"}, {"authorId": "2288817206", "name": "Hui Wang"}, {"authorId": "2223107434", "name": "Wen Gao"}], "abstract": "Adam has proven remarkable successful in training deep neural networks, but the mechanisms underlying its empirical successes and limitations remain underexplored. In this study, we demonstrate that the effectiveness of Adam stems largely from its similarity to SignSGD in robustly handling large gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes due to its uncontrolled update scaling. To enhance the advantage of Adam and mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with three key innovations. \\emph{First}, S3 generalizes the sign-like update by employing a flexible $p$-th order momentum ($p \\geq 1$) in the denominator, departing from the conventional second-order momentum (variance) preconditioning. This design enables enhanced performance while achieving stable training even with aggressive learning rates. \\emph{Second}, S3 minimizes the occurrences of loss spikes through unified exponential moving average coefficients for numerator and denominator momenta, which inherently bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \\emph{Third}, S3 incorporates an equivalent Nesterov's accelerated gradient(NAG) module, accelerating convergence without memory overhead. Theoretically, we prove that S3 achieves the optimal convergence rate of $O\\left(\\frac{1}{T^{\\sfrac{1}{4}}}\\right)$ for general nonconvex stochastic optimization under weak assumptions. Extensive experiments across a range of vision and language tasks show that \\textsf{\\small S3} not only converges more rapidly and improves performance but also rarely experiences loss spikes, even with a \\textbf{$\\bm{10 \\times}$} larger learning rate. In fact, S3 delivers performance comparable to or better than AdamW with \\textbf{$2 \\times$} the training steps, establishing its efficacy in both efficiency and final task performance."}
{"paperId": "bb79cd2ea38f6a79ebeb4108cae1b07bd0bad885", "url": "https://www.semanticscholar.org/paper/bb79cd2ea38f6a79ebeb4108cae1b07bd0bad885", "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i9.32974", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i9.32974?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i9.32974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2316521512", "name": "Yunzhe Xu"}, {"authorId": "2316526437", "name": "Yiyuan Pan"}, {"authorId": "2274060921", "name": "Zhe Liu"}, {"authorId": "2316674851", "name": "Hesheng Wang"}], "abstract": "Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence."}
{"paperId": "bbc6a4aaa19a6d210d67e902558fc3f8639d9774", "url": "https://www.semanticscholar.org/paper/bbc6a4aaa19a6d210d67e902558fc3f8639d9774", "title": "Preliminary Experiments of Inferring Human Intention by Analyzing Time-Series Images from Multiple Views", "venue": "IEEE/SICE International Symposium on System Integration", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SII59315.2025.10870983?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SII59315.2025.10870983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-21", "authors": [{"authorId": "2266502940", "name": "Masae Yokota"}, {"authorId": "7843281", "name": "Sarthak Pathak"}, {"authorId": "1701851", "name": "M. Niitsuma"}, {"authorId": "2260769324", "name": "Kazunori Umeda"}], "abstract": "The objective of this research is to construct an intelligent human-robot environment that can infer human behavorial intentions and adjust the space accordingly. In this research, we perform preliminary studies and verify whether inferring of human behavorial intention can be done from image information alone. First, the vision and Language Model (VLM) and object detection methods are used to infer possible human actions for each object detected in images. Differences between inference results and actual behavior are identified and methods needed for more accurate inference are discussed. The spatial relationship between the skeletal points and the object by observation reveals which skeletal points to focus on in order to predict the behavior. We confirmed that it is possible to predict behaviors by focusing on the neck point for actions performed with the clear intention of sitting on or passing by a chair. Parameters for the neck skeletal points are selected and each behavior is predicted by a Temporal Convolutional Network (TCN) with 91% performance. Through preliminary experiments, we discuss the methods necessary for inferring human behavioral intentions from images."}
{"paperId": "bc1105f0754ce719175309bc0cd8f9d2390d1f98", "url": "https://www.semanticscholar.org/paper/bc1105f0754ce719175309bc0cd8f9d2390d1f98", "title": "Integrated platforms and techniques for photonic neural networks", "venue": "npj Nanophotonics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s44310-025-00088-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s44310-025-00088-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2315245615", "name": "Haoran Zhang"}, {"authorId": "2315646460", "name": "Yuhang Song"}, {"authorId": "2277630293", "name": "Shifan Chen"}, {"authorId": "2277578921", "name": "Yunping Bai"}, {"authorId": "2279782910", "name": "Xingyuan Xu"}, {"authorId": "2307328494", "name": "Chaoran Huang"}, {"authorId": "2388277069", "name": "Jian Wang"}, {"authorId": "2387625218", "name": "Hongwei Chen"}, {"authorId": "2249224068", "name": "David J. Moss"}, {"authorId": "2186547572", "name": "Kun Xu"}], "abstract": null}
{"paperId": "bc43690c013b752f0ccc42c0f36a824bf1d1d523", "url": "https://www.semanticscholar.org/paper/bc43690c013b752f0ccc42c0f36a824bf1d1d523", "title": "Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.12735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-16", "authors": [{"authorId": "39457817", "name": "Ankita Raj"}, {"authorId": "2325144866", "name": "Chetan Arora"}], "abstract": "Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting."}
{"paperId": "bc6bcd51a83dec3e87a4321a0bc48053928e3e82", "url": "https://www.semanticscholar.org/paper/bc6bcd51a83dec3e87a4321a0bc48053928e3e82", "title": "Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-24", "authors": [{"authorId": "1519512669", "name": "J. Grannen"}, {"authorId": "2394172859", "name": "Michelle Pan"}, {"authorId": "2394172723", "name": "Kenneth Llontop"}, {"authorId": "2394669633", "name": "Cherie Ho"}, {"authorId": "10708100", "name": "Mark Zolotas"}, {"authorId": "2323565347", "name": "Jeannette Bohg"}, {"authorId": "1779671", "name": "Dorsa Sadigh"}], "abstract": "Foundation models (FM) have unlocked powerful zero-shot capabilities in vision and language, yet their reliance on internet pretraining data leaves them brittle in unstructured, real-world settings. The messy, real-world data encountered during deployment (e.g. occluded or multilingual text) remains massively underrepresented in existing corpora. Robots, as embodied agents, are uniquely positioned to close this gap: they can act in physical environments to collect large-scale, real-world data that enriches FM training with precisely the examples current models lack. We introduce the Robot-Powered Data Flywheel, a framework that transforms robots from FM consumers into data generators. By deploying robots equipped with FMs in the wild, we enable a virtuous cycle: robots perform useful tasks while collecting real-world data that improves both domain-specific adaptation and domain-adjacent generalization. We instantiate this framework with Scanford, a mobile manipulator deployed in the East Asia Library for 2 weeks. Scanford autonomously scans shelves, identifies books using a vision-language model (VLM), and leverages the library catalog to label images without human annotation. This deployment both aids librarians and produces a dataset to finetune the underlying VLM, improving performance on the domain-specific in-the-wild library setting and on domain-adjacent multilingual OCR benchmarks. Using data collected from 2103 shelves, Scanford improves VLM performance on book identification from 32.0% to 71.8% and boosts domain-adjacent multilingual OCR from 24.8% to 46.6% (English) and 30.8% to 38.0% (Chinese), while saving an ~18.7 hrs of human time. These results highlight how robot-powered data flywheels can both reduce human effort in real deployments and unlock new pathways for continually adapting FMs to the messiness of reality. More details are at: https://scanford-robot.github.io"}
{"paperId": "bc79827421f570cecb91ce805b59fc4fceb1c4bc", "url": "https://www.semanticscholar.org/paper/bc79827421f570cecb91ce805b59fc4fceb1c4bc", "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.10756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-12", "authors": [{"authorId": "2272993958", "name": "Yuhang Zhang"}, {"authorId": "2367010609", "name": "Haosheng Yu"}, {"authorId": "31787367", "name": "Jiaping Xiao"}, {"authorId": "2143647020", "name": "Mir Feroskhan"}], "abstract": "Vision-and-language navigation (VLN) is a long-standing challenge in autonomous robotics, aiming to empower agents with the ability to follow human instructions while navigating complex environments. Two key bottlenecks remain in this field: generalization to out-of-distribution environments and reliance on fixed discrete action spaces. To address these challenges, we propose Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles (UAVs) to execute language-guided flight. Without the requirement for localization or active ranging sensors, VLFly outputs continuous velocity commands purely from egocentric observations captured by an onboard monocular camera. The VLFly integrates three modules: an instruction encoder based on a large language model (LLM) that reformulates high-level language into structured prompts, a goal retriever powered by a vision-language model (VLM) that matches these prompts to goal images via vision-language similarity, and a waypoint planner that generates executable trajectories for real-time UAV control. VLFly is evaluated across diverse simulation environments without additional fine-tuning and consistently outperforms all baselines. Moreover, real-world VLN tasks in indoor and outdoor environments under direct and indirect instructions demonstrate that VLFly achieves robust open-vocabulary goal understanding and generalized navigation capabilities, even in the presence of abstract language input."}
{"paperId": "bcd2f2c45904515f0425d32ea59df9c226f97f97", "url": "https://www.semanticscholar.org/paper/bcd2f2c45904515f0425d32ea59df9c226f97f97", "title": "Bandit Guided Submodular Curriculum for Adaptive Subset Selection", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.22944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-28", "authors": [{"authorId": "2290484410", "name": "Prateek Chanda"}, {"authorId": "2395539518", "name": "Prayas Agrawal"}, {"authorId": "2372246941", "name": "Saral Sureka"}, {"authorId": "2395539656", "name": "Lokesh Reddy Polu"}, {"authorId": "2395537305", "name": "Atharv Kshirsagar"}, {"authorId": "2315129124", "name": "Ganesh Ramakrishnan"}], "abstract": "Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce ONLINESUBMOD, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, ONLINESUBMOD outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validationdriven reward metrics offer a principled way to guide the curriculum schedule."}
{"paperId": "bcd8a3df75fa5904cecc3616017f3a9a24dad8ea", "url": "https://www.semanticscholar.org/paper/bcd8a3df75fa5904cecc3616017f3a9a24dad8ea", "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.15541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-18", "authors": [{"authorId": "2176276154", "name": "Oluwadamilola Fasina"}, {"authorId": "1971038267", "name": "R. V. Pohle"}, {"authorId": "2365687988", "name": "Pei-Chun Su"}, {"authorId": "2270664865", "name": "Ronald R Coifman"}], "abstract": "We examine the intrinsic (within the attention head) and extrinsic (amongst the attention heads) structure of the self-attention mechanism in transformers. Theoretical evidence for invariance of the self-attention mechanism to softmax activation is obtained by appealing to paradifferential calculus, (and is supported by computational examples), which relies on the intrinsic organization of the attention heads. Furthermore, we use an existing methodology for hierarchical organization of tensors to examine network structure by constructing hierarchal partition trees with respect to the query, key, and head axes of network 3-tensors. Such an organization is consequential since it allows one to profitably execute common signal processing tasks on a geometry where the organized network 3-tensors exhibit regularity. We exemplify this qualitatively, by visualizing the hierarchical organization of the tree comprised of attention heads and the diffusion map embeddings, and quantitatively by investigating network sparsity with the expansion coefficients of individual attention heads and the entire network with respect to the bi and tri-haar bases (respectively) on the space of queries, keys, and heads of the network. To showcase the utility of our theoretical and methodological findings, we provide computational examples using vision and language transformers. The ramifications of these findings are two-fold: (1) a subsequent step in interpretability analysis is theoretically admitted, and can be exploited empirically for downstream interpretability tasks (2) one can use the network 3-tensor organization for empirical network applications such as model pruning (by virtue of network sparsity) and network architecture comparison."}
{"paperId": "bcde2bc31126f60a81a45fcd46cb2579a4a3f9cc", "url": "https://www.semanticscholar.org/paper/bcde2bc31126f60a81a45fcd46cb2579a4a3f9cc", "title": "Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19561, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-24", "authors": [{"authorId": null, "name": "Zecheng Pan"}, {"authorId": "2344949577", "name": "Zhikang Chen"}, {"authorId": "2383513827", "name": "Ding Li"}, {"authorId": "2394469810", "name": "Min Zhang"}, {"authorId": "2392914911", "name": "Sen Cui"}, {"authorId": "2394246679", "name": "Hongshuo Jin"}, {"authorId": "2395359092", "name": "Luqi Tao"}, {"authorId": "2364849920", "name": "Yi Yang"}, {"authorId": "2391707362", "name": "Deheng Ye"}, {"authorId": "2391755644", "name": "Yu Zhang"}, {"authorId": "2397877875", "name": "Tingting Zhu"}, {"authorId": "2283413084", "name": "Tianling Ren"}], "abstract": "Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging."}
{"paperId": "bd9137a6f07ceaffefddfa03f9539378167060c9", "url": "https://www.semanticscholar.org/paper/bd9137a6f07ceaffefddfa03f9539378167060c9", "title": "LMNav: Landmark-Centric Multimodal Reasoning for Vision-And-Language Navigation in Continuous Environments", "venue": "2025 7th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IoTAAI66837.2025.11213348?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IoTAAI66837.2025.11213348, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-09-12", "authors": [{"authorId": "2388822072", "name": "Kun Luo"}, {"authorId": "2393378732", "name": "Jinghui Qin"}, {"authorId": "2389023887", "name": "Yicong Chen"}], "abstract": "This paper presents LMNav, a landmark-centric navigation framework for Vision-And-Language Navigation in Continuous Environments (VLN-CE). Inspired by the way humans navigate using landmarks and their spatial relationships, LMNav explicitly extracts semantic landmarks and their temporal order from natural language instructions via a language model, constructing a structured directed graph. This graph guides both the agent’s reasoning and perception processes. To enhance visual understanding, LMNav employs view expansion and a semantic-depth fusion module that integrates semantic segmentation and depth cues using an SE attention mechanism. Landmark features are further modulated by a dynamic masking mechanism based on distance, and then fused with RGB embeddings via a cross-attention module. These rich multimodal representations are injected into the policy network to improve decision-making. We evaluate LMNav on the R2R-CE benchmark using standard VLN metrics. The proposed model achieves a success rate of 51.6% and an SPL of 43.0, while also reducing trajectory length compared to baselines. The results demonstrate that LMNav enables more accurate and efficient navigation by leveraging structured landmark reasoning and cross-modal integration."}
{"paperId": "bdcc980cced97e519c04ae8947aabe0761cec3ac", "url": "https://www.semanticscholar.org/paper/bdcc980cced97e519c04ae8947aabe0761cec3ac", "title": "Structural Equation Modeling for Analysis of Vision and Language Standards in the Knowledge Sharing Process of Students in Higher Education", "venue": "JSAI (Journal Scientific and Applied Informatics)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.36085/jsai.v8i1.7909?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.36085/jsai.v8i1.7909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-31", "authors": [{"authorId": "2315173706", "name": "Vina Ayumi"}], "abstract": "This study aims to analyze the factors influencing Tacit Knowledge Sharing (TKS) among university students using Structural Equation Modeling (SEM) based on Partial Least Squares (PLS). The research model focuses on three main factors: Identification, Shared Language, and Shared Vision, which are believed to contribute to knowledge sharing in academic environments. Data were collected through self-administered questionnaires from 420 students, selected using the simple random sampling method. Hypothesis testing results indicate that Identification (β = 0.253, p < 0.05) and Shared Language (β = 0.326, p < 0.05) have a significant influence on TKS, while Shared Vision (β = 0.095, p = 0.057) has a weaker effect. These findings confirm that students’ sense of belonging within the academic community and shared language in communication play a crucial role in enhancing knowledge sharing, whereas a shared vision requires further strengthening strategies. This study provides implications for universities in developing policies that promote a knowledge-sharing culture to improve the quality of learning and academic collaboration."}
{"paperId": "be42a6b60de4bad903bec61605e9255d3b49b1cf", "url": "https://www.semanticscholar.org/paper/be42a6b60de4bad903bec61605e9255d3b49b1cf", "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.09843, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-14", "authors": [{"authorId": "2265493455", "name": "Lu Yue"}, {"authorId": "2340676116", "name": "Dongliang Zhou"}, {"authorId": "2337780010", "name": "Liang Xie"}, {"authorId": "2140620437", "name": "Erwei Yin"}, {"authorId": "2278836201", "name": "Feitian Zhang"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments."}
{"paperId": "be87b8d00ee60927e2de2b2f3ab79e41750bb516", "url": "https://www.semanticscholar.org/paper/be87b8d00ee60927e2de2b2f3ab79e41750bb516", "title": "VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.18214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-21", "authors": [{"authorId": "2306781707", "name": "Shruti Palaskar"}, {"authorId": "2386818254", "name": "Leon Gatys"}, {"authorId": "2386817215", "name": "Mona Abdelrahman"}, {"authorId": "2386816501", "name": "Mar Jacobo"}, {"authorId": "2386818496", "name": "Larry Lindsey"}, {"authorId": "2077592767", "name": "Rutika Moharir"}, {"authorId": "2386817269", "name": "Gunnar Lund"}, {"authorId": "2386853408", "name": "Yang Xu"}, {"authorId": "2385566176", "name": "Navid Shiee"}, {"authorId": "2386817328", "name": "Jeffrey Bigham"}, {"authorId": "2313910276", "name": "Charlie Maalouf"}, {"authorId": "2364823954", "name": "J. Y. Cheng"}], "abstract": "Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety."}
{"paperId": "bf3dd8067d66d619a0fc7cde8f5395d3c342dfad", "url": "https://www.semanticscholar.org/paper/bf3dd8067d66d619a0fc7cde8f5395d3c342dfad", "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.04675, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-06", "authors": [{"authorId": "2333919046", "name": "Jinlai Liu"}, {"authorId": "2334135635", "name": "Jian Han"}, {"authorId": "2333893614", "name": "Bin Yan"}, {"authorId": "2344866056", "name": "Hui Wu"}, {"authorId": "2344598263", "name": "Fengda Zhu"}, {"authorId": "2390974659", "name": "Xing Wang"}, {"authorId": "2294789302", "name": "Yi Jiang"}, {"authorId": "2294806434", "name": "Bingyue Peng"}, {"authorId": "2244754235", "name": "Zehuan Yuan"}], "abstract": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation."}
{"paperId": "bf4c23552cec77e2fc121fb934094201e87036f2", "url": "https://www.semanticscholar.org/paper/bf4c23552cec77e2fc121fb934094201e87036f2", "title": "AI-ENABLED INTERACTIVE INSTALLATIONS FOR MODERN MUSEUMS", "venue": "ShodhKosh Journal of Visual and Performing Arts", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.29121/shodhkosh.v6.i3.2025.6670?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.29121/shodhkosh.v6.i3.2025.6670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-30", "authors": [{"authorId": "2355115516", "name": "Preetjot Singh"}, {"authorId": "2397262563", "name": "Nishant Trivedi"}, {"authorId": "2397263074", "name": "Priyanka S. Utage"}, {"authorId": "2397222541", "name": "Neha"}, {"authorId": "2397300087", "name": "Saiqa Khan"}, {"authorId": "2397262624", "name": "Deepak Minhas"}], "abstract": "The high development of Artificial Intelligence (AI) has altered the visitor interaction paradigm in the contemporary museums. The paper describes the design, implementation, and evaluation of AI-enabled interactive installations that provide more cultural experience by improving the mechanisms of intelligent perception, the personalisation and adaptive interactivity. The suggested system will combine computer vision, natural language processing and multimodal sensing to analyze visitor behaviors and preference in real time. Using deep learning for gesture and speech analytics, the installation adapts to individual users/groups and responds through various tech-based interventions such as narrative, visual projection and ambient response. The system architecture combines low-latency interaction by using edge computing with continual learning and content optimization by having a cloud-based analytics layer. In pilot museum testing, it showed that visitor immersion, retention of learning and engagement measures were significantly improved when compared to traditional fixed exhibitions. The findings show that the duration of interaction with users has increased by 45 % and that the accuracy of the information that is displayed has improved by 32 %. In addition, qualitative feedback proved that the emotional appeal and inclusivity achieved by adaptive storytelling mechanisms were significant. This study shows a promise of applying AI-driven installations to transform the space of museums into a participatory, data-driven space that connects art, history, and technology. The next-generation of work is based on ethical governance of data, scalability with a variety of cultural backgrounds, and integration of affective computing to further human and machine co-sensationalize the heritage interpretation process."}
{"paperId": "bf568131562616c95c34f90cf3e5a84837f2f31f", "url": "https://www.semanticscholar.org/paper/bf568131562616c95c34f90cf3e5a84837f2f31f", "title": "OCNav: Object-centric Navigation via Parallel Language Grounding on Semantic Topological Graphs", "venue": "International Journal of Control, Automation and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s12555-025-0531-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s12555-025-0531-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-01", "authors": [{"authorId": "2355046713", "name": "Jeong-Seop Park"}, {"authorId": "2200441184", "name": "Y. Lee"}, {"authorId": "2397823885", "name": "Jong-Chan Park"}, {"authorId": "2267644309", "name": "Sung-Gil Park"}, {"authorId": "2042720002", "name": "Woojin Ahn"}, {"authorId": "2354665515", "name": "J. Woo"}, {"authorId": "2067658204", "name": "M. Lim"}], "abstract": null}
{"paperId": "bf9c2f82807c81147eb2dc62ac8bec7cbb4bc0cc", "url": "https://www.semanticscholar.org/paper/bf9c2f82807c81147eb2dc62ac8bec7cbb4bc0cc", "title": "Medical Vision-Language Modeling With Semantic Interaction and Adaptive Refinement Prompting for Bias Mitigation.", "venue": "IEEE journal of biomedical and health informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JBHI.2025.3631270?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JBHI.2025.3631270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-24", "authors": [{"authorId": "2277810129", "name": "Cheng Li"}, {"authorId": "47504396", "name": "Weijian Huang"}, {"authorId": "2257352642", "name": "Hao Yang"}, {"authorId": "2239160015", "name": "Jiarun Liu"}, {"authorId": "2277686185", "name": "Yong Liang"}, {"authorId": "2239096413", "name": "Shanshan Wang"}], "abstract": "Vision-Language Models (VLMs) have demonstrated impressive capabilities across various medical tasks, including report generation and visual question answering (VQA). However, pixel-level tasks such as image segmentation remain relatively underexplored, despite their critical importance for clinical decision-making, surgical planning, and model interpretability. Moreover, the scarcity of high-quality segmentation annotations in the medical domain often leads to biased data distributions, characterized by imbalances in disease types, anatomical coverage, and image quality. These biases are frequently overlooked during both model development and evaluation, limiting the robustness and real-world applicability of VLMs in healthcare scenarios. In this study, we propose a unified medical vision-language model applicable for a variety of clinical tasks, including report generation, VQA, and pixel-level image segmentation. Within the model, we propose a semantic interaction mechanism aimed at enhancing pixel-level vision and language representation learning. To mitigate the impact of biased data distributions, we explicitly develop an adaptive refinement prompting method involving the iterative re-prompting of hard samples. The proposed method is thoroughly validated through experiments on eight datasets and comparisons with nine state-of-the-art methods. The experimental results indicate that our model achieves superior performance in both medical VQA and segmentation tasks. These results highlight the potential of our approach in advancing the deployment of medical VLMs in real-world clinical applications. Code will be released at: https://github.com/SZUHvern/Unified-Medical-Vision-Language-Modeling."}
{"paperId": "c04d614afb3b663b068a61444518206e3ebc7d3b", "url": "https://www.semanticscholar.org/paper/c04d614afb3b663b068a61444518206e3ebc7d3b", "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-24", "authors": [{"authorId": "2284878860", "name": "Semanto Mondal"}], "abstract": "As a social being, we have an intimate bond with the environment. A plethora of things in human life, such as lifestyle, health, and food are dependent on the environment and agriculture. It comes under our responsibility to support the environment as well as agriculture. However, traditional farming practices often result in inefficient resource use and environmental challenges. To address these issues, precision agriculture has emerged as a promising approach that leverages advanced technologies to optimise agricultural processes. In this work, a hybrid approach is proposed that combines the three different potential fields of model AI: object detection, large language model (LLM), and Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to combine the vision and language models to work together to identify potential diseases in the tree leaf. This study introduces a novel AI-based precision agriculture system that uses Retrieval Augmented Generation (RAG) to provide context-aware diagnoses and natural language processing (NLP) and YOLOv8 for crop disease detection. The system aims to tackle major issues with large language models (LLMs), especially hallucinations and allows for adaptive treatment plans and real-time disease detection. The system provides an easy-to-use interface to the farmers, which they can use to detect the different diseases related to coffee leaves by just submitting the image of the affected leaf the model will detect the diseases as well as suggest potential remediation methodologies which aim to lower the use of pesticides, preserving livelihoods, and encouraging environmentally friendly methods. With an emphasis on scalability, dependability, and user-friendliness, the project intends to improve RAG-integrated object detection systems for wider agricultural applications in the future."}
{"paperId": "c070a4d367f5c562fdf50a76a9ea53c8a58109f3", "url": "https://www.semanticscholar.org/paper/c070a4d367f5c562fdf50a76a9ea53c8a58109f3", "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20776, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-26", "authors": [{"authorId": "2272197593", "name": "Shijie Zhou"}, {"authorId": "2352761661", "name": "Hui Ren"}, {"authorId": "1693059352", "name": "Yijia Weng"}, {"authorId": "2352211743", "name": "Shuwang Zhang"}, {"authorId": "2376006647", "name": "Zhen Wang"}, {"authorId": "1575684088", "name": "Dejia Xu"}, {"authorId": "47774591", "name": "Zhiwen Fan"}, {"authorId": "2269808407", "name": "Suya You"}, {"authorId": "2227945855", "name": "Zhangyang Wang"}, {"authorId": "2349386799", "name": "Leonidas J. Guibas"}, {"authorId": "2425405", "name": "Achuta Kadambi"}], "abstract": "Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The \"X\" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction."}
{"paperId": "c086ea0f0e23fe7a3d377456daab10354b1f2a7f", "url": "https://www.semanticscholar.org/paper/c086ea0f0e23fe7a3d377456daab10354b1f2a7f", "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2308217874", "name": "Pingrui Zhang"}, {"authorId": "2332311229", "name": "Yifei Su"}, {"authorId": "2364365788", "name": "Pengyuan Wu"}, {"authorId": "2273854714", "name": "Dong An"}, {"authorId": "2363587663", "name": "Li Zhang"}, {"authorId": "2145078379", "name": "Zhigang Wang"}, {"authorId": "2287802281", "name": "Dong Wang"}, {"authorId": "2316892567", "name": "Yan Ding"}, {"authorId": "2256773314", "name": "Bin Zhao"}, {"authorId": "2192821449", "name": "Xuelong Li"}], "abstract": "Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}."}
{"paperId": "c0ba431076e0a20a914495fcddd8acb105f49e30", "url": "https://www.semanticscholar.org/paper/c0ba431076e0a20a914495fcddd8acb105f49e30", "title": "Subgroups Matter for Robust Bias Mitigation", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2167911380", "name": "Anissa Alloula"}, {"authorId": "2152376592", "name": "Charles Jones"}, {"authorId": "2260627290", "name": "Ben Glocker"}, {"authorId": "2363571118", "name": "Bartlomiej W. Papie.z"}], "abstract": "Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and presents it as an alternative lever for improving the robustness and fairness of machine learning models."}
{"paperId": "c0d05654afdf8b16756a77b3da41c4925e192986", "url": "https://www.semanticscholar.org/paper/c0d05654afdf8b16756a77b3da41c4925e192986", "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-06", "authors": [{"authorId": "2228615502", "name": "Qiyuan He"}, {"authorId": "2384160604", "name": "Yicong Li"}, {"authorId": "2322625100", "name": "Haotian Ye"}, {"authorId": "2384310489", "name": "Jinghao Wang"}, {"authorId": "2385463206", "name": "Xinyao Liao"}, {"authorId": "2267115363", "name": "P. Heng"}, {"authorId": "2302797121", "name": "Stefano Ermon"}, {"authorId": "2375381565", "name": "James Zou"}, {"authorId": "2293394511", "name": "Angela Yao"}], "abstract": "Visual autoregressive (AR) generation offers a promising path toward unifying vision and language models, yet its performance remains suboptimal against diffusion models. Prior work often attributes this gap to tokenizer limitations and rasterization ordering. In this work, we identify a core bottleneck from the perspective of generator-tokenizer inconsistency, i.e., the AR-generated tokens may not be well-decoded by the tokenizer. To address this, we propose reAR, a simple training strategy introducing a token-wise regularization objective: when predicting the next token, the causal transformer is also trained to recover the visual embedding of the current token and predict the embedding of the target token under a noisy context. It requires no changes to the tokenizer, generation order, inference pipeline, or external models. Despite its simplicity, reAR substantially improves performance. On ImageNet, it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard rasterization-based tokenizer. When applied to advanced tokenizers, it achieves a gFID of 1.42 with only 177M parameters, matching the performance with larger state-of-the-art diffusion models (675M)."}
{"paperId": "c19bf3e7ccdf1037fa5ac14a4f2786bb099bcb77", "url": "https://www.semanticscholar.org/paper/c19bf3e7ccdf1037fa5ac14a4f2786bb099bcb77", "title": "Task-Adaptive Parameter-Efficient Fine-Tuning for Weather Foundation Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.22020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-26", "authors": [{"authorId": "2306768858", "name": "Shilei Cao"}, {"authorId": "2260832033", "name": "Hehai Lin"}, {"authorId": "2382888908", "name": "Jiashun Cheng"}, {"authorId": "2382850600", "name": "Yang Liu"}, {"authorId": "2352853790", "name": "Guowen Li"}, {"authorId": "2382820800", "name": "Xuehe Wang"}, {"authorId": "50844234", "name": "Juepeng Zheng"}, {"authorId": "2353777629", "name": "Haoyuan Liang"}, {"authorId": "2356493279", "name": "Meng Jin"}, {"authorId": "2383968687", "name": "Chengwei Qin"}, {"authorId": "2383105144", "name": "Hong Cheng"}, {"authorId": "2237650343", "name": "Haohuan Fu"}], "abstract": "While recent advances in machine learning have equipped Weather Foundation Models (WFMs) with substantial generalization capabilities across diverse downstream tasks, the escalating computational requirements associated with their expanding scale increasingly hinder practical deployment. Current Parameter-Efficient Fine-Tuning (PEFT) methods, designed for vision or language tasks, fail to address the unique challenges of weather downstream tasks, such as variable heterogeneity, resolution diversity, and spatiotemporal coverage variations, leading to suboptimal performance when applied to WFMs. To bridge this gap, we introduce WeatherPEFT, a novel PEFT framework for WFMs incorporating two synergistic innovations. First, during the forward pass, Task-Adaptive Dynamic Prompting (TADP) dynamically injects the embedding weights within the encoder to the input tokens of the pre-trained backbone via internal and external pattern extraction, enabling context-aware feature recalibration for specific downstream tasks. Furthermore, during backpropagation, Stochastic Fisher-Guided Adaptive Selection (SFAS) not only leverages Fisher information to identify and update the most task-critical parameters, thereby preserving invariant pre-trained knowledge, but also introduces randomness to stabilize the selection. We demonstrate the effectiveness and efficiency of WeatherPEFT on three downstream tasks, where existing PEFT methods show significant gaps versus Full-Tuning, and WeatherPEFT achieves performance parity with Full-Tuning using fewer trainable parameters. The code of this work will be released."}
{"paperId": "c225c14f0cb6b172172ee1dfb2cc013aa19406aa", "url": "https://www.semanticscholar.org/paper/c225c14f0cb6b172172ee1dfb2cc013aa19406aa", "title": "Towards Open World Detection: A Survey", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-22", "authors": [{"authorId": "2190198895", "name": "Andrei-Ștefan Bulzan"}, {"authorId": "1403664270", "name": "C. Cernăzanu-Glăvan"}], "abstract": "For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up today's state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground/background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception."}
{"paperId": "c240515f65866c6f8f792cb6084a689c0f89efd4", "url": "https://www.semanticscholar.org/paper/c240515f65866c6f8f792cb6084a689c0f89efd4", "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.10013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-14", "authors": [{"authorId": "2138342692", "name": "T. Kouwenhoven"}, {"authorId": "2312923173", "name": "Kiana Shahrasbi"}, {"authorId": "2260799876", "name": "T. Verhoef"}], "abstract": "Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like `bouba'with round shapes and `kiki'with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as a measure of model preference, and we use Grad-CAM as a novel approach to interpret visual attention in shape-word matching tasks. Our findings show that these model variants do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both model variants lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models'responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions."}
{"paperId": "c3b248967897b056d92505eac09f6dea6a22ef20", "url": "https://www.semanticscholar.org/paper/c3b248967897b056d92505eac09f6dea6a22ef20", "title": "Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.19353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-31", "authors": [{"authorId": "4500245", "name": "Ting-Yao Hsu"}, {"authorId": "2069602444", "name": "Yi-Li Hsu"}, {"authorId": "40408676", "name": "Shaurya Rohatgi"}, {"authorId": "2261579861", "name": "Chieh-Yang Huang"}, {"authorId": "2182572240", "name": "Ho Yin 'Sam' Ng"}, {"authorId": "2066337266", "name": "Ryan A. Rossi"}, {"authorId": "2261424174", "name": "Sungchul Kim"}, {"authorId": "2338876236", "name": "Tong Yu"}, {"authorId": "1746959", "name": "Lun-Wei Ku"}, {"authorId": "2250617677", "name": "C. L. Giles"}, {"authorId": "2338963370", "name": "T. K. Huang"}], "abstract": "Since the SciCap datasets launch in 2021, the research community has made significant progress in generating captions for scientific figures in scholarly articles. In 2023, the first SciCap Challenge took place, inviting global teams to use an expanded SciCap dataset to develop models for captioning diverse figure types across various academic fields. At the same time, text generation models advanced quickly, with many powerful pre-trained large multimodal models (LMMs) emerging that showed impressive capabilities in various vision-and-language tasks. This paper presents an overview of the first SciCap Challenge and details the performance of various models on its data, capturing a snapshot of the fields state. We found that professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Following this key finding, we conducted detailed analyses to answer this question: Have advanced LMMs solved the task of generating captions for scientific figures?"}
{"paperId": "c3fb229c115fff825cb73fefa2901fd9f333810d", "url": "https://www.semanticscholar.org/paper/c3fb229c115fff825cb73fefa2901fd9f333810d", "title": "Frankenstein Optimizer: Harnessing the Potential by Revisiting Optimization Tricks", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-04", "authors": [{"authorId": "2148966747", "name": "Chia-Wei Hsu"}, {"authorId": "9191249", "name": "N. Tsou"}, {"authorId": "2348409876", "name": "Yu-Cheng Chen"}, {"authorId": "2173715412", "name": "Yang Jeong Park"}, {"authorId": "2305733128", "name": "Ju Li"}], "abstract": "Gradient-based optimization drives the unprecedented performance of modern deep neural network models across diverse applications. Adaptive algorithms have accelerated neural network training due to their rapid convergence rates; however, they struggle to find ``flat minima\"reliably, resulting in suboptimal generalization compared to stochastic gradient descent (SGD). By revisiting various adaptive algorithms' mechanisms, we propose the Frankenstein optimizer, which combines their advantages. The proposed Frankenstein dynamically adjusts first- and second-momentum coefficients according to the optimizer's current state to directly maintain consistent learning dynamics and immediately reflect sudden gradient changes. Extensive experiments across several research domains such as computer vision, natural language processing, few-shot learning, and scientific simulations show that Frankenstein surpasses existing adaptive algorithms and SGD empirically regarding convergence speed and generalization performance. Furthermore, this research deepens our understanding of adaptive algorithms through centered kernel alignment analysis and loss landscape visualization during the learning process. Code is available at https://github.com/acctouhou/Frankenstein_optimizer"}
{"paperId": "c412a1814c3b7d68b593f8387d49f753e4fa8f31", "url": "https://www.semanticscholar.org/paper/c412a1814c3b7d68b593f8387d49f753e4fa8f31", "title": "Enhancing Transformers Through Conditioned Embedded Tokens", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-19", "authors": [{"authorId": "101834772", "name": "Hemanth Saratchandran"}, {"authorId": "2293303939", "name": "Simon Lucey"}], "abstract": "Transformers have transformed modern machine learning, driving breakthroughs in computer vision, natural language processing, and robotics. At the core of their success lies the attention mechanism, which enables the modeling of global dependencies among input tokens. However, we reveal that the attention block in transformers suffers from inherent ill-conditioning, which hampers gradient-based optimization and leads to inefficient training. To address this, we develop a theoretical framework that establishes a direct relationship between the conditioning of the attention block and that of the embedded tokenized data. Building on this insight, we introduce conditioned embedded tokens, a method that systematically modifies the embedded tokens to improve the conditioning of the attention mechanism. Our analysis demonstrates that this approach significantly mitigates ill-conditioning, leading to more stable and efficient training. We validate our methodology across various transformer architectures, achieving consistent improvements in image classification, object detection, instance segmentation, and natural language processing, highlighting its broad applicability and effectiveness."}
{"paperId": "c42e5271cf7c4f1f2c9e210883dfae0a7f920908", "url": "https://www.semanticscholar.org/paper/c42e5271cf7c4f1f2c9e210883dfae0a7f920908", "title": "Escaping Plato's Cave: JAM for Aligning Independently Trained Vision and Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.01201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-01", "authors": [{"authorId": "2372611282", "name": "Lauren Hyoseo Yoon"}, {"authorId": "2374212687", "name": "Yisong Yue"}, {"authorId": "2372371633", "name": "Been Kim"}], "abstract": "Independently trained vision and language models inhabit disjoint representational spaces, shaped by their respective modalities, objectives, and architectures. The Platonic Representation Hypothesis (PRH) suggests these models may nonetheless converge toward a shared statistical model of reality. This raises a fundamental question: can we move beyond post-hoc detection of such alignment and explicitly optimize for it? We argue this challenge is most critical in fine-grained contextual distinctions-where multiple descriptions share global semantics but differ in subtle compositional details. We address this with the Joint Autoencoder Modulator (JAM), which aligns frozen unimodal models by jointly training modality-specific autoencoders with coordinated reconstruction and cross-modal alignment objectives. We systematically evaluate JAM across three design axes: (i) alignment objectives, introducing our multimodal Spread Loss that outperforms classic contrastive methods; (ii) the layer depth at which alignment is most effective; and (iii) the role of foundation model scale in representational convergence. Our findings show that JAM reliably induces alignment even across independently trained representations, offering both theoretical insight into the structure of shared semantics and practical guidance for transforming generalist unimodal foundations into specialist multimodal models."}
{"paperId": "c468b28fc698346aa1b1929d6a41a7f025be70c9", "url": "https://www.semanticscholar.org/paper/c468b28fc698346aa1b1929d6a41a7f025be70c9", "title": "Measurement to Meaning: A Validity-Centered Framework for AI Evaluation", "venue": "arXiv.org", "year": 2025, "citationCount": 14, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-13", "authors": [{"authorId": "2041792512", "name": "Olawale Salaudeen"}, {"authorId": "2209882963", "name": "Anka Reuel"}, {"authorId": "2362189679", "name": "Ahmed M. Ahmed"}, {"authorId": "2296770804", "name": "Suhana Bedi"}, {"authorId": "2362035178", "name": "Zachary Robertson"}, {"authorId": "2273925278", "name": "Sudharsan Sundar"}, {"authorId": "2375749942", "name": "Ben Domingue"}, {"authorId": "2349388428", "name": "Angelina Wang"}, {"authorId": "143812875", "name": "Oluwasanmi Koyejo"}], "abstract": "While the capabilities and utility of AI systems have advanced, rigorous norms for evaluating these systems have lagged. Grand claims, such as models achieving general reasoning capabilities, are supported with model performance on narrow benchmarks, like performance on graduate-level exam questions, which provide a limited and potentially misleading assessment. We provide a structured approach for reasoning about the types of evaluative claims that can be made given the available evidence. For instance, our framework helps determine whether performance on a mathematical benchmark is an indication of the ability to solve problems on math tests or instead indicates a broader ability to reason. Our framework is well-suited for the contemporary paradigm in machine learning, where various stakeholders provide measurements and evaluations that downstream users use to validate their claims and decisions. At the same time, our framework also informs the construction of evaluations designed to speak to the validity of the relevant claims. By leveraging psychometrics'breakdown of validity, evaluations can prioritize the most critical facets for a given claim, improving empirical utility and decision-making efficacy. We illustrate our framework through detailed case studies of vision and language model evaluations, highlighting how explicitly considering validity strengthens the connection between evaluation evidence and the claims being made."}
{"paperId": "c4e13b0800d468a15358017b675a62b2894ac541", "url": "https://www.semanticscholar.org/paper/c4e13b0800d468a15358017b675a62b2894ac541", "title": "Multi-Modal Understanding and Generation for Object Tracking", "venue": "IEEE transactions on circuits and systems for video technology (Print)", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCSVT.2024.3510735?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCSVT.2024.3510735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-01", "authors": [{"authorId": "2220210868", "name": "Hong Zhu"}, {"authorId": "2337591898", "name": "Pingping Zhang"}, {"authorId": "2258410043", "name": "Lei Xue"}, {"authorId": "1643669386", "name": "Guangling Yuan"}], "abstract": "Vision-Language Tracking (VLT) aims to predict the target state in video sequences using two types of heterogeneous information: 1) the static text description detailing main characteristics of the tracked object, and 2) the dynamic image patches containing the target and its surroundings. However, as the tracking proceeds, inconsistencies may arise between the linguistic information embedded in the text description and the visual representations stored in the search images. In such cases, the direct fusion of vision and language could result in conflicts. To tackle this issue, we propose MugTracker, which integrates image-to-text generation into the VLT framework and attempts a generative updating way to mitigate the effects of inconsistencies. Specifically, we design two branch tasks: multi-modal understanding for reasoning and multi-modal generation for updating. We develop a dynamic text generator based on the hybrid architecture of the pre-trained foundation model BLIP and adaptively update the text reference as the context varies for more accurate target modeling. The semantically consistent visual and linguistic representations are then aligned and associated by the reasoning branch built on the BLIP dual-encoder to infer the target state. To better transfer the foundation model to build a strong tracker, we introduce the proposed TE-Adapter in the visual components for target enhancement and Text-Adapter in the linguistic components to strengthen the learning of discriminative semantics. Our MugTracker has been extensively evaluated on three datasets, and the superior performance compared to the state-of-the-arts demonstrates its effectiveness."}
{"paperId": "c509246b052e0d5a54e3eda1bd1450b5b748c23f", "url": "https://www.semanticscholar.org/paper/c509246b052e0d5a54e3eda1bd1450b5b748c23f", "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.15703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-19", "authors": [{"authorId": "2293220595", "name": "Beichen Zhang"}, {"authorId": "12862495", "name": "Yuhang Zang"}, {"authorId": "2118187561", "name": "Xiao-wen Dong"}, {"authorId": "50206929", "name": "Yuhang Cao"}, {"authorId": "31463937", "name": "Haodong Duan"}, {"authorId": "2237734015", "name": "Dahua Lin"}, {"authorId": "2267494294", "name": "Jiaqi Wang"}], "abstract": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33\\% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/InternLM/ARC-VL."}
{"paperId": "c52501a96211d82b127428cab3f8c3e46f42bf8e", "url": "https://www.semanticscholar.org/paper/c52501a96211d82b127428cab3f8c3e46f42bf8e", "title": "Expert Routing with Synthetic Data for Domain Incremental Learning", "venue": "Trans. Mach. Learn. Res.", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2284867000", "name": "Yewon Byun"}, {"authorId": "47613860", "name": "Sanket Vaibhav Mehta"}, {"authorId": "2329369780", "name": "Saurabh Garg"}, {"authorId": "2268272", "name": "Emma Strubell"}, {"authorId": "2293170828", "name": "Michael Oberst"}, {"authorId": "2293171727", "name": "Bryan Wilder"}, {"authorId": "2329736652", "name": "Z. Lipton"}], "abstract": null}
{"paperId": "c5355e5c89a7a69d694101ef4e2d5a3c1034c8f7", "url": "https://www.semanticscholar.org/paper/c5355e5c89a7a69d694101ef4e2d5a3c1034c8f7", "title": "Cautionary lessons from real-world testing of GPT-4.1 AI for pediatric foreign body aspiration.", "venue": "European archives of oto-rhino-laryngology : official journal of the European Federation of Oto-Rhino-Laryngological Societies (EUFOS) : affiliated with the German Society for Oto-Rhino-Laryngology - Head and Neck Surgery", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00405-025-09856-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00405-025-09856-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-23", "authors": [{"authorId": "2298707691", "name": "Sholem Hack"}, {"authorId": "2353938893", "name": "Rebecca Attal"}, {"authorId": "2347840596", "name": "Dana Elazar"}, {"authorId": "2253880313", "name": "Yaniv Alon"}, {"authorId": "2394046319", "name": "Raphael Meyuchas"}, {"authorId": "2293608149", "name": "Adva Livne"}, {"authorId": "7892246", "name": "O. Madgar"}, {"authorId": "2392501462", "name": "Mor Saban"}], "abstract": null}
{"paperId": "c54fe6317e57b9ef959607a16b51e61fdedbefd3", "url": "https://www.semanticscholar.org/paper/c54fe6317e57b9ef959607a16b51e61fdedbefd3", "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-21", "authors": [{"authorId": "2183206926", "name": "Peter S'uken'ik"}, {"authorId": "2267335630", "name": "Christoph H. Lampert"}, {"authorId": "2302797572", "name": "Marco Mondelli"}], "abstract": "The empirical emergence of neural collapse -- a surprising symmetry in the feature representations of the training data in the penultimate layer of deep neural networks -- has spurred a line of theoretical research aimed at its understanding. However, existing work focuses on data-agnostic models or, when data structure is taken into account, it remains limited to multi-layer perceptrons. Our paper fills both these gaps by analyzing modern architectures in a data-aware regime: we prove that global optima of deep regularized transformers and residual networks (ResNets) with LayerNorm trained with cross entropy or mean squared error loss are approximately collapsed, and the approximation gets tighter as the depth grows. More generally, we formally reduce any end-to-end large-depth ResNet or transformer training into an equivalent unconstrained features model, thus justifying its wide use in the literature even beyond data-agnostic settings. Our theoretical results are supported by experiments on computer vision and language datasets showing that, as the depth grows, neural collapse indeed becomes more prominent."}
{"paperId": "c5e44abd0c47592252e2413c78a5f68e00a64dce", "url": "https://www.semanticscholar.org/paper/c5e44abd0c47592252e2413c78a5f68e00a64dce", "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.17909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-23", "authors": [{"authorId": "2312752426", "name": "Bram Grooten"}, {"authorId": "2363347913", "name": "Farid Hasanov"}, {"authorId": "2363408494", "name": "Chenxiang Zhang"}, {"authorId": "2272970707", "name": "Qiao Xiao"}, {"authorId": "46791907", "name": "Boqian Wu"}, {"authorId": "1768130522", "name": "Zahra Atashgahi"}, {"authorId": "67102129", "name": "Ghada Sokar"}, {"authorId": "2131166985", "name": "Shiwei Liu"}, {"authorId": "2308632849", "name": "Lu Yin"}, {"authorId": "2288694362", "name": "Elena Mocanu"}, {"authorId": "1691997", "name": "Mykola Pechenizkiy"}, {"authorId": "2571038", "name": "D. Mocanu"}], "abstract": "Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters."}
{"paperId": "c5e876257994721ffbfdad107f2f1b0e14d012e8", "url": "https://www.semanticscholar.org/paper/c5e876257994721ffbfdad107f2f1b0e14d012e8", "title": "NLP MODELS FOR ARTISTIC STATEMENT GENERATION", "venue": "ShodhKosh Journal of Visual and Performing Arts", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.29121/shodhkosh.v6.i1s.2025.6673?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.29121/shodhkosh.v6.i1s.2025.6673, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-12-10", "authors": [{"authorId": "2398842689", "name": "R.M. Gomathi"}, {"authorId": "2398692379", "name": "Pooja Srishti"}, {"authorId": "2398800295", "name": "Prateek Garg"}, {"authorId": "2398836475", "name": "Roselin"}, {"authorId": "2397245097", "name": "Hemal Thakker"}, {"authorId": "2363346850", "name": "Sumeet Kaur"}], "abstract": "In this paper we propose a multimodal vision-language multimodality detransformer framework for coherent, expressive, and visually grounded artistic statement generation, which takes advantage of multimodal vision- and language modeling on top of a strong transformer-based text generation network. The proposed system is comprised of a visual encoder to interpret compositional and stylistic aspects of an artwork, a fine-tuned transformer decoder that acts as a conceptually rich story engine and a cross-modal fusion module to ensure the alignment between visual clues and linguistic output. Combined with creative and grounding-based reward mechanisms from reinforcement learning, the interpretive depth and style-grounding are further advanced. Using automated similarity measures, multimodality alignment scores and human expert subjectivity measurement, it is shown that the hybrid model greatly improves over traditional captioning and text-only methods at extracting artistry, emotionality and conceptuality. While the method has great potential, challenges exist in dealing with cultural bias, data limitations, interpretive subjectivity and computational demands. Overall, the research brings forward the field of AI-assisted artistic communication and provides a scalable solution to help artists, curators, educators, and digital art platforms to create quality artistic statements."}
{"paperId": "c6136d80ab2c999818bf1e105dc1712cca7ccac6", "url": "https://www.semanticscholar.org/paper/c6136d80ab2c999818bf1e105dc1712cca7ccac6", "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation", "venue": "arXiv.org", "year": 2025, "citationCount": 21, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.09577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-14", "authors": [{"authorId": "2256775583", "name": "Chaofan Zhang"}, {"authorId": "2298907411", "name": "Peng Hao"}, {"authorId": "66819025", "name": "Xiaoge Cao"}, {"authorId": "2313750556", "name": "Xiaoshuai Hao"}, {"authorId": "1853836031", "name": "Shaowei Cui"}, {"authorId": "2360886366", "name": "Shuo Wang"}], "abstract": "While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla"}
{"paperId": "c6b4d2291832444da737920c5e632ea8aa38de8c", "url": "https://www.semanticscholar.org/paper/c6b4d2291832444da737920c5e632ea8aa38de8c", "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.20422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-25", "authors": [{"authorId": "2366008514", "name": "Bo Pang"}, {"authorId": "2311457468", "name": "Chenxi Xu"}, {"authorId": "2334157069", "name": "Jierui Ren"}, {"authorId": "2380185380", "name": "Guoping Wang"}, {"authorId": "2384069066", "name": "Sheng Li"}], "abstract": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry ->physical attributes ->modal parameters ->acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced."}
{"paperId": "c6b73a0d6ea27b2ba160653bfda513b78f6bc069", "url": "https://www.semanticscholar.org/paper/c6b73a0d6ea27b2ba160653bfda513b78f6bc069", "title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-31", "authors": [{"authorId": "2265569010", "name": "Abdurrahim Yilmaz"}, {"authorId": "2343632466", "name": "Furkan Yuceyalcin"}, {"authorId": "2343632544", "name": "Ece Gokyayla"}, {"authorId": "2343760707", "name": "Donghee Choi"}, {"authorId": "2348929080", "name": "Ozan Erdem Ali Anil Demircali"}, {"authorId": "2042697009", "name": "Rahmetullah Varol"}, {"authorId": "2121534091", "name": "Ufuk Gorkem Kirabali"}, {"authorId": "3752318", "name": "G. Gencoglan"}, {"authorId": "3081379", "name": "J. Posma"}, {"authorId": "2278999012", "name": "Burak Temelkuran"}], "abstract": "A major barrier to developing vision large language models (LLMs) in dermatology is the lack of large image--text pairs dataset. We introduce DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using Gemini 2.0, we used clinically related prompts and self-instruct method to generate diverse and rich synthetic texts. Metadata of the datasets were incorporated into the input prompts by targeting to reduce potential hallucinations. The resulting dataset builds upon open access dermatological image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We anticipate this dataset to support and accelerate AI research in dermatology. Data and code underlying this work are accessible at https://github.com/abdurrahimyilmaz/DermaSynth."}
{"paperId": "c6e3d1b8ce67a96025fe4526a11ba66ef747e33a", "url": "https://www.semanticscholar.org/paper/c6e3d1b8ce67a96025fe4526a11ba66ef747e33a", "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.16633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-20", "authors": [{"authorId": "2187933424", "name": "A. S. Penamakuri"}, {"authorId": "2381817450", "name": "Navlika Singh"}, {"authorId": "50128338", "name": "Piyush Arora"}, {"authorId": "2329527941", "name": "Anand Mishra"}], "abstract": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available."}
{"paperId": "c7223abd57f73d926c250b28b51d8a08231f25ee", "url": "https://www.semanticscholar.org/paper/c7223abd57f73d926c250b28b51d8a08231f25ee", "title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.19244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-26", "authors": [{"authorId": "2273066972", "name": "Oishi Deb"}, {"authorId": "2056385736", "name": "Anjun Hu"}, {"authorId": "2285214499", "name": "Ashkan Khakzar"}, {"authorId": "2105908638", "name": "Philip Torr"}, {"authorId": "2714036", "name": "C. Rupprecht"}], "abstract": "We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\\% of the time, confirm its superiority over existing approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/"}
{"paperId": "c75d051f7584e0f38c1fae4d9914dffec6afe637", "url": "https://www.semanticscholar.org/paper/c75d051f7584e0f38c1fae4d9914dffec6afe637", "title": "Adversarial Attacks and Defense Mechanisms in Machine Learning: A Structured Review of Methods, Domains, and Open Challenges", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3624409?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3624409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2316399582", "name": "Aidos Askhatuly"}, {"authorId": "2378065649", "name": "Dinara Berdysheva"}, {"authorId": "2316404066", "name": "A. Berdyshev"}, {"authorId": "2279598335", "name": "A. Adamova"}, {"authorId": "9279619", "name": "D. Yedilkhan"}], "abstract": "Machine Learning (ML) and Deep Learning (DL) models are increasingly deployed in critical domains such as computer vision, Natural Language Processing (NLP), Automatic Speech Recognition (ASR), time-series analysis, and cybersecurity. Despite their success, these models remain vulnerable to adversarial attacks that can mislead predictions and undermine system reliability. To address this challenge, we conduct a Systematic Literature Review (SLR) of adversarial attacks and defenses in ML/DL, following PRISMA guidelines. Our review analyzes 132 peer-reviewed studies published between 2017 and 2025, identified from leading digital libraries including IEEE, ACM, ScienceDirect, and SpringerLink. The objectives of this study are to: i) identify state-of-the-art adversarial attack and defense techniques and assess their strengths and limitations; ii) evaluate how novel threats and countermeasures are addressed across domains; and iii) propose a unified taxonomy that integrates adversarial attacks, defenses, evaluation metrics, and application domains. Our findings show that adversarial training and certified robustness dominate defense research, but both face trade-offs in scalability and generalizability. Computer vision remains the most studied domain, while NLP, ASR, and time-series research is growing but lacks standardized benchmarks. This review contributes a taxonomy and cross-domain synthesis of adversarial ML research, highlights research gaps, and outlines future directions to advance robustness and support the development of trustworthy AI systems."}
{"paperId": "c81918db3fcde578d856e58422279aa5859f1481", "url": "https://www.semanticscholar.org/paper/c81918db3fcde578d856e58422279aa5859f1481", "title": "FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.07135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-08", "authors": [{"authorId": "2372528420", "name": "Franccois Garderes"}, {"authorId": "2248998076", "name": "Shizhe Chen"}, {"authorId": "2372537296", "name": "Camille-Sovanneary Gauthier"}, {"authorId": "2260688320", "name": "Jean Ponce"}], "abstract": "The composed image retrieval (CIR) task is to retrieve target images given a reference image and a modification text. Recent methods for CIR leverage large pretrained vision-language models (VLMs) and achieve good performance on general-domain concepts like color and texture. However, they still struggle with application domains like fashion, because the rich and diverse vocabulary used in fashion requires specific fine-grained vision and language understanding. An additional difficulty is the lack of large-scale fashion datasets with detailed and relevant annotations, due to the expensive cost of manual annotation by specialists. To address these challenges, we introduce FACap, a large-scale, automatically constructed fashion-domain CIR dataset. It leverages web-sourced fashion images and a two-stage annotation pipeline powered by a VLM and a large language model (LLM) to generate accurate and detailed modification texts. Then, we propose a new CIR model FashionBLIP-2, which fine-tunes the general-domain BLIP-2 model on FACap with lightweight adapters and multi-head query-candidate matching to better account for fine-grained fashion-specific information. FashionBLIP-2 is evaluated with and without additional fine-tuning on the Fashion IQ benchmark and the enhanced evaluation dataset enhFashionIQ, leveraging our pipeline to obtain higher-quality annotations. Experimental results show that the combination of FashionBLIP-2 and pretraining with FACap significantly improves the model's performance in fashion CIR especially for retrieval with fine-grained modification texts, demonstrating the value of our dataset and approach in a highly demanding environment such as e-commerce websites. Code is available at https://fgxaos.github.io/facap-paper-website/."}
{"paperId": "c884855153b6c010d096c2926f4facff087710bc", "url": "https://www.semanticscholar.org/paper/c884855153b6c010d096c2926f4facff087710bc", "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-29", "authors": [{"authorId": "2152297304", "name": "Chunlong Xie"}, {"authorId": "2281807587", "name": "Jialing He"}, {"authorId": "2035986946", "name": "Shangwei Guo"}, {"authorId": "2364119084", "name": "Jiacheng Wang"}, {"authorId": "2303964833", "name": "Shudong Zhang"}, {"authorId": "2315891045", "name": "Tianwei Zhang"}, {"authorId": "2280441943", "name": "Tao Xiang"}], "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments."}
{"paperId": "c88aab06fc57bd619a7e1909620f2cd8be2c9b5f", "url": "https://www.semanticscholar.org/paper/c88aab06fc57bd619a7e1909620f2cd8be2c9b5f", "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.08706, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-12", "authors": [{"authorId": "2310296090", "name": "Zhengxue Cheng"}, {"authorId": "2375751807", "name": "Yiqian Zhang"}, {"authorId": "2372935625", "name": "Wenkang Zhang"}, {"authorId": "2375906073", "name": "Haoyu Li"}, {"authorId": "2297478999", "name": "Keyu Wang"}, {"authorId": "2356014391", "name": "Li Song"}, {"authorId": "2363321787", "name": "Hengdi Zhang"}], "abstract": "Recent vision-language-action (VLA) models build upon vision-language foundations, and have achieved promising results and exhibit the possibility of task generalization in robot manipulation. However, due to the heterogeneity of tactile sensors and the difficulty of acquiring tactile data, current VLA models significantly overlook the importance of tactile perception and fail in contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a novel architecture involving tactile sensing. Specifically, our contributions are threefold. First, our OmniVTLA features a dual-path tactile encoder framework. This framework enhances tactile perception across diverse vision-based and force-based tactile sensors by using a pretrained vision transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we introduce ObjTac, a comprehensive force-based tactile dataset capturing textual, visual, and tactile information for 56 objects across 10 categories. With 135K tri-modal samples, ObjTac supplements existing visuo-tactile datasets. Third, leveraging this dataset, we train a semantically-aligned tactile encoder to learn a unified tactile representation, serving as a better initialization for OmniVTLA. Real-world experiments demonstrate substantial improvements over state-of-the-art VLA baselines, achieving 96.9% success rates with grippers, (21.9% higher over baseline) and 100% success rates with dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides, OmniVTLA significantly reduces task completion time and generates smoother trajectories through tactile sensing compared to existing VLA. Our ObjTac dataset can be found at https://readerek.github.io/Objtac.github.io"}
{"paperId": "c97ed443e2b9689d81a11656ac8ef61aa2d2499b", "url": "https://www.semanticscholar.org/paper/c97ed443e2b9689d81a11656ac8ef61aa2d2499b", "title": "Rwkv-vg: visual grounding with RWKV-driven encoder-decoder framework", "venue": "Multimedia Systems", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00530-025-01720-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00530-025-01720-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-21", "authors": [{"authorId": "3151799", "name": "Fudong Nian"}, {"authorId": "2186593267", "name": "Yanhong Gu"}, {"authorId": "2346760463", "name": "Wentao Wang"}, {"authorId": "2349743158", "name": "Aoyu Liu"}, {"authorId": "2347019336", "name": "Dong Zhang"}, {"authorId": "2346952427", "name": "Fanding Li"}], "abstract": null}
{"paperId": "c993ff925f75e81c3e8c7a586baf6d5e92f8c605", "url": "https://www.semanticscholar.org/paper/c993ff925f75e81c3e8c7a586baf6d5e92f8c605", "title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-03", "authors": [{"authorId": "2343780634", "name": "Xiaorui Ma"}, {"authorId": "2328618047", "name": "Haoran Xie"}, {"authorId": "2275144277", "name": "S. J. Qin"}], "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs."}
{"paperId": "ca4e36fe79e2f7ac09a80c38a79e6851c0f4b40e", "url": "https://www.semanticscholar.org/paper/ca4e36fe79e2f7ac09a80c38a79e6851c0f4b40e", "title": "Review of Object Detection Techniques and AI-Driven Recipe Generation for Smart Kitchens", "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem43554?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem43554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-04-03", "authors": [{"authorId": "2353698145", "name": "Melgibson Shiju"}, {"authorId": "2353670372", "name": "Mathew George"}, {"authorId": "2353701957", "name": "Navaneeth As"}, {"authorId": "2353698386", "name": "Muhammed Sinan PT"}, {"authorId": "2353691808", "name": "Sr. Reema Jose"}], "abstract": "The integration of object detection and conversa- tional AI in smart kitchen systems presents a promising avenue for enhancing cooking convenience. This review explores object detection techniques for identifying available ingredients and their application in generating personalized recipe suggestions. The system leverages computer vision to detect and catalog kitchen items and employs conversational AI to engage with users and recommend recipes tailored to their preferences, dietary restrictions, and available ingredients. By combining advanced object detection methods with natural language processing mod- els for dynamic recipe generation, the system aims to provide an intuitive user experience, some key challenges includes real- time performance, website integration, and personalization, are addressed, offering insights into the development of intelligent cooking assistants that bridge the gap between AI and daily life. Index Terms—Smart Kitchen Systems, Object Detection, Con- versational AI, Computer Vision, Natural Language Processing (NLP), You Only Look Once(YOLO), Single Shot Detector(SSD),\n\nDeep Neural Networks (DNNs)"}
{"paperId": "cb117c67b5bdb876c6ac058a70b9246f0671ebc9", "url": "https://www.semanticscholar.org/paper/cb117c67b5bdb876c6ac058a70b9246f0671ebc9", "title": "AI-Powered Classroom Monitoring: Enhancing Learning Environments", "venue": "IEEE International Conference on Consumer Electronics", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCE-Taiwan66881.2025.11207887?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCE-Taiwan66881.2025.11207887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-07-16", "authors": [{"authorId": "2323041878", "name": "Wai Yie Leong"}], "abstract": "The integration of Artificial Intelligence (AI) in classroom monitoring is transforming traditional education by enabling real-time analysis, personalized feedback, and automated oversight of learning environments. AI-powered classroom monitoring systems leverage computer vision, natural language processing (NLP), machine learning, and Internet of Things (IoT) sensors to track student engagement, behavior patterns, and academic progress. These systems help educators enhance instructional methods, optimize classroom management, and identify students who need additional support. This paper explores the evolution, methodologies, and applications of AI in classroom monitoring, examining key technologies such as facial recognition for attention tracking, sentiment analysis for emotional assessment, automated speech recognition (ASR) for participation analysis, and AI-driven adaptive learning. Through comparative studies, the research highlights the advantages of AI-powered monitoring over traditional classroom management techniques, showcasing improvements in student engagement, teacher efficiency, and learning outcomes."}
{"paperId": "cb55deb958ec9915f30a30bd7bb2980db3b92331", "url": "https://www.semanticscholar.org/paper/cb55deb958ec9915f30a30bd7bb2980db3b92331", "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.16641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-18", "authors": [{"authorId": "2119566888", "name": "Young-Jun Lee"}, {"authorId": "2386931047", "name": "Byung-Kwan Lee"}, {"authorId": "2386625410", "name": "Jianshu Zhang"}, {"authorId": "2296345851", "name": "Yechan Hwang"}, {"authorId": "9726578", "name": "ByungSoo Ko"}, {"authorId": "2386807548", "name": "Han-Gyu Kim"}, {"authorId": "2238951719", "name": "Dongyu Yao"}, {"authorId": "2346838104", "name": "Xuankun Rong"}, {"authorId": "2212094660", "name": "Eojin Joo"}, {"authorId": "2376551645", "name": "Seungmin Han"}, {"authorId": "1576568885", "name": "Bowon Ko"}, {"authorId": "2260282476", "name": "Ho-Jin Choi"}], "abstract": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues - each averaging four turns - derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs."}
{"paperId": "cba576e5d1c9813fa4e2dbcc257f64de6c883640", "url": "https://www.semanticscholar.org/paper/cba576e5d1c9813fa4e2dbcc257f64de6c883640", "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.18865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-23", "authors": [{"authorId": "2275753723", "name": "Masato Kobayashi"}, {"authorId": "2281827221", "name": "Thanpimon Buamanee"}], "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/"}
{"paperId": "cbd3bb85d7b5e9643e7fbf4a9e6ab6b144079905", "url": "https://www.semanticscholar.org/paper/cbd3bb85d7b5e9643e7fbf4a9e6ab6b144079905", "title": "Interactions between vision and language when reading words", "venue": "Journal of Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1167/jov.25.9.1660?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1167/jov.25.9.1660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-15", "authors": [{"authorId": "2323363276", "name": "Alex White"}, {"authorId": "47546722", "name": "Vassiki S. Chauhan"}], "abstract": null}
{"paperId": "cc3dbdf920f4964a62b8baf68b1d61051ba4e5f9", "url": "https://www.semanticscholar.org/paper/cc3dbdf920f4964a62b8baf68b1d61051ba4e5f9", "title": "MMFood'25: 1st International Workshop on Multi-modal Food Computing", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3762386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3762386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2306648953", "name": "Lipika Dey"}, {"authorId": "2258323555", "name": "Marianna Obrist"}, {"authorId": "2337998225", "name": "Stavroula-Georgia Mougiakakou"}], "abstract": "MMFood'25, the 1st International Workshop on Multi-modal Food Computing, brings together researchers and practitioners at the intersection of artificial intelligence, computer vision, natural language processing, and sensory modeling to advance the study of food. The workshop highlights how multimodal methods can be applied to food recognition, recommendation, analysis, and monitoring to address pressing challenges in health, nutrition, sustainability, and food culture. It features a rich program including a keynote, an invited talk, paper presentations, a poster session, and a panel discussion on the role of multimodal AI in preserving cultural heritage, fostering sustainable food futures, and enabling personal well-being. By convening experts from academia, industry, and healthcare, MMFood'25 provides a unique platform for fostering interdisciplinary collaboration and for shaping the emerging field of multimodal food computing. The workshop proceedings can be found at: https://dl.acm.org/doi/proceedings/10.1145/3746264."}
{"paperId": "cca431c69807196211c657a35697644743944b12", "url": "https://www.semanticscholar.org/paper/cca431c69807196211c657a35697644743944b12", "title": "Visual Large Language Models for Generalized and Specialized Applications", "venue": "arXiv.org", "year": 2025, "citationCount": 32, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-06", "authors": [{"authorId": "2267393864", "name": "Yifan Li"}, {"authorId": "2334744441", "name": "Zhixin Lai"}, {"authorId": "2342453", "name": "Wentao Bao"}, {"authorId": "2264369617", "name": "Zhen Tan"}, {"authorId": "2295671925", "name": "Anh Dao"}, {"authorId": "2350511920", "name": "Kewei Sui"}, {"authorId": "2329363913", "name": "Jiayi Shen"}, {"authorId": "2338973383", "name": "Dong Liu"}, {"authorId": "2267421158", "name": "Huan Liu"}, {"authorId": "2267333754", "name": "Yu Kong"}], "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs."}
{"paperId": "cd5ed671161e5479e3af3006a7157b614f9e1a1e", "url": "https://www.semanticscholar.org/paper/cd5ed671161e5479e3af3006a7157b614f9e1a1e", "title": "Enhancing Parameter Efficiency and Generalization in Large Models: A Regularized and Masked Low-Rank Adaptation Approach", "venue": "Trans. Mach. Learn. Res.", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2271028778", "name": "Yuzhu Mao"}, {"authorId": "2156163661", "name": "Zihao Zhao"}, {"authorId": "2234024443", "name": "Siqi Ping"}, {"authorId": "2215326365", "name": "Yang Liu"}, {"authorId": "2303427849", "name": "Wenbo Ding"}], "abstract": null}
{"paperId": "cd9f7ad1ae335a301c0900d2fa114875c67d5064", "url": "https://www.semanticscholar.org/paper/cd9f7ad1ae335a301c0900d2fa114875c67d5064", "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-03-09", "authors": [{"authorId": "2349938581", "name": "Jianwen Sun"}, {"authorId": "2349426352", "name": "Yukang Feng"}, {"authorId": "2294251099", "name": "Chuanhao Li"}, {"authorId": "2350533645", "name": "Fanrui Zhang"}, {"authorId": "2349380936", "name": "Zizhen Li"}, {"authorId": "2349374151", "name": "Jiaxin Ai"}, {"authorId": "2349391740", "name": "Sizhuo Zhou"}, {"authorId": "2349402231", "name": "Yu Dai"}, {"authorId": "2350252584", "name": "Shenglin Zhang"}, {"authorId": "2349384231", "name": "Kaipeng Zhang"}], "abstract": "Unified multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate'' algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://github.com/finyorko/armor."}
{"paperId": "cdd817531e9569528812dfd6652dd1bd1b33e5ed", "url": "https://www.semanticscholar.org/paper/cdd817531e9569528812dfd6652dd1bd1b33e5ed", "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-08", "authors": [{"authorId": "2397550113", "name": "Yahong Wang"}, {"authorId": "2288530444", "name": "Juncheng Wu"}, {"authorId": "2397379020", "name": "Zhangkai Ni"}, {"authorId": "2167248189", "name": "Longzhen Yang"}, {"authorId": "2180963339", "name": "Yihang Liu"}, {"authorId": "2397555425", "name": "Chengmei Yang"}, {"authorId": "2357547531", "name": "Ying Wen"}, {"authorId": "2344829565", "name": "Xianfeng Tang"}, {"authorId": "2353061362", "name": "Hui Liu"}, {"authorId": "2344789932", "name": "Yuyin Zhou"}, {"authorId": "2315951928", "name": "Lianghua He"}], "abstract": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by\"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as\"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon."}
{"paperId": "cde4c87643253d43162f7e8fecc04f2c01cc278d", "url": "https://www.semanticscholar.org/paper/cde4c87643253d43162f7e8fecc04f2c01cc278d", "title": "The Effect of Asymmetric Transistor Aging on Systolic Arrays for Mission Critical Machine Learning Applications", "venue": "IEEE Access", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2025.3548966", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3548966?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3548966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2332408945", "name": "Firas Ramadan"}, {"authorId": "19190632", "name": "Gil Shomron"}, {"authorId": "3147903", "name": "F. Gabbay"}], "abstract": "Deep neural networks (DNNs) excel in various applications, such as computer vision, natural language processing, and other mission-critical systems. As the computational complexity of these models grows, there is an increasing need for specialized accelerators to handle the demanding workloads. In response, advancements in Very Large Scale Integration (VLSI) process nodes have significantly intensified the development of machine learning (ML) accelerators, offering enhanced transistor miniaturization and power efficiency. However, the susceptibility of these advanced nodes to transistor aging poses risks to ML accelerator performance, prediction accuracy, and reliability, which can impact the functional safety of mission-critical systems. This study focuses on the impact of asymmetric transistor aging, induced by Bias Temperature Instability (BTI), on systolic arrays (SAs), which are integral to many ML accelerators in mission-critical systems. Our aging-aware analysis indicates that SAs experience asymmetric aging, causing logical elements to age at varying rates. In addition, our simulations show that asymmetric transistor aging introduces persistent and transient faults in the SA’s datapath, compromising the overall resiliency of the ML model. Our simulation results show that even with less than 1% of transient failure events, the top-1 prediction accuracy of ResNet-18 ML model drops significantly by 32–50% and with approximately 0.8% of transient failure events PTQ4ViT drops by almost 90%. To address this issue, we propose new hardware mechanisms and design flow solutions that can successfully mitigate the impact of asymmetric transistor aging on ML accelerator reliability with minimal power and area overhead."}
{"paperId": "cf07dffeb71bd1d43d87c5be3e5be8d00b3a3423", "url": "https://www.semanticscholar.org/paper/cf07dffeb71bd1d43d87c5be3e5be8d00b3a3423", "title": "A Knowledge Graph-Enhanced Multimodal AI Framework for Intelligent Tax Data Integration and Compliance Enhancement", "venue": "Frontiers in Business and Finance", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.71465/fbf384?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.71465/fbf384, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-14", "authors": [{"authorId": "2387075956", "name": "Tiantian Zhang"}], "abstract": "Tax administration in the era of digital transformation confronts substantial challenges in integrating and interpreting heterogeneous, multimodal data at enterprise scale. This study proposes a knowledge graph-based multimodal artificial intelligence framework that combines computer vision, natural language processing, and tabular modeling to deliver end-to-end analysis of invoice images, financial texts, and transactional records. At its core, the framework builds a tax-specific knowledge graph that connects transaction entities, voucher attributes, and regulatory terms, and applies graph neural networks for semantic alignment and reasoning. To address privacy and governance constraints, the framework integrates federated learning and differential privacy to protect sensitive financial data while enabling collaborative model improvement. Explainable AI methods generate warning signals and traceable evidence chains for auditors and accountants. We validate the framework using a simulated audit scenario of a medium-sized manufacturing enterprise in a remote, underdeveloped region of the United States that processes over 100,000 invoices annually. The results demonstrate automated linkage of invoice-level details to relevant regulations (for example, value-added tax deduction rules where applicable), extraction of textual anomalies such as duplicate deductions, detection of approximately 15 percent potential compliance risks under privacy protection, and visualized evidence traceability. Preliminary experiments show a 70 percent reduction in processing time and an error rate below 2 percent, indicating significant audit efficiency gains and the potential to reduce labor costs and disputes. The approach offers scalable implications for national tax administration and can be adapted to other industries to advance intelligent tax governance."}
{"paperId": "cf0f25b8ba3e345889ced2e30df97e5707620801", "url": "https://www.semanticscholar.org/paper/cf0f25b8ba3e345889ced2e30df97e5707620801", "title": "UniAud: A Unified Auditing Framework for High Auditing Power and Utility with One Training Run", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.04457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-06", "authors": [{"authorId": "2284590386", "name": "Ruixuan Liu"}, {"authorId": "2340523290", "name": "Li Xiong"}], "abstract": "Differentially private (DP) optimization has been widely adopted as a standard approach to provide rigorous privacy guarantees for training datasets. DP auditing verifies whether a model trained with DP optimization satisfies its claimed privacy level by estimating empirical privacy lower bounds through hypothesis testing. Recent O(1) frameworks improve auditing efficiency by checking the membership status of multiple audit samples in a single run, rather than checking individual samples across multiple runs. However, we reveal that there is no free lunch for this improved efficiency: data dependency and an implicit conflict between auditing and utility impair the tightness of the auditing results. Addressing these challenges, our key insights include reducing data dependency through uncorrelated data and resolving the auditing-utility conflict by decoupling the criteria for effective auditing and separating objectives for utility and auditing. We first propose a unified framework, UniAud, for data-independent auditing that maximizes auditing power through a novel uncorrelated canary construction and a self-comparison framework. We then extend this framework as UniAud++ for data-dependent auditing, optimizing the auditing and utility trade-off through multi-task learning with separate objectives for auditing and training. Experimental results validate that our black-box O(1) framework matches the state-of-the-art auditing results of O(T) auditing with thousands of runs, demonstrating the best efficiency-auditing trade-off across vision and language tasks. Additionally, our framework provides meaningful auditing with only slight utility degradation compared to standard DP training, showing the optimal utility-auditing trade-off and the benefit of requiring no extra training for auditing."}
{"paperId": "cf5c3f47b43e84ac53a8066a61ba9e0110456a76", "url": "https://www.semanticscholar.org/paper/cf5c3f47b43e84ac53a8066a61ba9e0110456a76", "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications", "venue": "International Symposium on High-Performance Computer Architecture", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.12053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "49417466", "name": "Yujun Lin"}, {"authorId": "2286139423", "name": "Zhekai Zhang"}, {"authorId": "2354512259", "name": "Song Han"}], "abstract": "Modern tensor applications, especially foundation models and generative AI applications require multiple input modalities (both vision and language), which increases the demand for flexible accelerator architecture. Existing frameworks suffer from the trade-off between design flexibility and productivity of RTL generation: either limited to very few hand-written templates or cannot automatically generate the RTL. To address this challenge, we propose the LEGO framework, which targets tensor applications and automatically generates spatial architecture design and outputs synthesizable RTL code without handwritten RTL design templates. Leveraging the affine-transformation-based architecture representation, LEGO front end finds interconnections between function units, synthesizes the memory system, and fuses different spatial dataflow designs based on data reuse analysis. LEGO back end then translates the hardware in a primitive-level graph to perform lower-level optimizations, and applies a set of linear-programming algorithms to optimally insert pipeline registers and reduce the overhead of unused logic when switching spatial dataflows. Our evaluation demonstrates that LEGO can achieve $3.2 \\times$ speedup and $2.4 \\times$ energy efficiency compared to previous work Gemmini, and can generate one architecture for diverse modern foundation models in generative AI applications."}
{"paperId": "cfed7fd5022de4d1c31e1576c9cd9ae251d13351", "url": "https://www.semanticscholar.org/paper/cfed7fd5022de4d1c31e1576c9cd9ae251d13351", "title": "Sub-network Knowledge Injection and Transferable Parameter Updating Strategy for Continual Learning of Vision-and-Language Tasks", "venue": "IEEE Transactions on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tai.2025.3564915?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tai.2025.3564915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2187055461", "name": "Jiong Wang"}, {"authorId": "2283189754", "name": "Qing Zhang"}, {"authorId": "2283302649", "name": "Jie Liu"}, {"authorId": "2323365657", "name": "Kaifeng Nie"}, {"authorId": "2323389036", "name": "Ze Zhang"}, {"authorId": "2346062805", "name": "Linqi Song"}, {"authorId": "2330657791", "name": "Yinqiao Li"}], "abstract": null}
{"paperId": "d02fdafe45960482cbb7cd8e037795eef8086b6d", "url": "https://www.semanticscholar.org/paper/d02fdafe45960482cbb7cd8e037795eef8086b6d", "title": "IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout", "venue": "arXiv.org", "year": 2025, "citationCount": 14, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.01949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-02", "authors": [{"authorId": "2261750522", "name": "Fei Shen"}, {"authorId": "2242075520", "name": "Xiaoyu Du"}, {"authorId": "2364985752", "name": "Yutong Gao"}, {"authorId": "2364556454", "name": "Jian Yu"}, {"authorId": "2379221117", "name": "Yushe Cao"}, {"authorId": "2364998655", "name": "Xing Lei"}, {"authorId": "2343836734", "name": "Jinhui Tang"}], "abstract": "Recent diffusion models have advanced image editing by improving fidelity and controllability across creative and personalized applications. However, multi-object scenes remain challenging, as reliable control over object categories, counts, and spatial layout is difficult to achieve. For that, we first study quantity and layout consistent image editing, abbreviated as QL-Edit, which targets control of object quantity and spatial layout in multi-object scenes. Then, we present IMAGHarmony, a straightforward framework featuring a plug-and-play harmony aware (HA) module that fuses perception semantics while modeling object counts and locations, resulting in accurate edits and strong structural consistency. We further observe that diffusion models are sensitive to the choice of initial noise and tend to prefer certain noise patterns. Based on this finding, we present a preference-guided noise selection (PNS) strategy that selects semantically aligned initial noise through vision and language matching, thereby further improving generation stability and layout consistency in multiple object editing. To support evaluation, we develop HarmonyBench, a comprehensive benchmark that covers a diverse range of quantity and layout control scenarios. Extensive experiments demonstrate that IMAGHarmony outperforms prior methods in both structural alignment and semantic accuracy, utilizing only 200 training images and 10.6M of trainable parameters. Code, models, and data are available at https://github.com/muzishen/IMAGHarmony."}
{"paperId": "d0e0942d5f2074227060c2dfdd6bdadb6b9f8ab8", "url": "https://www.semanticscholar.org/paper/d0e0942d5f2074227060c2dfdd6bdadb6b9f8ab8", "title": "BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-22", "authors": [{"authorId": "2165225814", "name": "EunJeong Hwang"}, {"authorId": "2347042374", "name": "Peter West"}, {"authorId": "2121375800", "name": "Vered Shwartz"}], "abstract": "Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes). Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce \\method{}, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction."}
{"paperId": "d11b47dca87a14cf059d0f8aba844b56cf555650", "url": "https://www.semanticscholar.org/paper/d11b47dca87a14cf059d0f8aba844b56cf555650", "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-21", "authors": [{"authorId": "80846715", "name": "Alvin Heng"}, {"authorId": "2302327772", "name": "Harold Soh"}], "abstract": "Selective classification enhances the reliability of predictive models by allowing them to abstain from making uncertain predictions. In this work, we revisit the design of optimal selection functions through the lens of the Neyman--Pearson lemma, a classical result in statistics that characterizes the optimal rejection rule as a likelihood ratio test. We show that this perspective not only unifies the behavior of several post-hoc selection baselines, but also motivates new approaches to selective classification which we propose here. A central focus of our work is the setting of covariate shift, where the input distribution at test time differs from that at training. This realistic and challenging scenario remains relatively underexplored in the context of selective classification. We evaluate our proposed methods across a range of vision and language tasks, including both supervised learning and vision-language models. Our experiments demonstrate that our Neyman--Pearson-informed methods consistently outperform existing baselines, indicating that likelihood ratio-based selection offers a robust mechanism for improving selective classification under covariate shifts. Our code is publicly available at https://github.com/clear-nus/sc-likelihood-ratios."}
{"paperId": "d13f7ff43361e7f34fb223eb9ef18d904534f955", "url": "https://www.semanticscholar.org/paper/d13f7ff43361e7f34fb223eb9ef18d904534f955", "title": "AIPerfLLM: 3rd International Workshop on Performance Optimization in the LLM world", "venue": "International Conference on Performance Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3680256.3721304?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3680256.3721304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2025-05-05", "authors": [{"authorId": "144441258", "name": "K. Chow"}, {"authorId": "2723005", "name": "Emilio Incerto"}, {"authorId": "2300291719", "name": "Marin Litoiu"}, {"authorId": "2359041698", "name": "Zhihao Chang"}, {"authorId": "2300388007", "name": "Anil Rajput"}, {"authorId": "2379420", "name": "Khun Ban"}, {"authorId": "31184359", "name": "Daniele Masti"}, {"authorId": "2300313524", "name": "Zhiheng Lyu"}], "abstract": "Artificial Intelligence (AI) has been widely adopted in various domains (e.g., computer vision, natural language processing, and reliability analysis). However, its use for performance modeling and evaluation remains limited, and its benefits to the performance engineering field are still unclear. Researchers and practitioners have recently started focusing on methods such as explainable or white-box AI-based solutions in performance engineering, but the tools, methodologies, and datasets that enable wider adoption are still lacking. Meanwhile, the rapid rise of large language models (LLMs) such as ChatGPT poses new challenges in performance optimization and cost containment. LLM pre-training is expensive, and the necessary infrastructure also incurs significant carbon footprint. This workshop aims to bridge research and practice by bringing together academia and industry to share experiences and insights in performance engineering for LLM-based services and AI applications. We target techniques and methodologies to optimize performance while reducing energy consumption and cost."}
{"paperId": "d192305826b3415540d3cd95207ce1563365d3de", "url": "https://www.semanticscholar.org/paper/d192305826b3415540d3cd95207ce1563365d3de", "title": "Learning Event Completeness for Weakly Supervised Video Anomaly Detection", "venue": "International Conference on Machine Learning", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.13095, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-16", "authors": [{"authorId": "2303840020", "name": "Yu Wang"}, {"authorId": "2303531868", "name": "Shiwei Chen"}], "abstract": "Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing temporal intervals containing anomalous events within untrimmed videos, utilizing only video-level annotations. However, a significant challenge arises due to the absence of dense frame-level annotations, often leading to incomplete localization in existing WS-VAD methods. To address this issue, we present a novel LEC-VAD, Learning Event Completeness for Weakly Supervised Video Anomaly Detection, which features a dual structure designed to encode both category-aware and category-agnostic semantics between vision and language. Within LEC-VAD, we devise semantic regularities that leverage an anomaly-aware Gaussian mixture to learn precise event boundaries, thereby yielding more complete event instances. Besides, we develop a novel memory bank-based prototype learning mechanism to enrich concise text descriptions associated with anomaly-event categories. This innovation bolsters the text's expressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates remarkable advancements over the current state-of-the-art methods on two benchmark datasets XD-Violence and UCF-Crime."}
{"paperId": "d205ba3acad1335bc3f6f370e2d7e27cb9f62830", "url": "https://www.semanticscholar.org/paper/d205ba3acad1335bc3f6f370e2d7e27cb9f62830", "title": "RefCap: Zero-shot Video Corpus Moment Retrieval Based on Refined Dense Video Captioning", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP49660.2025.10888164?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP49660.2025.10888164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-06", "authors": [{"authorId": "2282566840", "name": "Yi Pan"}, {"authorId": "1591133581", "name": "Yujia Zhang"}, {"authorId": "2293273322", "name": "Michael C. Kampffmeyer"}, {"authorId": "2222621503", "name": "Xiaoguang Zhao"}], "abstract": "Video corpus moment retrieval (VCMR) is a challenging task aimed at localizing specific segments from untrimmed videos within a vast video collection. It has long been addressed using end-to-end supervised or weakly-supervised methods, which often lack explainability and rely on laborious annotations. To address these issues, we exploit generated captions from Vision Large Language Models (VLLMs) and propose the first zero-shot training-free VCMR system, RefCap. The system consists of two decoupled stages: the Construction Stage to propose dense events and construct corresponding captions, and the Retrieval Stage to retrieve events based on text similarities between queries and captions. In the Construction Stage, we design a Sliding-Window Denoiser and a Quality-Measured Event Generator to produce high-quality dense captions, and derive the Indexing Keyword Sets to fully utilize the generated captions. In the Retrieval Stage, we propose a multi-granularity retrieval strategy integrating both sentence-level and word-level textual similarities between queries and event captions. Extensive experiments on the Charades and ActivityNet datasets demonstrate the effectiveness and competitiveness of our method. Code is available at https://github.com/BUAAPY/RefCap."}
{"paperId": "d25a448b04e52623b31e28f2c3ae962ba75c9ce2", "url": "https://www.semanticscholar.org/paper/d25a448b04e52623b31e28f2c3ae962ba75c9ce2", "title": "Instance-Level Composed Image Retrieval", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.25387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-29", "authors": [{"authorId": "2366559346", "name": "Bill Psomas"}, {"authorId": "1730770", "name": "G. Retsinas"}, {"authorId": "2389334584", "name": "Nikos Efthymiadis"}, {"authorId": "30192180", "name": "P. Filntisis"}, {"authorId": "1744904", "name": "Yannis Avrithis"}, {"authorId": "2292179285", "name": "Petros Maragos"}, {"authorId": "1700928", "name": "Ondřej Chum"}, {"authorId": "1706195", "name": "Giorgos Tolias"}], "abstract": "The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives. To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/."}
{"paperId": "d2a1ffd5081e42687fe854a13f400258c97bbfe3", "url": "https://www.semanticscholar.org/paper/d2a1ffd5081e42687fe854a13f400258c97bbfe3", "title": "EXPLORING THE IMPACT OF ARTIFICIAL INTELLIGENCE IN MARKETING", "venue": "Naveen International Journal of Multidisciplinary Sciences (NIJMS)", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.71126/nijms.v1i5.50?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.71126/nijms.v1i5.50, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-31", "authors": [{"authorId": "2369189756", "name": "Digeshwari sahu"}, {"authorId": "2369207079", "name": "Dr. Annapurna Metta"}], "abstract": "ABSTRACT: \nIn an ever-advancing digital space, Artificial Intelligence (AI) has found to be revolutionizing factor across several sector, with marketing positioning as one of the most influenced areas. The present study examines the multifaceted effect of AI in the field of marketing, assessing how AI tools are transforming strategic decision-making, data analytics, content generation, consumer service, personalization and consumer engagement. The inculcation of AI has empowered marketers to shift from conventional, intuition-assisted strategies to highly targeted, automated and data-driven companies. One such core impact of AI in marketing is its capabilities to process huge chunk of unstructured and structured data in real-time, providing actionable insights for ensuring improved decision-making and predictive analytics. Technologies like computer vision, natural language processing (NLP) and machine learning algorithms have assisted in designing intelligent marketing tool used in optimization of content delivery, predicting future trends, and recognizing the consumer behaviour. This research paper highlights the implications and potential effects of AI in future prospects. It emphasizes on the advanced AI systems like predictive customer lifetime value models, AI-driven influences, AR integration and voice commerce. The study discusses that AI is not only a technological application but a strategic tool which enables the market players to attain greater agility, efficiency and customer-centricity. While difficulties remain persistent, the consistent development of AI capacities creates huge opportunities for marketing innovations.\nKeywords:  Marketing, AI, Impact, ML/NLP.\n "}
{"paperId": "d2f0cf442617ddcf71ef8e180bc329d0f3b108f3", "url": "https://www.semanticscholar.org/paper/d2f0cf442617ddcf71ef8e180bc329d0f3b108f3", "title": "MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04184, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-07", "authors": [{"authorId": "1658996638", "name": "W. Ikezogwo"}, {"authorId": "2339022727", "name": "Kevin Zhang"}, {"authorId": "1997591", "name": "M. S. Seyfioglu"}, {"authorId": "1625854618", "name": "Fatemeh Ghezloo"}, {"authorId": "2333599397", "name": "Linda Shapiro"}, {"authorId": "2257273968", "name": "Ranjay Krishna"}], "abstract": "We propose MedicalNarratives, a dataset curated from medical pedagogical videos similar in nature to data collected in Think-Aloud studies and inspired by Localized Narratives, which collects grounded image-text data by curating instructors' speech and mouse cursor movements synchronized in time. MedicalNarratives enables pretraining of both semantic and dense objectives, alleviating the need to train medical semantic and dense tasks disparately due to the lack of reasonably sized datasets. Our dataset contains 4.7M image-text pairs from videos and articles, with 1M samples containing dense annotations in the form of traces and bounding boxes. To evaluate the utility of MedicalNarratives, we train GenMedClip based on the CLIP architecture using our dataset spanning 12 medical domains and demonstrate that it outperforms previous state-of-the-art models on a newly constructed medical imaging benchmark that comprehensively evaluates performance across all modalities. Data, demo, code and models available at https://medical-narratives.github.io"}
{"paperId": "d31fd7ee46d56e7b87f199bbacddd7241e895e41", "url": "https://www.semanticscholar.org/paper/d31fd7ee46d56e7b87f199bbacddd7241e895e41", "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.15250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-18", "authors": [{"authorId": "2284684476", "name": "Wenda Qin"}, {"authorId": "2054917634", "name": "Andrea Burns"}, {"authorId": "2257229053", "name": "Bryan A. Plummer"}, {"authorId": "1723703", "name": "Margrit Betke"}], "abstract": "Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS."}
{"paperId": "d35927e0b346ab7e3da89295c24bf35e25d81968", "url": "https://www.semanticscholar.org/paper/d35927e0b346ab7e3da89295c24bf35e25d81968", "title": "A Model Zoo on Phase Transitions in Neural Networks", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-25", "authors": [{"authorId": "2312308054", "name": "Konstantin Schürholt"}, {"authorId": "2351602787", "name": "Léo Meynent"}, {"authorId": "2111405998", "name": "Yefan Zhou"}, {"authorId": "2311995497", "name": "Haiquan Lu"}, {"authorId": "2249529142", "name": "Yaoqing Yang"}, {"authorId": "2260706201", "name": "Damian Borth"}], "abstract": "Using the weights of trained Neural Network (NN) models as data modality has recently gained traction as a research field - dubbed Weight Space Learning (WSL). Multiple recent works propose WSL methods to analyze models, evaluate methods, or synthesize weights. Weight space learning methods require populations of trained models as datasets for development and evaluation. However, existing collections of models - called `model zoos'- are unstructured or follow a rudimentary definition of diversity. In parallel, work rooted in statistical physics has identified phases and phase transitions in NN models. Models are homogeneous within the same phase but qualitatively differ from one phase to another. We combine the idea of `model zoos'with phase information to create a controlled notion of diversity in populations. We introduce 12 large-scale zoos that systematically cover known phases and vary over model architecture, size, and datasets. These datasets cover different modalities, such as computer vision, natural language processing, and scientific ML. For every model, we compute loss landscape metrics and validate full coverage of the phases. With this dataset, we provide the community with a resource with a wide range of potential applications for WSL and beyond. Evidence suggests the loss landscape phase plays a role in applications such as model training, analysis, or sparsification. We demonstrate this in an exploratory study of the downstream methods like transfer learning or model weights averaging."}
{"paperId": "d3702207e11dc1d6d9ea12c36ffb5d831f2b4851", "url": "https://www.semanticscholar.org/paper/d3702207e11dc1d6d9ea12c36ffb5d831f2b4851", "title": "FITMag: A Framework for Generating Fashion Journalism Using Multimodal LLMs, Social Media Influence, and Graph RAG", "venue": "AHFE International", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54941/ahfe1006038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54941/ahfe1006038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2307769859", "name": "Jinda Han"}, {"authorId": "2238181744", "name": "Mengyao Guo"}, {"authorId": "2369634964", "name": "Shanghao Li"}, {"authorId": "2299154472", "name": "Kailash Thiyagarajan"}, {"authorId": "2370562773", "name": "Zhinan Cheng"}, {"authorId": "2369517962", "name": "Li Zhao"}], "abstract": "As generative artificial intelligence (AI) reshapes the landscape of media and communication, its integration with social media opens new possibilities for human-centered content creation. In the field of fashion journalism, which relies heavily on style, nuance, and visual culture, we present FITMag, a framework that combines multimodal large language models (LLMs) with real-time social media influence and graph data to generate fashion articles approaching professional quality.FITMag builds on the FITNet and FITViz datasets, which identify fashion influencer subgraphs on Twitter. It uses multimodal inputs including influencer metadata, retweets, mentions, hashtag trends (such as #NYFW and #sustainability), and image content to create structured prompts for both text and image generation. Leading LLMs such as ChatGPT, Claude, DeepSeek, and LLaMA are paired with Stable Diffusion to generate content in three primary formats: event-driven articles, niche community pieces, and trend-based narratives. Graph Retrieval-Augmented Generation (Graph RAG) is used to enhance contextual alignment by connecting influencer activity with fashion discourse.To assess FITMag’s effectiveness, we conducted a human-centered study with 15 fashion professionals including editors, stylists, bloggers, and researchers. Participants evaluated 52 fashion articles using 5-point Likert scales across three dimensions: authenticity, coherence, and style. They also completed a blind identification task to determine whether each article was human-written or generated by AI. Quantitative results show that GPT-4o with FITNet data achieved the highest overall performance among AI models, closely matching human-written content in stylistic quality. Participants frequently misclassified AI-generated text as human-written, especially when produced by GPT-4o and Claude, suggesting strong perceived realism. However, vision and language alignment remained a limitation. Participants observed that AI-generated images sometimes lacked contextual relevance or omitted recognizable influencers due to licensing restrictions.These findings highlight both the capabilities and current limitations of multimodal systems. While AI-generated articles can reach professional-level quality in text, challenges in image and text coherence persist. FITMag contributes to ongoing conversations about AI-assisted journalism by integrating social influence data, generative models, and user-centered evaluation. The research provides insight into how AI can support rather than replace human creativity in fashion media.Ultimately, FITMag serves as a testbed for studying AI and human collaboration in social media contexts. It offers practical tools and theoretical foundations for designing future systems that balance generative power, editorial integrity, and cultural sensitivity across digital platforms."}
{"paperId": "d40c834d617c2353e853624892b4a711eb17da04", "url": "https://www.semanticscholar.org/paper/d40c834d617c2353e853624892b4a711eb17da04", "title": "COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2277447303", "name": "Siqi Zhang"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2291994430", "name": "Qunbo Wang"}, {"authorId": "2353203065", "name": "Zike Yan"}, {"authorId": "2292302699", "name": "Qi Wu"}, {"authorId": "2350698764", "name": "Zhihua Wei"}, {"authorId": "2331583727", "name": "Jing Liu"}], "abstract": "Vision-and-Language Navigation (VLN) tasks have gained prominence within artificial intelligence research due to their potential application in fields like home assistants. Many contemporary VLN approaches, while based on transformer architectures, have increasingly incorporated additional components such as external knowledge bases or map information to enhance performance. These additions, while boosting performance, also lead to larger models and increased computational costs. In this paper, to achieve both high performance and low computational costs, we propose a novel architecture with the COmbination of Selective MemOrization (COSMO). Specifically, COSMO integrates state-space modules and transformer modules, and incorporates two VLN-customized selective state space modules: the Round Selective Scan (RSS) and the Cross-modal Selective State Space Module (CS3). RSS facilitates comprehensive inter-modal interactions within a single scan, while the CS3 module adapts the selective state space module into a dual-stream architecture, thereby enhancing the acquisition of cross-modal interactions. Experimental validations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not only demonstrate competitive navigation performance of our model but also show a significant reduction in computational costs."}
{"paperId": "d4337ce832c603c6c3b853ebefa4f0c7d2001f11", "url": "https://www.semanticscholar.org/paper/d4337ce832c603c6c3b853ebefa4f0c7d2001f11", "title": "Concept-Level Cognitive Modeling via Multimodal Interaction: A Neuro-Symbolic Hybrid Approach for Early Concept Acquisition", "venue": "International journal of pattern recognition and artificial intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1142/s0218001425500302?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1142/s0218001425500302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-05", "authors": [{"authorId": "2392850268", "name": "Tao Wang"}, {"authorId": "2391921988", "name": "Yunqing Ma"}, {"authorId": "2392554481", "name": "Rong Zhang"}], "abstract": "Concept-level cognitive modeling is fundamental to building intelligent systems that can interpret, reason, and interact with the world in a human-like manner. While recent advances in deep learning and multimodal representation have enabled impressive progress in pattern recognition, they fall short of capturing abstract, structured concepts such as spatial relations, agency, and containment that underpin human cognition. This paper presents a neuro-symbolic hybrid framework for early concept acquisition through multimodal interaction. Our approach integrates perceptual input from vision and language with symbolic abstraction, enabling the system to form and refine concept representations via active cross-modal grounding. Central to our method is a dynamic concept representation layer that links symbolic units to neural embeddings, allowing the model to incrementally build a structured concept space without requiring predefined ontologies or exhaustive retraining. An interactive learning loop drives the alignment between modalities, supports hypothesis testing, and facilitates continual concept refinement. We validate our framework on both synthetic and real-world datasets across a range of conceptual tasks including spatial relation understanding, object function reasoning, and affordance inference. Experimental results demonstrate that our model outperforms purely neural and purely symbolic baselines in generalization, interpretability, and adaptability. This work contributes to the development of cognitively inspired AI systems that learn abstract knowledge from interaction, bridging the gap between subsymbolic perception and symbolic reasoning."}
{"paperId": "d4549b190095fe3e01b1ae2d11a524abb35fde5f", "url": "https://www.semanticscholar.org/paper/d4549b190095fe3e01b1ae2d11a524abb35fde5f", "title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2502.01341?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2502.01341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "100832735", "name": "Ahmed Masry"}, {"authorId": "2116899991", "name": "Juan A. Rodriguez"}, {"authorId": "2305815558", "name": "Tianyu Zhang"}, {"authorId": "1643451565", "name": "Suyuchen Wang"}, {"authorId": "2344189229", "name": "Chao Wang"}, {"authorId": "1962954677", "name": "Aarash Feizi"}, {"authorId": "88971031", "name": "Akshay Kalkunte Suresh"}, {"authorId": "2310436656", "name": "Abhay Puri"}, {"authorId": "2334357492", "name": "Xiangru Jian"}, {"authorId": "1387900763", "name": "Pierre-Andr'e Noel"}, {"authorId": "100519532", "name": "Sathwik Tejaswi Madhusudhan"}, {"authorId": "3048367", "name": "Marco Pedersoli"}, {"authorId": "2241485472", "name": "Bang Liu"}, {"authorId": "2748188", "name": "Nicolas Chapados"}, {"authorId": "1865800402", "name": "Y. Bengio"}, {"authorId": "2274022429", "name": "Enamul Hoque"}, {"authorId": "2275240361", "name": "Christopher Pal"}, {"authorId": "3266173", "name": "I. Laradji"}, {"authorId": "2275588082", "name": "David Vázquez"}, {"authorId": "1784150", "name": "Perouz Taslakian"}, {"authorId": "2921001", "name": "Spandana Gella"}, {"authorId": "2306961146", "name": "Sai Rajeswar"}], "abstract": null}
{"paperId": "d49e861730b958f2e7f08eff778d66407b07ae04", "url": "https://www.semanticscholar.org/paper/d49e861730b958f2e7f08eff778d66407b07ae04", "title": "Follow the Beaten Path: The Role of Route Patterns on Vision-Language Navigation Agents Generalization Abilities", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2025.naacl-long.406?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2025.naacl-long.406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "147190281", "name": "K. T. Baghaei"}, {"authorId": "2319965714", "name": "Dieter Pfoser"}, {"authorId": "2261741456", "name": "Antonios Anastasopoulos"}], "abstract": "Vision and language navigation (VLN) is a challenging task towards the creation of embod-ied agents that requires spatial and temporal reasoning over the instructions provided in natural language and aligning them with the visual perception of an environment. Although a number of methods and approaches have been developed, none achieves human level performance in outdoor settings (by up to 75 percent). The contributions of visual and language modalities to the success of VLN have been studied, however here we focus on an overlooked property of routes and show that navigational instructions can be represented as patterns of actions that also describe trajectory shapes. Through carefully crafted experiments, we show that agents generalization to unseen environments depends not only on visual and linguistic features, but also on the shape of trajectories presented to the model during the fine-tuning. Our experiments show that the diversity of patterns of actions during training is a key contributor to high success rates for agents. Last, we pro-pose a solution based on data augmentation that fills the gap in missing patterns of training data. Our findings will guide researchers towards improved practices in the development and evaluation of VLN datasets and agents. 1"}
{"paperId": "d4a7a99cd0279d64bbf4430570740515dbdbb13c", "url": "https://www.semanticscholar.org/paper/d4a7a99cd0279d64bbf4430570740515dbdbb13c", "title": "面向人工智能工作负载的GPU硬件配置优化研究", "venue": "科学与技术探索", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.65196/qdrp6359?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.65196/qdrp6359, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-30", "authors": [{"authorId": "2393672937", "name": "明 唐"}], "abstract": "With the deep application of artificial intelligence (AI) technology in fields such as computer vision, natural language processing, deep learning training, and inference, AI workloads exhibit characteristics such as high computational intensity, large data throughput, and frequent memory access. Traditional GPU hardware configurations are no longer able to meet their efficient operation requirements. This article aims to improve the efficiency of AI workload operation and reduce resource consumption, focusing on optimizing GPU core hardware components. Firstly, analyze the typical characteristics of AI workloads, including computational parallelism, data locality, and memory access patterns; Subsequently, control experiments were designed to quantify the impact of GPU core frequency, VRAM bandwidth, CUDA core quantity, and multi GPU interconnect architecture on the performance of AI tasks (image classification, Transformer model inference); Finally, an adaptive optimization strategy for GPU configuration based on workload types is proposed, which achieves a balance between performance and energy consumption by dynamically adjusting hardware parameters. The experimental results show that in the ResNet-50 image classification task, the optimized GPU configuration can increase training speed by 23.5% and reduce energy consumption by 18.2%; In the BERT model inference task, latency decreased by 19.8% and throughput increased by 21.1%. This study provides theoretical basis and practical reference for GPU hardware selection and configuration optimization of AI servers."}
{"paperId": "d4f00a848a20fafed7973ff0f7ba65927a6fc63a", "url": "https://www.semanticscholar.org/paper/d4f00a848a20fafed7973ff0f7ba65927a6fc63a", "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey", "venue": "IEEE Internet of Things Journal", "year": 2025, "citationCount": 14, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-11", "authors": [{"authorId": "2265656556", "name": "Ahmed Sharshar"}, {"authorId": "2279219495", "name": "Latif U. Khan"}, {"authorId": "1885384073", "name": "Waseem Ullah"}, {"authorId": "2327863134", "name": "Mohsen Guizani"}], "abstract": "Vision large language models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains, such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings."}
{"paperId": "d555eb5a5b18a63c521c586b186c10d7e7d59fb0", "url": "https://www.semanticscholar.org/paper/d555eb5a5b18a63c521c586b186c10d7e7d59fb0", "title": "Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images", "venue": "Journal of imaging informatics in medicine", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-16", "authors": [{"authorId": "2265616782", "name": "Tuan Truong"}, {"authorId": "2352322848", "name": "Ivo M. Baltruschat"}, {"authorId": "2301202831", "name": "Mark Klemens"}, {"authorId": "2340393446", "name": "Grit Werner"}, {"authorId": "51429413", "name": "Matthias Lenga"}], "abstract": "De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key modules: text detection, text extraction, and text analysis. We benchmark three models-YOLOv11, EasyOCR, and GPT-4o- across different setups corresponding to these modules, evaluating their performance on two different datasets encompassing multiple imaging modalities and PHI categories. Our findings indicate that the optimal setup involves utilizing dedicated vision and language models for each module, which achieves a commendable balance in performance, latency, and cost associated with the usage of large language models (LLMs). Additionally, we show that the application of LLMs not only involves identifying PHI content but also enhances OCR tasks and facilitates an end-to-end PHI detection pipeline, showcasing promising outcomes through our analysis."}
{"paperId": "d55ee6fb30f9d112825f22400e4cd29fcf051f72", "url": "https://www.semanticscholar.org/paper/d55ee6fb30f9d112825f22400e4cd29fcf051f72", "title": "Enhancing Visual Aligning and Grounding for Aerial Vision-and-Dialog Navigation", "venue": "IEEE Signal Processing Letters", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2025.3585831?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2025.3585831, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2216329080", "name": "Guanhui Qiao"}, {"authorId": "2305614524", "name": "Dong Yi"}, {"authorId": "2282957242", "name": "Lingxiang Wu"}, {"authorId": "2370450607", "name": "Hanxiao Wu"}, {"authorId": "2297935161", "name": "Jinqiao Wang"}], "abstract": "Vision-and-Language Navigation tasks require an agent to navigate to a destination following natural language instructions. We focus on a challenging VLN dataset, Aerial Vision-and-Dialog Navigation, which encompasses a diverse array of environments and includes an additional altitude variable. Significant spatial and scale variations in the aerial agent’s view make destination visual grounding a crucial capability for the navigation task. However, the existing frameworks not only have insufficient attention to the vision model, but also lack the correlations between visual and textual modalities. To address this, we propose a model that aligns destination visual images with navigation instructions, featuring three innovative components. Firstly, we propose a multi-stage pre-training pipeline that enhances the model’s ability to associate language instructions with top-view images of destinations. Secondly, trajectories are augmented elastically to simulate the noise of the controlling process. Finally, a polygon regression loss function is introduced for rotated object detection, which significantly enhances the accuracy of altitude and orientation estimation. Experiments demonstrate the effectiveness of our approach, which achieves state-of-the-art advancements with improvements of 2.9% in the val unseen dataset and 3.0% in test unseen dataset in success weighted by path length."}
{"paperId": "d5c662d4f8984055a0985fe26904a22db9c40448", "url": "https://www.semanticscholar.org/paper/d5c662d4f8984055a0985fe26904a22db9c40448", "title": "A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-19", "authors": [{"authorId": "2263911200", "name": "Hao Huang"}, {"authorId": "1491104638", "name": "Shuaihang Yuan"}, {"authorId": "2264589226", "name": "Yu Hao"}, {"authorId": "2284219932", "name": "Congcong Wen"}, {"authorId": "2264345449", "name": "Yi Fang"}], "abstract": "A large-scale vision and language model that has been pretrained on massive data encodes visual and linguistic prior, which makes it easier to generate images and language that are more natural and realistic. Despite this, there is still a significant domain gap between the modalities of vision and language, especially when training data is scarce in few-shot settings, where only very limited data are available for training. In order to mitigate this issue, a multi-modal meta-learning framework has been proposed to bridge the gap between two frozen pretrained large vision and language models by introducing a tunable prompt connecting these two large models. For few-shot image captioning, the existing multi-model meta-learning framework utilizes a one-step prompting scheme to accumulate the visual features of input images to guide the language model, which struggles to generate accurate image descriptions with only a few training samples. Instead, we propose a chain-of-thought (CoT) meta-learning scheme as a multi-step image captioning procedure to better imitate how humans describe images. In addition, we further propose to learn different meta-parameters of the model corresponding to each CoT step in distinct subspaces to avoid interference. We evaluated our method on three commonly used image captioning datasets, i.e., MSCOCO, Flickr8k, and Flickr30k, under few-shot settings. The results of our experiments indicate that our chain-of-thought subspace meta-learning strategy is superior to the baselines in terms of performance across different datasets measured by different metrics."}
{"paperId": "d60c2b804c9bf5ce5242197f63563fc346486c4a", "url": "https://www.semanticscholar.org/paper/d60c2b804c9bf5ce5242197f63563fc346486c4a", "title": "DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-30", "authors": [{"authorId": "2378632292", "name": "Boyi Li"}, {"authorId": "2378598808", "name": "Ce Zhang"}, {"authorId": "2378705888", "name": "Richard M. Timmerman"}, {"authorId": "2378578944", "name": "Wenxuan Bao"}], "abstract": "The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global-Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual-Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual-Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training."}
{"paperId": "d622ddee0cfa614ac84d85b709ab8bd401d8a687", "url": "https://www.semanticscholar.org/paper/d622ddee0cfa614ac84d85b709ab8bd401d8a687", "title": "Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.21144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-26", "authors": [{"authorId": "2371088837", "name": "Yuguang Zhang"}, {"authorId": "2277733705", "name": "Kuangpu Guo"}, {"authorId": "2371091951", "name": "Zhihe Lu"}, {"authorId": "2355242610", "name": "Yunbo Wang"}, {"authorId": "2323464628", "name": "Jian Liang"}], "abstract": "Federated learning (FL) enables collaborative model training across decentralized clients without sharing local data, but is challenged by heterogeneity in data, computation, and communication. Pretrained vision-language models (VLMs), with their strong generalization and lightweight tuning via prompts, offer a promising solution. However, existing federated prompt-learning methods rely only on text prompts and overlook joint label-domain distribution shifts. In this paper, we propose a personalized FL framework based on dual-prompt learning and cross fusion, termed pFedDC. Specifically, each client maintains both global and local prompts across vision and language modalities: global prompts capture common knowledge shared across the federation, while local prompts encode client-specific semantics and domain characteristics. Meanwhile, a cross-fusion module is designed to adaptively integrate prompts from different levels, enabling the model to generate personalized representations aligned with each client's unique data distribution. Extensive experiments across nine datasets with various types of heterogeneity show that pFedDC consistently outperforms state-of-the-art methods."}
{"paperId": "d7356269a3a265a100e7adf210548b82e81128de", "url": "https://www.semanticscholar.org/paper/d7356269a3a265a100e7adf210548b82e81128de", "title": "Multimodal Graph-Based Stacked Transformer Network for Brain Tumor Classification, Segmentation and Report Generation", "venue": "IEEE Access", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3631733?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3631733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2392101357", "name": "Rishit Kapoor"}, {"authorId": "2239941325", "name": "S. Shridevi"}, {"authorId": "2066947148", "name": "N. Damodaran"}], "abstract": "Conventional artificial intelligence driven healthcare diagnostics tend to process medical images and clinical text in isolation, losing possible synergistic insights that emerge from their integration. This work presents a novel Multimodal Graph-Based Stacked Transformer Network (MM-GSTN) that combines both modalities of text and vision to create interpretable diagnostic reports. The MM-GSTN combines heterogeneous data through vision transformers and language models with a clinical knowledge graph to encode relationships among patient data, diseases, and symptoms. The proposed architecture, MM-GSTN, is powered by Meta’s Segment Anything Analysis (SAM) for accurate brain mapping, along with the EfficientNet-B0 model, which is combined with a multistacked transformer network, resulting in a classification accuracy of 99%. The system further produces a comprehensive diagnostic report from the finetuned Llama-3.2-11B Vision-Instruct language model utilizing Low-Rank Adaptation (LoRA) training technique, resulting in improved Rouge-1 and Rouge-L scores from 0.005 to 0.072 and BLEU score from 0.036 to 0.068. Integrating a graph network into the overall system maps the relationships between patient data, diseases, and symptoms, which significantly enhances the diagnostic accuracy and clinical decision-making. This work highlights the potential of MM-GSTN as a scalable and explainable AI tool for the diagnosis of brain tumors."}
{"paperId": "d79ef54c8807d0b245e5851cf2ed10b1295f535a", "url": "https://www.semanticscholar.org/paper/d79ef54c8807d0b245e5851cf2ed10b1295f535a", "title": "Coordinated Robustness Evaluation Framework for Vision-Language Models", "venue": "CVPR Workshops", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-05", "authors": [{"authorId": "33166796", "name": "Ashwin Ramesh Babu"}, {"authorId": "38319638", "name": "Sajad Mousavi"}, {"authorId": "1379688250", "name": "Vineet Gundecha"}, {"authorId": "2625454", "name": "Sahand Ghorbanpour"}, {"authorId": "16724108", "name": "Avisek Naug"}, {"authorId": "2219743921", "name": "Antonio Guillen"}, {"authorId": "2248296732", "name": "Ricardo Luna Gutiérrez"}, {"authorId": "2153583603", "name": "S. Sarkar"}], "abstract": "Vision-language models, which integrate computer vision and natural language processing capabilities, have demonstrated significant advancements in tasks such as image captioning and visual question and answering. However, similar to traditional models, they are susceptible to small perturbations, posing a challenge to their robustness, particularly in deployment scenarios. Evaluating the robustness of these models requires perturbations in both the vision and language modalities to learn their inter-modal dependencies. In this work, we train a generic surrogate model that can take both image and text as input and generate joint representation which is further used to generate adversarial perturbations for both the text and image modalities. This coordinated attack strategy is evaluated on the visual question and answering and visual reasoning datasets using various state-of-the-art vision-language models. Our results indicate that the proposed strategy outperforms other multi-modal attacks and single-modality attacks from the recent literature. Our results demonstrate their effectiveness in compromising the robustness of several state-of-the-art pre-trained multi-modal models such as instruct-BLIP, ViLT and others."}
{"paperId": "d7d2d1b388123f805aee146b3a2c6dce6effaa63", "url": "https://www.semanticscholar.org/paper/d7d2d1b388123f805aee146b3a2c6dce6effaa63", "title": "Integrating Vision and Language Foundation Models for Enhanced Navigation and Decision-Making in Connected Autonomous Vehicles", "venue": "IEEE Transactions on Vehicular Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVT.2025.3567975?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVT.2025.3567975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-01", "authors": [{"authorId": "2299341805", "name": "Yaqi Hu"}, {"authorId": "2299187084", "name": "Dongyuan Ou"}, {"authorId": "2299759406", "name": "Jinglin Huang"}, {"authorId": "3427682", "name": "Maoqiang Wu"}, {"authorId": "2310938572", "name": "Min Hao"}, {"authorId": "2311641964", "name": "Rong Yu"}], "abstract": "Traditional connected autonomous vehicles (CAVs) face challenges in personalized navigation and human-like driving decision-making. In this paper, we propose a self-driving system based on vision and language foundation models to address these issues. Specifically, we first design a ChatGPT-4-based vision-and-language navigation (VLN) model that navigates according to environmental observations and routes described in user instructions, demonstrating personalized navigation capabilities. The VLN model has a local navigation experience database that can be uploaded via wireless communication to roadside units (RSUs) for sharing with other vehicles, thereby enhancing ChatGPT's VLN capabilities. Next, we develop a ChatGPT-4-based knowledge-driven driving decision model that enables it to generate driving actions such as acceleration, deceleration, and lane-change based on knowledge, akin to human drivers. This driving decision model includes a local driving experience database that can be shared with other vehicles, further augmenting ChatGPT's driving decision capabilities. Finally, we design an obstacle-repulsion field-based lane-changing path planner for the lane-changing driving action, utilizing a combination of dynamic programming (DP) and quadratic programming (QP) to achieve a safe and smooth lane-changing path. Experimental results demonstrate that the system's VLN model, driving decision model, and lane-changing path planner respectively improve task completion (TC) by 6.1%, success rate (SR) by 5.4%, and curve smoothness by 21.4% compared to the baseline, underscoring the superiority of the system."}
{"paperId": "d7d76662c799d0e1533b8b815bbad3e8a7a02771", "url": "https://www.semanticscholar.org/paper/d7d76662c799d0e1533b8b815bbad3e8a7a02771", "title": "Automatic Text Box Placement for Supporting Typographic Design", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.07665, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-09", "authors": [{"authorId": "2384765649", "name": "Jun Muraoka"}, {"authorId": "2064706229", "name": "Daichi Haraguchi"}, {"authorId": "2311702503", "name": "Naoto Inoue"}, {"authorId": "2237989360", "name": "Wataru Shimoda"}, {"authorId": "2292144660", "name": "Kota Yamaguchi"}, {"authorId": "2190242028", "name": "Seiichi Uchida"}], "abstract": "In layout design for advertisements and web pages, balancing visual appeal and communication efficiency is crucial. This study examines automated text box placement in incomplete layouts, comparing a standard Transformer-based method, a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM (Gemini), and an extended Transformer that processes multiple images. Evaluations on the Crello dataset show the standard Transformer-based models generally outperform VLM-based approaches, particularly when incorporating richer appearance information. However, all methods face challenges with very small text or densely populated layouts. These findings highlight the benefits of task-specific architectures and suggest avenues for further improvement in automated layout design."}
{"paperId": "d834822b51b5c4c44c46f8810ebdbbbb0e89face", "url": "https://www.semanticscholar.org/paper/d834822b51b5c4c44c46f8810ebdbbbb0e89face", "title": "Na Vid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D Videos for Vision-and-Language Navigation", "venue": "IEEE International Conference on Robotics and Automation", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA55743.2025.11128467?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA55743.2025.11128467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2333402095", "name": "Haoran Liu"}, {"authorId": "2333309429", "name": "Weikang Wan"}, {"authorId": "2282087053", "name": "Xiqian Yu"}, {"authorId": "2333311294", "name": "Minghan Li"}, {"authorId": "2107990526", "name": "Jiazhao Zhang"}, {"authorId": "2333537647", "name": "Bo Zhao"}, {"authorId": "2287374267", "name": "Zhibo Chen"}, {"authorId": "2290188965", "name": "Zhongyuan Wang"}, {"authorId": "2287041015", "name": "Zhizheng Zhang"}, {"authorId": "2334325487", "name": "He Wang"}], "abstract": "Understanding and reasoning about the 4D space-time is crucial for Vision-and-Language Navigation (VLN). However, previous works lack in-depth exploration in this aspect, resulting in bottlenecked spatial perception and action precision of VLN agents. In this work, we introduce NaVid-4D, a Vision Language Model (VLM) based navigation agent taking the lead in explicitly showcasing the capabilities of spatial intelligence in the real world. Given natural language instructions, NaVid-4D requires only egocentric RGB-D video streams as observations to perform spatial understanding and reasoning for generating precise instruction-following robotic actions. NaVid-4D learns navigation policies using the data from simulation environments and is endowed with precise spatial understanding and reasoning capabilities using web data. Without the need to pre-train an RGB-D foundation model, we propose a method capable of directly injecting the depth features into the visual encoder of a VLM. We further compare the use of factually captured depth information with the monocularly estimated one and find NaVid-4D works well with both while using estimated depth offers greater gener-alization capability and better mitigates the sim-to-real gap. Extensive experiments demonstrate that NaVid-4D achieves state-of-the-art performance in simulation environment and makes impressive VLN performance with spatial intelligence happen in the real world."}
{"paperId": "d8e1608d73fbe4418e392bf3796662ce11b2ddb7", "url": "https://www.semanticscholar.org/paper/d8e1608d73fbe4418e392bf3796662ce11b2ddb7", "title": "Can off-the-shelf visual large language models detect and diagnose ocular diseases from retinal photographs?", "venue": "BMJ Open Ophthalmology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": "CCBYNC", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11977474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-01", "authors": [{"authorId": "2302084555", "name": "Sahana Srinivasan"}, {"authorId": "2264201789", "name": "H. Ji"}, {"authorId": "2234019422", "name": "D. Chen"}, {"authorId": "2279975227", "name": "W. Wong"}, {"authorId": "20453551", "name": "Z. Soh"}, {"authorId": "1665094043", "name": "Jocelyn Hui Lin Goh"}, {"authorId": "2233655333", "name": "Krithi Pushpanathan"}, {"authorId": "2286256526", "name": "Xiaofei Wang"}, {"authorId": "2311314421", "name": "Weizhi Ma"}, {"authorId": "2163823817", "name": "T. Wong"}, {"authorId": "2388127298", "name": "Y. X. Wang"}, {"authorId": "2233819026", "name": "Ching-Yu Cheng"}, {"authorId": "6412770", "name": "Y. Tham"}], "abstract": "Background The advent of generative artificial intelligence has led to the emergence of multiple vision large language models (VLLMs). This study aimed to evaluate the capabilities of commonly available VLLMs, such as OpenAI’s GPT-4V and Google’s Gemini, in detecting and diagnosing ocular diseases from retinal images. Methods and analysis From the Singapore Epidemiology of Eye Diseases (SEED) study, we selected 44 representative retinal photographs, including 10 healthy and 34 representing six eye diseases (age-related macular degeneration, diabetic retinopathy, glaucoma, visually significant cataract, myopic macular degeneration and retinal vein occlusion). OpenAI’s GPT-4V (both default and data analyst modes) and Google Gemini were prompted with each image to determine if the retina was normal or abnormal and to provide diagnostic descriptions if deemed abnormal. The outputs from the VLLMs were evaluated for accuracy by three attending-level ophthalmologists using a three-point scale (poor, borderline, good). Results GPT-4V default mode demonstrated the highest detection rate, correctly identifying 33 out of 34 detected correctly (97.1%), outperforming its data analyst mode (61.8%) and Google Gemini (41.2%). Despite the relatively high detection rates, the quality of diagnostic descriptions was generally suboptimal—with only 21.2% of GPT-4V’s (default) responses, 4.8% of GPT-4V’s (data analyst) responses and 28.6% for Google Gemini’s responses rated as good. Conclusions Although GPT-4V default mode showed generally high sensitivity in abnormality detection, all evaluated VLLMs were inadequate in providing accurate diagnoses for ocular diseases. These findings emphasise the need for domain-customised VLLMs and suggest the continued need for human oversight in clinical ophthalmology."}
{"paperId": "d91c4abab7194e986cf0ec5a261ed6780ad6b69a", "url": "https://www.semanticscholar.org/paper/d91c4abab7194e986cf0ec5a261ed6780ad6b69a", "title": "Comparison of Different Feature Extraction Techniques for Deep Learning: A Comprehensive Analysis", "venue": "International Journal for Research in Applied Science and Engineering Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22214/ijraset.2025.73716?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22214/ijraset.2025.73716, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-31", "authors": [{"authorId": "2378106409", "name": "Soumyajit Sarkar"}], "abstract": "Feature extraction remains a critical component in deep learning architectures, significantly influencing model\nperformance across various domains including computer vision, natural language processing, and signal processing. This paper\npresents a comprehensive comparison of different feature extraction techniques employed in deep learning frameworks. We\nanalyze traditional handcrafted features, learned representations through convolutional neural networks (CNNs), attention\nmechanisms, and modern transformer-based approaches. Our experimental evaluation across multiple benchmark datasets\ndemonstrates that while learned features generally outperform handcrafted alternatives, the optimal choice depends on dataset\ncharacteristics, computational constraints, and specific application requirements. The results indicate that hybrid approaches\ncombining multiple feature extraction strategies achieve superior performance, with attention-based mechanisms showing\nparticular promise for complex pattern recognition tasks"}
{"paperId": "d93511f702de5a02db0319f4ac52beca5ff45b1c", "url": "https://www.semanticscholar.org/paper/d93511f702de5a02db0319f4ac52beca5ff45b1c", "title": "Distilling Cross-Modal Knowledge via Feature Disentanglement", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.19887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-25", "authors": [{"authorId": "2313690696", "name": "Junhong Liu"}, {"authorId": "2364545703", "name": "Yuan Zhang"}, {"authorId": "2394243684", "name": "Tao Huang"}, {"authorId": "2394347591", "name": "Wenchao Xu"}, {"authorId": "2394206116", "name": "Renyu Yang"}], "abstract": "Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at https://github.com/Johumliu/FD-CMKD."}
{"paperId": "d96a52edb5e81908211414c0ab4d0d82b845d6ab", "url": "https://www.semanticscholar.org/paper/d96a52edb5e81908211414c0ab4d0d82b845d6ab", "title": "Incongruent Multimodal Federated Learning for Medical Vision and Language-based Multi-label Disease Detection", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1609/aaai.v39i27.35054", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i27.35054?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i27.35054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2254295532", "name": "Pramit Saha"}, {"authorId": "2254276411", "name": "Divyanshu Mishra"}, {"authorId": "2064514991", "name": "Felix Wagner"}, {"authorId": "2237797017", "name": "K. Kamnitsas"}, {"authorId": "2254292901", "name": "J. Noble"}], "abstract": "Federated Learning (FL) in healthcare ensures patient privacy by allowing hospitals to collaboratively train machine learning models while keeping sensitive medical data secure and localized. Most existing research in FL has concentrated on unimodal scenarios, where all healthcare institutes share the same type of data. However, in real-world healthcare situations, some clients may have access to multiple types of data pertaining to the same disease. Multimodal Federated Learning (MMFL) utilizes multiple modalities to build a more powerful FL model than its unimodal counterpart. However, the impact of missing modality in different clients, called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly, we introduce several client-level and server-level regularization techniques including Modality-aware knowledge Distillation (MAD) and Leave-one-out teacher (LOOT) towards mitigating modality incongruity effects. Experiments are conducted with Chest X-Ray and radiology reports under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I."}
{"paperId": "d98a9f97b889f37ea39ca2c2d2594d92156d2341", "url": "https://www.semanticscholar.org/paper/d98a9f97b889f37ea39ca2c2d2594d92156d2341", "title": "Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.17913, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-21", "authors": [{"authorId": "2372428762", "name": "Jinjie Wei"}, {"authorId": "2353316550", "name": "Jiyao Liu"}, {"authorId": "2349380824", "name": "Lihao Liu"}, {"authorId": "2333217649", "name": "Ming Hu"}, {"authorId": "2353285720", "name": "Junzhi NIng (Raymond) Ning"}, {"authorId": "2188978724", "name": "Mingcheng Li"}, {"authorId": "2339667559", "name": "Weijie Yin"}, {"authorId": "2349442517", "name": "Junjun He"}, {"authorId": "2339723153", "name": "Xiao Liang"}, {"authorId": "2371185490", "name": "Chao Feng"}, {"authorId": "2367180199", "name": "Dingkang Yang"}], "abstract": "Graphical User Interface (GUI) agents have made significant progress in automating digital tasks through the utilization of computer vision and language models. Nevertheless, existing agent systems encounter notable limitations. Firstly, they predominantly depend on trial and error decision making rather than progressive reasoning, thereby lacking the capability to learn and adapt from interactive encounters. Secondly, these systems are assessed using overly simplistic single step accuracy metrics, which do not adequately reflect the intricate nature of real world GUI interactions. In this paper, we present CogniGUI, a cognitive framework developed to overcome these limitations by enabling adaptive learning for GUI automation resembling human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach combines two main components: (1) an omni parser engine that conducts immediate hierarchical parsing of GUI elements through quick visual semantic analysis to identify actionable components, and (2) a Group based Relative Policy Optimization (GRPO) grounding agent that assesses multiple interaction paths using a unique relative reward system, promoting minimal and efficient operational routes. This dual-system design facilitates iterative''exploration learning mastery''cycles, enabling the agent to enhance its strategies over time based on accumulated experience. Moreover, to assess the generalization and adaptability of agent systems, we introduce ScreenSeek, a comprehensive benchmark that includes multi application navigation, dynamic state transitions, and cross interface coherence, which are often overlooked challenges in current benchmarks. Experimental results demonstrate that CogniGUI surpasses state-of-the-art methods in both the current GUI grounding benchmarks and our newly proposed benchmark."}
{"paperId": "d98d24d94d96c043bcccf3aad885be89d573ab40", "url": "https://www.semanticscholar.org/paper/d98d24d94d96c043bcccf3aad885be89d573ab40", "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09040, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-10", "authors": [{"authorId": "2336523218", "name": "Dianyi Wang"}, {"authorId": "2305695633", "name": "Wei Song"}, {"authorId": "2355325652", "name": "Yikun Wang"}, {"authorId": "2363177846", "name": "Siyuan Wang"}, {"authorId": "2306948071", "name": "Kaicheng yu"}, {"authorId": "2307981771", "name": "Zhongyu Wei"}, {"authorId": "2350699973", "name": "Jiaqi Wang"}], "abstract": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR."}
{"paperId": "d9b37dd8166bca801699146f710833bcc6ec9ad2", "url": "https://www.semanticscholar.org/paper/d9b37dd8166bca801699146f710833bcc6ec9ad2", "title": "Innovative Sunshine Duration Observations with AI: Bridging the Gap in Climatological Data", "venue": "Bulletin of The American Meteorological Society - (BAMS)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1175/bams-d-24-0186.1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1175/bams-d-24-0186.1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-07", "authors": [{"authorId": "2336075604", "name": "Dongwei Liu"}, {"authorId": "2336236398", "name": "Jianguo Tan"}, {"authorId": "2377513374", "name": "Yadong Wang"}, {"authorId": "2377523806", "name": "Jun Shi"}, {"authorId": "2288636955", "name": "Xiangyu Ao"}, {"authorId": "2377545368", "name": "Xiaochuan He"}, {"authorId": "2378099796", "name": "Qianshan He"}, {"authorId": "2377403531", "name": "Haizhen Mu"}, {"authorId": "2377400942", "name": "Wenxuan Hou"}, {"authorId": "2377861904", "name": "Juan Sun"}, {"authorId": "2377540188", "name": "Jie Peng"}, {"authorId": "2335917330", "name": "Miaomiao Liu"}], "abstract": "\nSunshine duration is an important parameter to characterize local climatology and has been observed for a long time. We propose a novel method for observing sunshine duration using video images, based on a Large Vision-Language Model (LVLM) named ViLT (Vision-and-Language Transformer). The method can directly identify weather phenomena from video images without the need for fine-tuning, and can determine whether sunshine is present based on ViLT’s assessment of whether it is a sunny day or not. Evaluation results indicate that the cumulative percentage errors of the annual sunshine duration derived from video observations are 1.3% and 1.1% at the Chongming and Xujiahui stations, respectively. These errors are well below the WMO’s achievable measurement uncertainty for sunshine duration (2% or 0.1 h). Since the method only requires images from standard video equipment as input, it offers a cost-effective and convenient technology for observing sunshine duration, particularly in areas lacking sunshine observations but with available video data. Further studies demonstrate that when combined with the Ångström-Prescott formula, the method can be utilized to estimate total solar radiation from video data. Compared to observations from solar radiometers, the percentage error in the estimated total annual solar radiation is around 4.3%, indicating promising applications of the method in solar resource assessment and climate studies."}
{"paperId": "da16c1f92e0f1c1ab1b8845ab450dab51386d0c1", "url": "https://www.semanticscholar.org/paper/da16c1f92e0f1c1ab1b8845ab450dab51386d0c1", "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-09-29", "authors": [{"authorId": "2375861560", "name": "Yue Zhang"}, {"authorId": "2375765420", "name": "Tianyi Ma"}, {"authorId": "2382933980", "name": "Zun Wang"}, {"authorId": "2386517660", "name": "Yanyuan Qiao"}, {"authorId": "2317083519", "name": "Parisa Kordjamshidi"}], "abstract": "Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance."}
{"paperId": "da2ec9bac5728d37a734432a6caae621caf7275e", "url": "https://www.semanticscholar.org/paper/da2ec9bac5728d37a734432a6caae621caf7275e", "title": "Social in/justice and the deficit foundations of oracy", "venue": "", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "https://www.tandfonline.com/doi/pdf/10.1080/03054985.2024.2311134?needAccess=true", "status": "HYBRID", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/03054985.2024.2311134?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/03054985.2024.2311134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-05-04", "authors": [{"authorId": "2281498794", "name": "Ian Cushing"}], "abstract": "ABSTRACT Oracy is a hot topic in England’s education landscape, increasingly deployed as part of a bipartisan theory of social justice which claims that improved abilities in spoken language can afford working-class and racialised children a route out of the economic and racial inequalities they experience. In this article, I reject these logics, making two main arguments. First, I examine the language ideological foundations of how oracy was first theorised in 1960s’ academic scholarship, showing how it was informed by a flawed theory of language rooted in deficit and dichotomous framings which essentialised working-class, disabled, and racialised children as producing less legitimate language than their wealthier, able-bodied, and white peers. Second, I show how the contemporary oracy agenda relies on a flawed theory of change in its assumptions that social justice can be unlocked by marginalised children making tweaks to their language. I argue that this theory of change frames social justice as a matter of individualised remediation and thus obscures the structural dimensions of inequality. I show how these logics are embedded in purportedly progressive academic scholarship and guises of charitable benevolence. I call for new visions of language education rooted in radical, transformative justice."}
{"paperId": "da5d5f3bf37b521a454afe421ed8c1628bc8e57b", "url": "https://www.semanticscholar.org/paper/da5d5f3bf37b521a454afe421ed8c1628bc8e57b", "title": "TDHook: A Lightweight Framework for Interpretability", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.25475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-29", "authors": [{"authorId": "2052022843", "name": "Yoann Poupart"}], "abstract": "Interpretability of Deep Neural Networks (DNNs) is a growing field driven by the study of vision and language models. Yet, some use cases, like image captioning, or domains like Deep Reinforcement Learning (DRL), require complex modelling, with multiple inputs and outputs or use composable and separated networks. As a consequence, they rarely fit natively into the API of popular interpretability frameworks. We thus present TDHook, an open-source, lightweight, generic interpretability framework based on $\\texttt{tensordict}$ and applicable to any $\\texttt{torch}$ model. It focuses on handling complex composed models which can be trained for Computer Vision, Natural Language Processing, Reinforcement Learning or any other domain. This library features ready-to-use methods for attribution, probing and a flexible get-set API for interventions, and is aiming to bridge the gap between these method classes to make modern interpretability pipelines more accessible. TDHook is designed with minimal dependencies, requiring roughly half as much disk space as $\\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a $\\times$2 speed-up over $\\texttt{captum}$ when running integrated gradients for multi-target pipelines on both CPU and GPU. In addition, to value our work, we showcase concrete use cases of our library with composed interpretability pipelines in Computer Vision (CV) and Natural Language Processing (NLP), as well as with complex models in DRL."}
{"paperId": "da6b412367452cce3e45498fbbb2008d95fd0257", "url": "https://www.semanticscholar.org/paper/da6b412367452cce3e45498fbbb2008d95fd0257", "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.08186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-12-09", "authors": [{"authorId": "2255922850", "name": "Meng Wei"}, {"authorId": "2372455959", "name": "Chenyang Wan"}, {"authorId": "2357982673", "name": "Jiaqi Peng"}, {"authorId": "2397554976", "name": "Xiqian Yu"}, {"authorId": "2360919920", "name": "Yuqiang Yang"}, {"authorId": null, "name": "Delin Feng"}, {"authorId": "2362197963", "name": "Wenzhe Cai"}, {"authorId": "2243446820", "name": "Chenming Zhu"}, {"authorId": null, "name": "Tai Wang"}, {"authorId": "2277447920", "name": "Jiangmiao Pang"}, {"authorId": "2253851118", "name": "Xihui Liu"}], "abstract": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner,\"grounds slowly\"by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy,\"moves fast\"by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments."}
{"paperId": "dab293e76da84838fa186799d61d8c3135a8931f", "url": "https://www.semanticscholar.org/paper/dab293e76da84838fa186799d61d8c3135a8931f", "title": "AI-Driven Personalized Fall Prevention for Older Adults", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i28.35342?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i28.35342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-11", "authors": [{"authorId": "2356245187", "name": "Katherine Xu"}], "abstract": "Falls among older adults pose a significant public health challenge, impacting quality of life and healthcare costs. This research proposal aims to develop an innovative AI-driven personalized fall prevention system for older adults, leveraging advanced machine learning techniques in computer vision, natural language processing, and reinforcement learning. The proposed system will encompass five key components: (1) Advanced pose estimation and activity recognition using HRNet with attention mechanisms and hybrid LSTM-GCN models; (2) Personalized risk assessment through multi-modal deep learning, combining CNNs, RNNs, and federated learning for privacy-preserving distributed training; (3) Adaptive intervention strategies employing Deep Q-Networks and model-based reinforcement learning with GAN-simulated environments; (4) Human-AI interaction utilizing SHAP values for explainable AI and fine-tuned GPT-3 for natural language communication; and (5) Privacy-preserving techniques including differential privacy and homomorphic encryption. The research will be conducted over a five-year period, involving data collection, model development, large-scale testing, and clinical trials. Expected outcomes include a scalable, privacy-preserving AI system capable of significantly reducing fall incidents among older adults, thereby improving quality of life and reducing healthcare costs. This interdisciplinary research contributes to advancing AI techniques in real-world healthcare applications while addressing critical ethical and privacy concerns, potentially transforming elderly care on a global scale."}
{"paperId": "db66858be469b8d782283f1e2ba6e6e4b9b565a4", "url": "https://www.semanticscholar.org/paper/db66858be469b8d782283f1e2ba6e6e4b9b565a4", "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.20804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-28", "authors": [{"authorId": "2375202031", "name": "Xueyao Wan"}, {"authorId": "2373565932", "name": "Hang Yu"}], "abstract": "Retrieval-Augmented Generation (RAG) enhances language model generation by retrieving relevant information from external knowledge bases. However, conventional RAG methods face the issue of missing multimodal information. Multimodal RAG methods address this by fusing images and text through mapping them into a shared embedding space, but they fail to capture the structure of knowledge and logical chains between modalities. Moreover, they also require large-scale training for specific tasks, resulting in limited generalizing ability. To address these limitations, we propose MMGraphRAG, which refines visual content through scene graphs and constructs a multimodal knowledge graph (MMKG) in conjunction with text-based KG. It employs spectral clustering to achieve cross-modal entity linking and retrieves context along reasoning paths to guide the generative process. Experimental results show that MMGraphRAG achieves state-of-the-art performance on the DocBench and MMLongBench datasets, demonstrating strong domain adaptability and clear reasoning paths."}
{"paperId": "dbf10fa63e2de1465099a64f070477357c049297", "url": "https://www.semanticscholar.org/paper/dbf10fa63e2de1465099a64f070477357c049297", "title": "A Multimodal, Multitask System for Generating E Commerce Text Listings from Images", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.21835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-22", "authors": [{"authorId": "2388703099", "name": "Nayan Kumar Singh"}], "abstract": "Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual\"hallucinations\". Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the model's own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score."}
{"paperId": "dc34c0c32a1f1a64c3e79fa99cd4fb40bf11fa5f", "url": "https://www.semanticscholar.org/paper/dc34c0c32a1f1a64c3e79fa99cd4fb40bf11fa5f", "title": "VLCIM: A Vision-Language Cyclic Interaction Model for Industrial Defect Detection", "venue": "IEEE Transactions on Instrumentation and Measurement", "year": 2025, "citationCount": 12, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIM.2025.3583364?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIM.2025.3583364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2151708401", "name": "Xiangkai Shen"}, {"authorId": "2258600095", "name": "Lei Li"}, {"authorId": "2347575633", "name": "Yushan Ma"}, {"authorId": "2110828763", "name": "Shaofeng Xu"}, {"authorId": "2281251432", "name": "Jinhai Liu"}, {"authorId": "2261462973", "name": "Zhiguo Yang"}, {"authorId": "2258537373", "name": "Yan Shi"}], "abstract": "Accurate defect detection is an important element in ensuring product quality and safe equipment operation. However, due to the lack of deep cross-modal interactions (CMIs) during vision feature extraction, existing methods often suffer from attention bias, which ultimately limits detection accuracy. To address this issue, this article proposes a vision-language cyclic interaction model (VLCIM), which progressively optimizes vision feature extraction by integrating domain prior knowledge and generic large model, effectively bridging the dual-domain barrier between “generic-specific” and “vision-language.” Specifically, progressive cyclic interaction learning is proposed for the first time, which integrates a recursive guidance module (RGM) and CMI strategy to realize bidirectional dynamic fusion and collaborative optimization of vision and language features. Furthermore, the proposed dual-view synergistic detection mechanism (DSDM) enhances discriminative decision responses, significantly improving the model’s boundary perception ability and decision-making accuracy in complex scenarios. VLCIM achieves high-precision defect detection by establishing a cyclic interaction mechanism between domain-specific language features and vision representations. The experimental results on three industrial datasets demonstrate that VLCIM achieves improvements of 5.9%, 5.6%, and 4.1% in mean intersection over union (mIoU) over the state-of-the-art (SOTA) methods, indicating its validity and generalization in different scenarios."}
{"paperId": "dc803cd8c4c5575c0828b3d652a29385fcbc86de", "url": "https://www.semanticscholar.org/paper/dc803cd8c4c5575c0828b3d652a29385fcbc86de", "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-14", "authors": [{"authorId": "2380574044", "name": "Yunheng Wang"}, {"authorId": "2112809319", "name": "Yuetong Fang"}, {"authorId": "2380517389", "name": "Taowen Wang"}, {"authorId": "2380628162", "name": "Yixiao Feng"}, {"authorId": "2380575512", "name": "Yawen Tan"}, {"authorId": "2380574024", "name": "Shuning Zhang"}, {"authorId": "2314968936", "name": "Peiran Liu"}, {"authorId": "2384828621", "name": "Yiding Ji"}, {"authorId": "2385455909", "name": "Renjing Xu"}], "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\\% and 18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs."}
{"paperId": "dcb2087598da588f43d472b4a01daad5c68b194a", "url": "https://www.semanticscholar.org/paper/dcb2087598da588f43d472b4a01daad5c68b194a", "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-16", "authors": [{"authorId": "2332527018", "name": "Zihan Wang"}, {"authorId": "2294600666", "name": "Seungjun Lee"}, {"authorId": "2332479307", "name": "Gim Hee Lee"}], "abstract": "Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment."}
{"paperId": "dcc5c1df58f1c9d2dac3e51a21ee7594d70aa0ce", "url": "https://www.semanticscholar.org/paper/dcc5c1df58f1c9d2dac3e51a21ee7594d70aa0ce", "title": "SynerCD: Synergistic Tri-Branch and Vision-Language Coupling for Remote Sensing Change Detection", "venue": "Remote Sensing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs17223694?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs17223694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-12", "authors": [{"authorId": "2396425623", "name": "Yumei Tong"}, {"authorId": "2296320356", "name": "Panpan Zheng"}, {"authorId": "2347875650", "name": "Wenbin Tang"}, {"authorId": "2352418191", "name": "Shuli Cheng"}, {"authorId": "2143501518", "name": "Liejun Wang"}], "abstract": "RSCD faces persistent challenges in high-resolution imagery due to complex spatial structures, temporal heterogeneity, and semantic ambiguity. While deep learning methods have significantly advanced the field, most existing models still rely on static and homogeneous processing, treating all channels and modalities equally, which limits their capacity to capture fine-grained semantic shifts or adapt to region-dependent variations. To address these issues, we propose SynerCD, a unified Siamese encoder–decoder framework that introduces dynamic, content-adaptive perception through channel decoupling, frequency-domain enhancement, and vision-language collaboration. The encoder employs a Tri-branch Synergistic Coupling (TSC) module that dynamically rebalances channel responses and captures multi-scale spatial-frequency dependencies via Mamba-based long-sequence modeling and wavelet decomposition. The decoder integrates a vision-aware language-guided attention (VAL-Att) module, which adaptively modulates visual-textual fusion using CLIP-based semantic prompts to guide attention toward meaningful change regions. Extensive experiments on four benchmark datasets verify that SynerCD achieves superior localization accuracy and semantic robustness, establishing a dynamic and adaptive paradigm for multimodal change detection."}
{"paperId": "dd85ea56dc20ed059195cca6fb6ab837c3a9b98a", "url": "https://www.semanticscholar.org/paper/dd85ea56dc20ed059195cca6fb6ab837c3a9b98a", "title": "Your contrastive learning problem is secretly a distribution alignment problem", "venue": "Neural Information Processing Systems", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-27", "authors": [{"authorId": "2338669616", "name": "Zihao Chen"}, {"authorId": "2299500074", "name": "Chi-Heng Lin"}, {"authorId": "2275527711", "name": "Ran Liu"}, {"authorId": "2315912642", "name": "Jingyun Xiao"}, {"authorId": "2275090240", "name": "Eva L. Dyer"}], "abstract": "Despite the success of contrastive learning (CL) in vision and language, its theoretical foundations and mechanisms for building representations remain poorly understood. In this work, we build connections between noise contrastive estimation losses widely used in CL and distribution alignment with entropic optimal transport (OT). This connection allows us to develop a family of different losses and multistep iterative variants for existing CL methods. Intuitively, by using more information from the distribution of latents, our approach allows a more distribution-aware manipulation of the relationships within augmented sample sets. We provide theoretical insights and experimental evidence demonstrating the benefits of our approach for {\\em generalized contrastive alignment}. Through this framework, it is possible to leverage tools in OT to build unbalanced losses to handle noisy views and customize the representation space by changing the constraints on alignment. By reframing contrastive learning as an alignment problem and leveraging existing optimization tools for OT, our work provides new insights and connections between different self-supervised learning models in addition to new tools that can be more easily adapted to incorporate domain knowledge into learning."}
{"paperId": "de3b304b6394cb19399ae00ea1f206f2182c696c", "url": "https://www.semanticscholar.org/paper/de3b304b6394cb19399ae00ea1f206f2182c696c", "title": "Bridging the Visual Gap: Integrating Vision Language Models (VLM) and Artificial Intelligence (AI) with Enterprise Resource Planning (ERP) Software System", "venue": "International research journal of innovations in engineering and technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.47001/irjiet/2025.iccis-202532?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.47001/irjiet/2025.iccis-202532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2367983109", "name": "T. B. Chandra"}], "abstract": "Abstract - Businesses generate vast visual data (e.g., quality check photos, warehouse snapshots, invoices, customer images), but traditional Enterprise Resource Planning (ERP) systems, built for structured data, cannot process it. This study explores integrating Vision Language Models (VLMs), AI combining computer vision and language processing, with ERPs to automate tasks like quality control, inventory monitoring, and document processing. We assess integration feasibility with Microsoft Dynamics 365 Business Central, Salesforce, and SAP S/4HANA, proposing an API-driven system architecture. VLMs face precision challenges, and ERP readiness varies: Microsoft Dynamics needs custom development, Salesforce offers flexible APIs, and SAP S/4HANA is robust but complex. Strategic planning and leveraging VLM strengths enable AI-enhanced enterprise systems."}
{"paperId": "de6eee20fc95c31d18b72a914f689820b095640a", "url": "https://www.semanticscholar.org/paper/de6eee20fc95c31d18b72a914f689820b095640a", "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-17", "authors": [{"authorId": "11673377", "name": "Bonan Li"}, {"authorId": "2275613490", "name": "Zicheng Zhang"}, {"authorId": "2275285154", "name": "Songhua Liu"}, {"authorId": "2312692351", "name": "Weihao Yu"}, {"authorId": "2319167983", "name": "Xinchao Wang"}], "abstract": "Visual instruction tuning aims to enable large language models to comprehend the visual world, with a pivotal challenge lying in establishing an effective vision-to-language projection. However, existing methods often grapple with the intractable trade-off between accuracy and efficiency. In this paper, we present LLaVA-Meteor, a novel approach designed to break this deadlock, equipped with a novel Top-Down Compression paradigm that strategically compresses visual tokens without compromising core information. Specifically, we construct a trainable Flash Global Fusion module based on efficient selective state space operators, which aligns the feature space while enabling each token to perceive holistic visual context and instruction preference at low cost. Furthermore, a local-to-single scanning manner is employed to effectively capture local dependencies, thereby enhancing the model's capability in vision modeling. To alleviate computational overhead, we explore a Visual-Native Selection mechanism that independently assesses token significance by both the visual and native experts, followed by aggregation to retain the most critical subset. Extensive experiments show that our approach reduces visual tokens by 75--95% while achieving comparable or superior performance across 12 benchmarks, significantly improving efficiency."}
{"paperId": "de86e90243fb8db0e67557a7e0a229ec09264da2", "url": "https://www.semanticscholar.org/paper/de86e90243fb8db0e67557a7e0a229ec09264da2", "title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.10419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-14", "authors": [{"authorId": "2315295846", "name": "Michal Turski"}, {"authorId": "2355354283", "name": "Mateusz Chili'nski"}, {"authorId": "2336956975", "name": "Lukasz Borchmann"}], "abstract": "Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance. The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA"}
{"paperId": "def046b6591504dfe06108a794025291b166b98d", "url": "https://www.semanticscholar.org/paper/def046b6591504dfe06108a794025291b166b98d", "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "venue": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-19", "authors": [{"authorId": "2290177128", "name": "Hengxing Cai"}, {"authorId": "2362281955", "name": "Jinhan Dong"}, {"authorId": "2334521880", "name": "Jingjun Tan"}, {"authorId": "2363131506", "name": "Jingcheng Deng"}, {"authorId": "2290233651", "name": "Sihang Li"}, {"authorId": "2363035410", "name": "Zhifeng Gao"}, {"authorId": "2362314323", "name": "Haidong Wang"}, {"authorId": "2363690484", "name": "Zicheng Su"}, {"authorId": "2984493", "name": "A. Sumalee"}, {"authorId": "2334741038", "name": "Renxin Zhong"}], "abstract": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available."}
{"paperId": "def1dafccb58cbf542bb7ebe1197dfa2cd569bd6", "url": "https://www.semanticscholar.org/paper/def1dafccb58cbf542bb7ebe1197dfa2cd569bd6", "title": "D.I. Fonvizin in the Journal Polemics of 1805–1806", "venue": "Literary Fact", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22455/2541-8297-2025-36-298-313?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22455/2541-8297-2025-36-298-313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-06-01", "authors": [{"authorId": "119103644", "name": "L. Trakhtenberg"}], "abstract": "The paper shows that in 1805–1806, there was a discussion about Denis Fonvizin’s oeuvre and personality in Russian literary magazines. He was mentioned by Nikolai Brusilov’s Zhurnal rossiiskoi slovesnosti, Ivan Martynov’s Severnyi vestnik, Aleksei Varentsov’s Zhurnal dlia pol’zy i udovol’stviia and Drug prosveshcheniia edited by Pavel Golenishchev-Kutuzov, Grigorii Saltykov and Dmitrii Khvostov. At the same time, Zhurnal dlia pol’zy i udovol’stviia and Mikhail Kachenovskii’s Vestnik Evropy published Fonvizin’s works, which had remained in manuscript. Although Fonvizin (1745–1792) was near-contemporary, he was already unanimously recognised as a classic writer. In two articles printed in February and March 1805, Brusilov equalled him to Molière, including him in a list of the best Russian men of letters together with Karamzin. This was probably more to honour the latter than the former, as Karamzin’s reputation as an exemplary author, accepted by many, including Brusilov himself, was questioned by their literary rivals — the Archaists led by Alexander Shishkov. The discussion began around Fonvizin’s person- ality. In June, Severnyi vestnik came forward with an anecdote that illustrated not only his wit but also the self-command he showed when facing illness. Zhurnal dlia pol’zy i udovol’stviia criticized this anecdote in a July review and in September published Fonvizin’s Discourse on the Vain Life of Man, written in his later years, which demonstrated another side of his character — his Christian humility. The same view on Fonvizin was reflected in an entry about him for the dictionary of Russian writers by Bishop Eugene (Bolkhovitinov), which appeared at the same time, in September 1805, in Drug prosveshcheniia. This entry also mentioned the playwright’s vision of language, presenting him as a predecessor of the Archaists in recognising Church Slavonic as a basis of Russian. Then, in April and May 1806, Fonvizin’s letters from France to General Pyotr Panin came out in Vestnik Evropy. They expressed a highly critical view on French life and national character, so at the moment, shortly after Napoleon’s army defeated the Russians at Austerlitz, the publication was quite in tune with the times. Simultaneously, in April 1806, Khvostov’s ode To Denis Ivanovich Fonvizin was printed in Drug prosveshcheniia. Its author, a staunch Classicist, addressed Fonvizin as a supposed ally in his satire on Russian morals and letters of today. Notably, at the time of heated literary debate as it was, all who wrote about Fonvizin, from the Karamzinist Brusilov to the Shishkovist Khvostov, were united in his highest esteem. Everyone appealed to his works and his very image, attempting to strengthen their positions with his authority. The diverse interpretations of his legacy were possible so long as admiration for him was universal."}
{"paperId": "df7bf4d7497b85434aaefd13ea0a84aa2ad86c8d", "url": "https://www.semanticscholar.org/paper/df7bf4d7497b85434aaefd13ea0a84aa2ad86c8d", "title": "CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.07847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-25", "authors": [{"authorId": "2379766657", "name": "Mohamed Elrefaie"}, {"authorId": "2387864447", "name": "Dule Shu"}, {"authorId": "2397485387", "name": "Matt Klenk"}, {"authorId": "2291070568", "name": "Faez Ahmed"}], "abstract": "Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench."}
{"paperId": "e0c3561714fdec400f62508b15808d613bc883cb", "url": "https://www.semanticscholar.org/paper/e0c3561714fdec400f62508b15808d613bc883cb", "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.08589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-03", "authors": [{"authorId": "9449753", "name": "Nirmal Elamon"}, {"authorId": "84612079", "name": "R. Davoudi"}], "abstract": "The field of object detection and understanding is rapidly evolving, driven by advances in both traditional CNN-based models and emerging multi-modal large language models (LLMs). While CNNs like ResNet and YOLO remain highly effective for image-based tasks, recent transformer-based LLMs introduce new capabilities such as dynamic context reasoning, language-guided prompts, and holistic scene understanding. However, when used out-of-the-box, the full potential of LLMs remains underexploited, often resulting in suboptimal performance on specialized visual tasks. In this work, we conduct a comprehensive comparison of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and fine-tuned multi-modal LLMs on the challenging task of artificial text overlay detection in images. A key contribution of our study is demonstrating that LLMs can be effectively fine-tuned on very limited data (fewer than 1,000 images) to achieve up to 36% accuracy improvement, matching or surpassing CNN-based baselines that typically require orders of magnitude more data. By exploring how language-guided models can be adapted for precise visual understanding with minimal supervision, our work contributes to the broader effort of bridging vision and language, offering novel insights into efficient cross-modal learning strategies. These findings highlight the adaptability and data efficiency of LLM-based approaches for real-world object detection tasks and provide actionable guidance for applying multi-modal transformers in low-resource visual environments. To support continued progress in this area, we have made the code used to fine-tune the models available in our GitHub, enabling future improvements and reuse in related applications."}
{"paperId": "e16ac091a1acea220775a84b8b067ab287d2210a", "url": "https://www.semanticscholar.org/paper/e16ac091a1acea220775a84b8b067ab287d2210a", "title": "Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.06131, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-07", "authors": [{"authorId": "2354997556", "name": "Jiawei Mao"}, {"authorId": "2354896646", "name": "Yuhan Wang"}, {"authorId": "2384393778", "name": "Lifeng Chen"}, {"authorId": "2269185997", "name": "Can Zhao"}, {"authorId": "2256117160", "name": "Yucheng Tang"}, {"authorId": "2309095535", "name": "Dong Yang"}, {"authorId": "2279023837", "name": "Liangqiong Qu"}, {"authorId": "2348348077", "name": "Daguang Xu"}, {"authorId": "2354493408", "name": "Yuyin Zhou"}], "abstract": "Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs."}
{"paperId": "e1b52a81e5287dffb37d6fb31009e5c55ae271db", "url": "https://www.semanticscholar.org/paper/e1b52a81e5287dffb37d6fb31009e5c55ae271db", "title": "DeepSIX at ACM MM 2025 Grand Challenge: Enhancing Context Text Processing for Multimodal Hallucination Detection and Fact Verification", "venue": "Proceedings of the 33rd ACM International Conference on Multimedia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746027.3762061?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746027.3762061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-10-27", "authors": [{"authorId": "2374087868", "name": "Hoang Chu"}, {"authorId": "2374087866", "name": "Huy Chu"}, {"authorId": "2388019123", "name": "Tan-Minh Nguyen"}, {"authorId": "2265679916", "name": "Son T. Luu"}, {"authorId": "2387930525", "name": "Cuong Hoang"}, {"authorId": "2304699643", "name": "Hiep Nguyen"}, {"authorId": "2265676532", "name": "Vu Tran"}, {"authorId": "2331496302", "name": "Le-Minh Nguyen"}], "abstract": "Significant advancements have been achieved in both fields of Natural Language Processing (NLP) and Computer Vision (CV) with the advent of Multimodal Large Language Models (MLLMs), sometimes referred to as large vision-language models (LVMs). MLLMs show promising ability in multimodal tasks, such as image captioning, visual question answering, etc. However, there is a concerning trend associated with the advancement in MLLMs. These models exhibit an inclination to generate hallucinations and misleading facts, resulting in seemingly plausible yet factually spurious content. To address these challenges, our team, DeepSIX, leverages recent advances in MLLMs to enhance the ability to detect hallucination and verify factual information within the scope of the ACM MM 2025 grand challenge 8: Truthful and Responsible Multimodal Learning (ResMM). We participated in both tasks: Multimodal Hallucination Detection (Task 1) and Multimodal Fact Checking (Task 2). Our approach leverages the interpretive power of the vision and language components of vision language models (VLMs) to analyze and summarize insights from text and images. It performs contextual reasoning by uncovering semantic relationships among entities in the text and objects in the images. By employing diverse prompting techniques, our method deconstructs critical entities in the text, effectively uncovers implicit relationships between text and images, and identifies hallucinations and false facts. Experimental results demonstrate the strength of our approach: it achieved second place in the Hallucination Detection task and third place in the Fact Verification task, confirming the potential of LLM-based methods in MLLMs. We open-source our code at https://github.com/JAIST-DeepSIX/ACMMM25"}
{"paperId": "e1defe6768f972cad18c675ea1a4c1ef883c52d5", "url": "https://www.semanticscholar.org/paper/e1defe6768f972cad18c675ea1a4c1ef883c52d5", "title": "Conversational Q And A Chatbot System Using AI", "venue": "INTERNATIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55041/ijsrem52004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55041/ijsrem52004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-23", "authors": [{"authorId": "2376512544", "name": "Mohammed Jaffar Ganjur"}, {"authorId": null, "name": "Roopa R"}], "abstract": "In recent years, there has been a significant increase in the demand for intelligent and automated customer service solutions across various industries. Conversational AI-driven Question and Answer (Q&A) chatbots have emerged as a groundbreaking technology in this domain, transforming the way organizations engage with users. This project is centered on the creation of an AI-enhanced Q&A chatbot system that utilizes Natural Language Processing (NLP) and Machine Learning (ML) methodologies to analyze and comprehend user inquiries, discern intent, and produce precise, contextually relevant responses.\n\nIn contrast to conventional rule-based systems, the proposed chatbot incorporates sophisticated models such as transformers (BERT, GPT) to achieve a more profound understanding of language and intent recognition. It offers multilingual support, real-time database connectivity, and sentiment analysis to facilitate human-like, tailored interactions. The system is designed to be scalable, platform-agnostic, and continually enhances its performance through user interactions via adaptive learning techniques. By automating the retrieval of information and support functions, this chatbot improves user experience, alleviates the workload on human agents, and guarantees service availability around the clock.\n\n Keywords: Sign language, information retrieval, computer vision, natural language processing, accessibility, deaf individuals."}
{"paperId": "e208bb61722c2ee3139356eb04ef3b8549219e6e", "url": "https://www.semanticscholar.org/paper/e208bb61722c2ee3139356eb04ef3b8549219e6e", "title": "Recontextualization Practices of Nepalese Language in Educational Policy Discourses", "venue": "Journal of Jayaprithvi Multiple Campus", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3126/jjmc2.v1i1.81438?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3126/jjmc2.v1i1.81438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-07-31", "authors": [{"authorId": "2374882944", "name": "Dhan Bahadur Budha"}], "abstract": "Language in education policy discourses keep on changing periodically in the history of formal education of a nation-state due to the ideological influence oriented by national and international level of sociopolitical change. To discuss such issues, this paper theoretically analyzes Nepalese language in education policy provisions since 1956 to present, which are mentioned in different constitutional and educational policy discourses, to explore how those policy discourses are recontextualized under the influence of hegemonic political power, neoliberalism, globalization and linguistic human right-based ideologies in the formation process. The finding shows that there is strong influence of hegemonic political power before 1990, but the subsequent policy discourses indicate that there is greater influence of linguistic human right-based ideology along with globalization and neoliberalism in democratic period. Therefore, the study is beneficial for policy makers, implementors and researchers to enlighten how ideological visions influence language in education policy formation process of a country. Finally, this study provides some insightful thoughts to conduct further research studies in this domain to develop broader comprehensive overviews."}
{"paperId": "e2a425953775cb33f6a42aaf679f47113da00b8b", "url": "https://www.semanticscholar.org/paper/e2a425953775cb33f6a42aaf679f47113da00b8b", "title": "Robust Dual Embedding Contrastive Learning for Text-to-Image Person Re-identification with Noisy Correspondence", "venue": "Proceedings of the 7th ACM International Conference on Multimedia in Asia", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3743093.3770945?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3743093.3770945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "Conference"], "publicationDate": "2025-12-06", "authors": [{"authorId": "2396661781", "name": "Jingjie Zhang"}, {"authorId": "2368665368", "name": "Lingli Tang"}, {"authorId": "2277407567", "name": "Jiachen Li"}, {"authorId": "2213317937", "name": "Jinyu Xu"}, {"authorId": "2400838", "name": "Yanchun Ma"}, {"authorId": "2392609794", "name": "Qing Xie"}], "abstract": "Text-to-Image person re-identification (TIReID) aims to retrieve pedestrian images from a gallery based on textual descriptions, thus bridging vision and language modalities for practical retrieval scenarios. Despite recent advances leveraging various cross-modal alignment strategies, existing methods typically assume all image-text pairs in training datasets are correctly matched, overlooking the pervasive Noisy Correspondence (NC) problem—erroneous image-text associations that degrade model robustness. Prior approaches either lack noise identification mechanisms or rely on direct filtering of detected noisy samples, which only partially mitigates the adverse effects of noise and cannot fully prevent overfitting to incorrect correspondences during training. Addressing this challenge, we propose Robust Dual Embedding Contrastive Learning (RDECL), which consists of two main components: 1) A Dual-View Cumulative Trust Division (DCTD) progressively constructs a high-confidence clean sample repository via adaptive sample selection, ensuring reliable image-text correspondence learning under uncertain noise detection.2) A Robust Generalized Contrastive Loss (RGCL) further enhances robustness by leveraging all negative samples and maximizing the loss distribution discrepancy between clean and noisy samples, thereby suppressing overfitting to noisy labels. We conduct extensive experiments on three public benchmark datasets, namely CUHK-PEDES, ICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our RDECL."}
{"paperId": "e2ce2c03f672afadbb8922338304654eb02cfbb9", "url": "https://www.semanticscholar.org/paper/e2ce2c03f672afadbb8922338304654eb02cfbb9", "title": "HyHE:Enhancing Image-Text Retrieval through Hyperbolic Hierarchical Embeddings", "venue": "International Conference on Multimedia Retrieval", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3731715.3733361?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3731715.3733361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2025-06-30", "authors": [{"authorId": "2368445600", "name": "Aohui Miao"}, {"authorId": "2260802218", "name": "Wei Wei"}], "abstract": "Image-text retrieval is a fundamental task in bridging vision and language, yet existing methods predominantly focus on one-to-one correspondences between image-text pairs, overlooking the inherent semantic generalization differences between the two modalities. While images provide detailed, pixel-level specificity, textual descriptions tend to be abstract and generalized, leading to a natural one-to-many semantic relationship. To explicitly model this relationship, we propose an image-text retrieval enhancement method called Hyperbolic Hierarchical Embeddings (HyHE), which captures two key one-to-many relationships: (1) keyword to context-keyword (WTW), where an abstract keyword (e.g., ''dog'') corresponds to multiple contextualized textual expressions (e.g., ''black dog,'' ''running dog''); and (2) context-keyword to image (WTI), where a contextualized textual concept corresponds to multiple concrete image instances. Specifically, to construct these relationships, we first leverage a part-of-speech tagging model to extract meaningful keywords from text, ensuring that key semantic elements are accurately captured.Then, these relationships together form a tree-like hierarchical semantic structure, which we embed in hyperbolic space to better preserve and model these associations. Additionally, we introduce a dynamic queue to cache context-keyword features in the historical batches, expanding the pool of negative samples and enhancing contrastive learning. Extensive experiments on the Flickr30K and MS-COCO datasets demonstrate that HyHE significantly outperforms existing methods, achieving new state-of-the-art performance in image-text retrieval tasks."}
{"paperId": "e32476e3bfda01eb302d510d4ec6f2b6c4e474c1", "url": "https://www.semanticscholar.org/paper/e32476e3bfda01eb302d510d4ec6f2b6c4e474c1", "title": "LEVERAGING COMPUTER VISION AND NATURAL LANGUAGE PROCESSING FOR OBJECT DETECTION AND LOCALIZATION", "venue": "Operations Research and Applications An International Journal", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5121/oraj.2025.12101?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5121/oraj.2025.12101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-28", "authors": [{"authorId": "2281486150", "name": "B. Rahmani"}, {"authorId": "2367905055", "name": "S. Bhavanasi"}, {"authorId": "2298040637", "name": "A. Maazallahi"}, {"authorId": "2367908404", "name": "HS Korapala"}, {"authorId": "2367905051", "name": "Jls Yenugu"}, {"authorId": "2367905716", "name": "YK Bhatia"}, {"authorId": "2367892974", "name": "MA Salari"}, {"authorId": "2226512616", "name": "E. Snir"}, {"authorId": "2281483697", "name": "P. Norouzzadeh"}, {"authorId": "2367905569", "name": "J. Fritts"}], "abstract": "This paper presents a novel approach leveraging the integration of Computer Vision, Natural Language Processing (NLP), and Speech Recognition technologies to create an AI-powered system capable of detecting and locating objects through voice commands. The system developed using Flask, OpenCV, spaCy, and Blip VQA Model, aims to assist caregivers, visually impaired individuals, and the elderly in various daily tasks, such as locating items in the home and checking for potential hazards like leaving the stove on. We also provide the code used for this project1."}
{"paperId": "e4c285272e6cfcc4df496146ac5e004f7dee898d", "url": "https://www.semanticscholar.org/paper/e4c285272e6cfcc4df496146ac5e004f7dee898d", "title": "Exploration of Vision and Language in Education: A Bibliometric Study and Its Implications", "venue": "Promotor", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32832/pro.v8i3.1190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32832/pro.v8i3.1190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-02", "authors": [{"authorId": "2365180912", "name": "Muh. Nur Haq I. S. Mannesa"}, {"authorId": "2365170947", "name": "Sabriasrifah Sabriasrifah"}, {"authorId": "2125358069", "name": "I. K. W. Pujhana"}, {"authorId": "2365185775", "name": "Sri Koesrohmaniah"}, {"authorId": "2348647511", "name": "Augustina Sulastri"}], "abstract": "This research aims to identify the specific contributions of vision and language technology to educational outcomes and to explore how these technologies can be used in current teaching practices. Using bibliometric analysis, clustering, and publications in the field of neuropsychology, this research examines the theoretical relationship between \"Vision\" and \"Language\" in the context of education. This research uses a quantitative method with a bibliometric approach to map and analyze 4,542 documents, consisting of 3,125 documents from Scopus publications and 1,414 documents from Web of Science publications from the period 1987 to 2025. This research reveals the most influential authors, collaboration maps, institutions, the most productive countries, research trends, developments, papers, and key concepts in the field of vision and language research in the context of education. The findings and conclusions of the study show the factors influencing public perception of educational research and the importance of literature in developing educational ideas and integrating productivity and environment for significant contributions in the areas of \"Vision\" and \"Language.\" This study also emphasizes the importance of using literature in a variety of fields, including theoretical studies and practical applications."}
{"paperId": "e516a3b3d0129abe25e92dc1d90862d3ca797dae", "url": "https://www.semanticscholar.org/paper/e516a3b3d0129abe25e92dc1d90862d3ca797dae", "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "venue": "arXiv.org", "year": 2025, "citationCount": 17, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.08751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-13", "authors": [{"authorId": "2304323459", "name": "Saurabh Dash"}, {"authorId": "2209282547", "name": "Yiyang Nan"}, {"authorId": "2303257581", "name": "John Dang"}, {"authorId": "2186822213", "name": "Arash Ahmadian"}, {"authorId": "2283844788", "name": "Shivalika Singh"}, {"authorId": "2303318993", "name": "Madeline Smith"}, {"authorId": "8795464", "name": "Bharat Venkitesh"}, {"authorId": "2353069638", "name": "Vladyslav Shmyhlo"}, {"authorId": "2283848534", "name": "Viraat Aryabumi"}, {"authorId": "2353071917", "name": "Walter Beller-Morales"}, {"authorId": "2353068008", "name": "Jeremy Pekmez"}, {"authorId": "2353068201", "name": "Jason Ozuzu"}, {"authorId": "16326904", "name": "Pierre H. Richemond"}, {"authorId": "153563548", "name": "Acyr F. Locatelli"}, {"authorId": "2303257441", "name": "Nick Frosst"}, {"authorId": "2283848746", "name": "Phil Blunsom"}, {"authorId": "2334132612", "name": "Aidan Gomez"}, {"authorId": "2316486966", "name": "Ivan Zhang"}, {"authorId": "2818759", "name": "Marzieh Fadaee"}, {"authorId": "2333890457", "name": "Manoj Govindassamy"}, {"authorId": "2333966995", "name": "Sudip Roy"}, {"authorId": "2304322010", "name": "Matthias Gall'e"}, {"authorId": "2445273", "name": "B. Ermiş"}, {"authorId": "82290814", "name": "A. Ustun"}, {"authorId": "2257040307", "name": "Sara Hooker"}], "abstract": "Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance."}
{"paperId": "e592dd20476c3c564e99655943592fdf9596795e", "url": "https://www.semanticscholar.org/paper/e592dd20476c3c564e99655943592fdf9596795e", "title": "Measuring the Contributions of Vision and Text Modalities", "venue": "Journal for Language Technology and Computational Linguistics", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21248/jlcl.38.2025.261?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21248/jlcl.38.2025.261, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-27", "authors": [{"authorId": "79647482", "name": "Letitia Parcalabescu"}], "abstract": "This dissertation investigates multimodal transformers that process both image and text modalities together to generate outputs for various tasks (such as answering questions about images). Specifically, methods are developed to assess the effectiveness of vision and language models in combining, understanding, utilizing, and explaining information from these two modalities. The dissertation contributes to the advancement of the field in three ways: (i) by measuring specific and task-independent capabilities of vision and language models, (ii) by interpreting these models to quantify the extent to which they use and integrate information from both modalities, and (iii) by evaluating their ability to provide self-consistent explanations of their outputs to users."}
{"paperId": "e5f6948a6244a551ab5f5401c0ef040b6001209f", "url": "https://www.semanticscholar.org/paper/e5f6948a6244a551ab5f5401c0ef040b6001209f", "title": "A Unified Framework for Zero-Shot Reinforcement Learning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.20542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-23", "authors": [{"authorId": "2342420187", "name": "Jacopo Di Ventura"}, {"authorId": "2387210365", "name": "Jan Felix Kleuker"}, {"authorId": "2562595", "name": "A. Plaat"}, {"authorId": "13477045", "name": "T. Moerland"}], "abstract": "Zero-shot reinforcement learning (RL) has emerged as a setting for developing general agents in an unsupervised manner, capable of solving downstream tasks without additional training or planning at test-time. Unlike conventional RL, which optimizes policies for a fixed reward, zero-shot RL requires agents to encode representations rich enough to support immediate adaptation to any objective, drawing parallels to vision and language foundation models. Despite growing interest, the field lacks a common analytical lens. We present the first unified framework for zero-shot RL. Our formulation introduces a consistent notation and taxonomy that organizes existing approaches and allows direct comparison between them. Central to our framework is the classification of algorithms into two families: direct representations, which learn end-to-end mappings from rewards to policies, and compositional representations, which decompose the representation leveraging the substructure of the value function. Within this framework, we highlight shared principles and key differences across methods, and we derive an extended bound for successor-feature methods, offering a new perspective on their performance in the zero-shot regime. By consolidating existing work under a common lens, our framework provides a principled foundation for future research in zero-shot RL and outlines a clear path toward developing more general agents."}
{"paperId": "e60c21818f228500db457817356ee478767e3710", "url": "https://www.semanticscholar.org/paper/e60c21818f228500db457817356ee478767e3710", "title": "MemQuant: Towards Memory-aware Post-Training Quantization for Multimodal Transformers", "venue": "International Conference on Artificial Intelligence Circuits and Systems", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AICAS64808.2025.11173143?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AICAS64808.2025.11173143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-04-28", "authors": [{"authorId": "1456456276", "name": "Kazi Barria Nine"}, {"authorId": "35016099", "name": "Foroozan Karimzadeh"}, {"authorId": "2256993495", "name": "Zishen Wan"}, {"authorId": "2209204815", "name": "A. Raychowdhury"}], "abstract": "Recently Transformer model has revolutionized the fields of natural language processing (NLP) and computer vision (CV). Even though it shows unprecedented efficiency and accuracy on most tasks, memory and computation overhead can be huge, making it inadequate for real-world applications such as edge devices. Post-training quantization (PTQ) can be an efficient way to overcome this bottleneck. In this paper, we propose a novel memory-aware mixed precision PTQ which will consider memory consumption of blocks and sub-blocks inside transformer model. Our motivation originates from profiling result which varies according to application. We evaluate the proposed methodology by employing both for language model for translation tasks and a vision-to-language model for image captioning tasks. Through our approach, we achieve significant memory savings while experiencing small degradation in accuracy. We are able to achieve 10× memory savings for the weights of the annotated transformer, 5× for T5 text-to-text, and 5.5× for weights of vision-to-language model."}
{"paperId": "e63a21b3f31db643857fbd2be35b45f25781117c", "url": "https://www.semanticscholar.org/paper/e63a21b3f31db643857fbd2be35b45f25781117c", "title": "Descriptive Image-Text Matching with Graded Contextual Similarity", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.09997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-15", "authors": [{"authorId": "2213805363", "name": "Jinhyun Jang"}, {"authorId": "2361563318", "name": "Jiyeong Lee"}, {"authorId": "2265537476", "name": "Kwanghoon Sohn"}], "abstract": "Image-text matching aims to build correspondences between visual and textual data by learning their pairwise similarities. Most existing approaches have adopted sparse binary supervision, indicating whether a pair of images and sentences matches or not. However, such sparse supervision covers a limited subset of image-text relationships, neglecting their inherent many-to-many correspondences; an image can be described in numerous texts at different descriptive levels. Moreover, existing approaches overlook the implicit connections from general to specific descriptions, which form the underlying rationale for the many-to-many relationships between vision and language. In this work, we propose descriptive image-text matching, called DITM, to learn the graded contextual similarity between image and text by exploring the descriptive flexibility of language. We formulate the descriptiveness score of each sentence with cumulative term frequency-inverse document frequency (TF-IDF) to balance the pairwise similarity according to the keywords in the sentence. Our method leverages sentence descriptiveness to learn robust image-text matching in two key ways: (1) to refine the false negative labeling, dynamically relaxing the connectivity between positive and negative pairs, and (2) to build more precise matching, aligning a set of relevant sentences in a generic-to-specific order. By moving beyond rigid binary supervision, DITM enhances the discovery of both optimal matches and potential positive pairs. Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the effectiveness of our method in representing complex image-text relationships compared to state-of-the-art approaches. In addition, DITM enhances the hierarchical reasoning ability of the model, supported by the extensive analysis on HierarCaps benchmark."}
{"paperId": "e6728966bb1e1d9f4947153670e8d9d0c2da6d5b", "url": "https://www.semanticscholar.org/paper/e6728966bb1e1d9f4947153670e8d9d0c2da6d5b", "title": "Advances in Designing Scalable Graph Neural Networks: The Perspective of Graph Data Management", "venue": "SIGMOD Conference Companion", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3722212.3725634?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3722212.3725634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book", "JournalArticle", "Conference", "Review"], "publicationDate": "2025-06-22", "authors": [{"authorId": "1940000983", "name": "Ningyi Liao"}, {"authorId": "2261689385", "name": "Siqiang Luo"}, {"authorId": "2290291188", "name": "Xiaokui Xiao"}, {"authorId": "2289121706", "name": "Reynold Cheng"}], "abstract": "Graph Neural Network (GNN) is a successful marriage of graph data management and deep learning, leading to notable improvements in learning quality over graphs. This advancement highly impacts graph-based applications in many areas, including computer vision, natural language processing, biology, medication, and social science. Despite the success, scaling up GNN models poses a formidable and long-lasting challenge, hindering the application to industrial-level graphs featuring millions or billions of nodes and edges. The rapid update of tasks and models requires continuous efforts in developing scalable GNN architectures. In specific, the scalability bottleneck of GNNs typically stem from graph-related computations, entailing more proficient processing and utilization of the unstructured graph data. There has been a marked trend of incorporation between GNN and data management to tackle newly-emerged scalability challenges. This includes the utilization of graph algorithms such as Personalized PageRank (PPR) and subgraph discovery in GNN models, as well as exploring topics in graph domain including multi-scale representation and graph spectrum. This primer tutorial (3 hours) aims to provide a comprehensive overview of scalable GNN designs, highlighting the most recent and prominent models that focus on the scalability issue. We will also summarize the technical challenges and suggest potential future directions regarding the rapid developments in this field. We believe that this work can be used as one important reference for researchers looking to develop scalable GNN models."}
{"paperId": "e69ee502ce4e0f9e68acc5c6c863c800baa2e6be", "url": "https://www.semanticscholar.org/paper/e69ee502ce4e0f9e68acc5c6c863c800baa2e6be", "title": "UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.17355, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review"], "publicationDate": "2025-11-21", "authors": [{"authorId": "2394372464", "name": "Taixi Chen"}, {"authorId": "2394011502", "name": "Jingyun Chen"}, {"authorId": "2393986842", "name": "Nancy Guo"}], "abstract": "Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis."}
{"paperId": "e73893701fb10ad4cdcd3b9309947965dacea03b", "url": "https://www.semanticscholar.org/paper/e73893701fb10ad4cdcd3b9309947965dacea03b", "title": "GroundingMate: Aiding Object Grounding for Goal-Oriented Vision-and-Language Navigation", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV61041.2025.00180?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV61041.2025.00180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2331233540", "name": "Qianyi Liu"}, {"authorId": "2277447303", "name": "Siqi Zhang"}, {"authorId": "80526284", "name": "Yanyuan Qiao"}, {"authorId": "2323532476", "name": "Junyou Zhu"}, {"authorId": "2351501152", "name": "Xiang Li"}, {"authorId": "26982950", "name": "Longteng Guo"}, {"authorId": "2291994430", "name": "Qunbo Wang"}, {"authorId": "153003010", "name": "Xingjian He"}, {"authorId": "2292302699", "name": "Qi Wu"}, {"authorId": "2331583727", "name": "Jing Liu"}], "abstract": "Goal-Oriented Vision-and-Language Navigation (VLN) aims to enable agents to navigate to specified locations and identify designated target objects following natural language instruction. This approach has gained popularity due to its close alignment with real-world scenarios. However, existing studies have predominantly focused on enhancing navigation performance, neglecting the ability to locate objects at the navigation endpoint. This oversight has resulted in a significant discrepancy between the success rates of navigation and object grounding. The challenge is compounded by the complex reasoning required by the instructions and the necessity to synthesize multiperspective images of objects, which overwhelms traditional object grounding methods. We leverage the Multi-Modal Large Language Model (MLLM) to bridge this gap, allowing agents to seek assistance from these models when struggling to locate the target object. The agent conducts a multi-stage evaluation to discern the cause of its confusion and promptly extracts and updates the most relevant information for MLLM to assess. Our method is plug-and-play and model-agnostic, facilitating integration with numerous existing VLN strategies without the need for retraining. Implementing our approach across four distinct methods has improved performance on the REVERIE and SOON datasets, demonstrating the effectiveness and generalizability of our technique."}
{"paperId": "e7a24fe39431edc429ec8608a66d850acee0cb60", "url": "https://www.semanticscholar.org/paper/e7a24fe39431edc429ec8608a66d850acee0cb60", "title": "Listen, Perceive, Grasp: CLIP-Driven Attribute-Aware Network for Language-Conditioned Visual Segmentation and Grasping", "venue": "IEEE Transactions on Automation Science and Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASE.2024.3510777?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASE.2024.3510777, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2244596479", "name": "Jialong Xie"}, {"authorId": "2108457928", "name": "Jin Liu"}, {"authorId": "2260432862", "name": "Saike Huang"}, {"authorId": "2260435916", "name": "Chaoqun Wang"}, {"authorId": "2256375023", "name": "Fengyu Zhou"}], "abstract": "Endowing robots with the ability to understand natural language and execute grasping is a challenging task in a human-centric environment. Existing works on language-conditioned grasping achieve end-to-end grasping detection based on language. However, these works lack fine-grained visual grounding, resulting in cognitive deficits for robots. Moreover, they ignore the correlation between visual attributes of objects and grasping, leading to coarse grasp poses. To this end, we propose a CLIP-driven aTtribute-aware network (CTNet) for language-conditioned visual segmentation and grasping, enabling the robots to listen, perceive, and grasp the referred object in real-world applications. Specifically, we first employ Listen stage to understand basic linguistic and visual concepts. Subsequently, we introduce Perceive stage to mine multi-modal features and visual attribute cues (e.g., boundary and spatial location), then yield a language-conditioned segmentation mask. Further, we design Grasp stage to aggregate the perceived attribute information and refine the spatial location and grasping rectangle, generating a high-quality grasp pose. Lastly, we provide an extended large dataset Ref-OCID-Grasp to train and test our method, achieving a grasping accuracy of 97.76% and segmentation OIoU of 91.82%. The real-world robotic applications demonstrate the effectiveness of our proposed approach. The project, video, and dataset can be found at https://ctnetgrasp.github.io. Note to Practitioners—Most of the existing grasping methods focus on clearing all objects in the workspace. However, as robots integrate into human society, robots should learn to grasp the desired object by understanding human language. Therefore, language-conditioned grasping is a significant skill for human-robot collaboration. The prior works directly complete the grasp detection through the language-grasp paradigm, but they ignore the discussion on whether the robot understands the concept of vision and language expression of the object. Therefore, this paper proposed the Listen-Perceive-Grasp paradigm, in which the Listen-Perceive stage is responsible for the conception alignment of the object in language expression and visual pixels, and the Perceive-Grasp stage achieves the constraining and refining the grasp detection by the perceived visual attributes such as boundary and shape. Experiments show that this method can obtain a refiner grasp pose in cluttered environments and perform language-conditioned grasping well in the real world. In future research, we will work on 6-DoF grasping and multi-object disambiguation conditioned on language."}
{"paperId": "e833a04307c896e4779873befad580c822f981c3", "url": "https://www.semanticscholar.org/paper/e833a04307c896e4779873befad580c822f981c3", "title": "VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.12845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-14", "authors": [{"authorId": "2293720361", "name": "Jesse Atuhurra"}, {"authorId": "2301581430", "name": "Iqra Ali"}, {"authorId": "34758951", "name": "Tomoya Iwakura"}, {"authorId": "2300756", "name": "Hidetaka Kamigaito"}, {"authorId": "2306967077", "name": "Tatsuya Hiraoka"}], "abstract": "Vision Language Models (VLMs) are pivotal for advancing perception in intelligent agents. Yet, evaluation of VLMs remains limited to predominantly English-centric benchmarks in which the image-text pairs comprise short texts. To evaluate VLM fine-grained abilities, in four languages under long-text settings, we introduce a novel multilingual benchmark VLURes featuring eight vision-and-language tasks, and a pioneering unrelatedness task, to probe the fine-grained Visual and Linguistic Understanding capabilities of VLMs across English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets, curated from web resources in the target language, encompass ten diverse image categories and rich textual context, introducing valuable vision-language resources for Swahili and Urdu. By prompting VLMs to generate responses and rationales, evaluated automatically and by native speakers, we uncover performance disparities across languages and tasks critical to intelligent agents, such as object recognition, scene understanding, and relationship understanding. We conducted evaluations of ten VLMs with VLURes. The best performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human performance by 6.7%, though the gap is larger for open-source models. The gap highlights VLURes'critical role in developing intelligent agents to tackle multi-modal visual reasoning."}
{"paperId": "e87d34032cc2a77a7711a651296a8f51c9474929", "url": "https://www.semanticscholar.org/paper/e87d34032cc2a77a7711a651296a8f51c9474929", "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents", "venue": "arXiv.org", "year": 2025, "citationCount": 41, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-02-20", "authors": [{"authorId": "51130642", "name": "Deepak Nathani"}, {"authorId": "151093281", "name": "Lovish Madaan"}, {"authorId": "2349814005", "name": "Nicholas Roberts"}, {"authorId": "2223756247", "name": "Niko-lay Bashlykov"}, {"authorId": "2313925780", "name": "A. Menon"}, {"authorId": "2346328435", "name": "Vincent Moens"}, {"authorId": "2346325585", "name": "Amar Budhiraja"}, {"authorId": "2408467", "name": "Despoina Magka"}, {"authorId": "2346324207", "name": "Vladislav Vorotilov"}, {"authorId": "2346326520", "name": "Gaurav Chaurasia"}, {"authorId": "3449411", "name": "Dieuwke Hupkes"}, {"authorId": "2313909428", "name": "Ricardo Silveira Cabral"}, {"authorId": "2342448415", "name": "Tatiana Shavrina"}, {"authorId": "2320805323", "name": "Jakob Foerster"}, {"authorId": "1698412", "name": "Yoram Bachrach"}, {"authorId": "2346440100", "name": "William Yang Wang"}, {"authorId": "48647153", "name": "R. Raileanu"}], "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents."}
{"paperId": "e92ca337fb992f0cae09edfc8bfd1a48132f719b", "url": "https://www.semanticscholar.org/paper/e92ca337fb992f0cae09edfc8bfd1a48132f719b", "title": "Vision Transformer-Based Context-Aware Image Captioning System with Transformer Decoder Integration", "venue": "International Conference on Circuit, Power and Computing Technologies", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCPCT65132.2025.11176681?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCPCT65132.2025.11176681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-07", "authors": [{"authorId": "2384464797", "name": "D.S. Jayakumari"}, {"authorId": "77841467", "name": "P. Venkadesh"}, {"authorId": "2258489242", "name": "S. V. Divya"}, {"authorId": "2386861668", "name": "K. Santhoshkumar"}, {"authorId": "2386858605", "name": "S. Elakkian"}, {"authorId": "2386859218", "name": "R. Sarathi"}], "abstract": "Image captioning focuses on enabling machines to generate meaningful descriptions of images by blending techniques from computer vision and natural language processing. Traditional approaches rely on Convolutional Neural Networks (CNNs) for feature extraction and Recurrent Neural Networks (RNNs) for sequence generation. However, these methods struggle with capturing global dependencies and complex semantic relationships within images. This paper proposes a novel image captioning framework that integrates Vision Transformers (ViTs) for robust feature extraction and a Transformer decoder for generating coherent captions. The ViT module captures global visual semantics, while the Transformer decoder ensures fluency and contextual accuracy in caption generation. Our approach is evaluated on benchmark datasets, including MS COCO and Flickr30k, demonstrating superior performance over conventional CNN-RNN architectures in BLEU, METEOR, and CIDEr scores. The results highlight the effectiveness of Transformer-based architectures in bridging vision and language tasks, paving the way for more advanced multimodal AI applications."}
{"paperId": "e99c2a920a4dcad589723b8afcbe28671ae3eefc", "url": "https://www.semanticscholar.org/paper/e99c2a920a4dcad589723b8afcbe28671ae3eefc", "title": "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-17", "authors": [{"authorId": "2345818221", "name": "Hugh Mee Wong"}, {"authorId": "1764022", "name": "R. Nouwen"}, {"authorId": "2307004114", "name": "Albert Gatt"}], "abstract": "Vague quantifiers such as\"a few\"and\"many\"are influenced by various contextual factors, including the number of objects present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20,300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes."}
{"paperId": "e9fcfaa6e265ce5682f513285e6adfb908b024ca", "url": "https://www.semanticscholar.org/paper/e9fcfaa6e265ce5682f513285e6adfb908b024ca", "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.13524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": null, "publicationDate": "2025-11-17", "authors": [{"authorId": "2350562306", "name": "Yuhang Peng"}, {"authorId": "2392986860", "name": "Yizhou Pan"}, {"authorId": "2393226949", "name": "Xinning He"}, {"authorId": "2350520012", "name": "Jihaoyu Yang"}, {"authorId": "2394121958", "name": "Xinyu Yin"}, {"authorId": "2351448536", "name": "Han Wang"}, {"authorId": "2292341211", "name": "Xiaoji Zheng"}, {"authorId": "2395848518", "name": "Chao Gao"}, {"authorId": "2350760242", "name": "Jiangtao Gong"}], "abstract": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality."}
{"paperId": "ea46541823439e9d0e187b0ec2960db192f9309e", "url": "https://www.semanticscholar.org/paper/ea46541823439e9d0e187b0ec2960db192f9309e", "title": "CE-FAM: Concept-Based Explanation via Fusion of Activation Maps", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.23849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-28", "authors": [{"authorId": "31323652", "name": "Michihiro Kuroki"}, {"authorId": "2382922850", "name": "Toshihiko Yamasaki"}], "abstract": "Although saliency maps can highlight important regions to explain the reasoning behind image classification in artificial intelligence (AI), the meaning of these regions is left to the user's interpretation. In contrast, conceptbased explanations decompose AI predictions into humanunderstandable concepts, clarifying their contributions. However, few methods can simultaneously reveal what concepts an image classifier learns, which regions are associated with them, and how they contribute to predictions. We propose a novel concept-based explanation method, Concept-based Explanation via Fusion of Activation Maps (CE-FAM). It employs a branched network that shares activation maps with an image classifier and learns to mimic the embeddings of a Vision and Language Model (VLM). The branch network predicts concepts in an image, and their corresponding regions are represented by a weighted sum of activation maps, with weights given by the gradients of the concept prediction scores. Their contributions are quantified based on their impact on the image classification score. Our method provides a general framework for identifying the concept regions and their contributions while leveraging VLM knowledge to handle arbitrary concepts without requiring an annotated dataset. Furthermore, we introduce a novel evaluation metric to assess the accuracy of the concept regions. Our qualitative and quantitative evaluations demonstrate our method outperforms existing approaches and excels in zero-shot inference for unseen concepts."}
{"paperId": "ead8cdd7b9240fd9f8f55fc5a73517ffe48019ca", "url": "https://www.semanticscholar.org/paper/ead8cdd7b9240fd9f8f55fc5a73517ffe48019ca", "title": "Automated data curation for self-supervised learning in underwater acoustic analysis", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-26", "authors": [{"authorId": "2287018031", "name": "Hilde I. Hummel"}, {"authorId": "1919080", "name": "S. Bhulai"}, {"authorId": "2080210232", "name": "Burooj Ghani"}, {"authorId": "2247089161", "name": "R. V. D. Mei"}], "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels of sound pollution, making monitoring crucial to understand its variability and impact. Passive acoustic monitoring (PAM) systems collect a large amount of underwater sound recordings, but the large volume of data makes manual analysis impossible, creating the need for automation. Although machine learning offers a potential solution, most underwater acoustic recordings are unlabeled. Self-supervised learning models have demonstrated success in learning from large-scale unlabeled data in various domains like computer vision, Natural Language Processing, and audio. However, these models require large, diverse, and balanced datasets for training in order to generalize well. To address this, a fully automated self-supervised data curation pipeline is proposed to create a diverse and balanced dataset from raw PAM data. It integrates Automatic Identification System (AIS) data with recordings from various hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw audio data is sampled and then combined with AIS samples to create a balanced and diverse dataset. The resulting curated dataset enables the development of self-supervised learning models, facilitating various tasks such as monitoring marine mammals and assessing sound pollution."}
{"paperId": "ebbe48097105af75a7cb1e4f4901d4008f1bbc02", "url": "https://www.semanticscholar.org/paper/ebbe48097105af75a7cb1e4f4901d4008f1bbc02", "title": "Bridging Vision and Language: Optimal Transport-Driven Radiology Report Generation via LLMs", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.03908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-04", "authors": [{"authorId": "2313579798", "name": "Haifeng Zhao"}, {"authorId": "2372416977", "name": "Yufei Zhang"}, {"authorId": "2268428177", "name": "Leilei Ma"}, {"authorId": "2355618848", "name": "Shuo Xu"}, {"authorId": "2313963935", "name": "Dengdi Sun"}], "abstract": "Radiology report generation represents a significant application within medical AI, and has achieved impressive results. Concurrently, large language models (LLMs) have demonstrated remarkable performance across various domains. However, empirical validation indicates that general LLMs tend to focus more on linguistic fluency rather than clinical effectiveness, and lack the ability to effectively capture the relationship between X-ray images and their corresponding texts, thus resulting in poor clinical practicability. To address these challenges, we propose Optimal Transport-Driven Radiology Report Generation (OTDRG), a novel framework that leverages Optimal Transport (OT) to align image features with disease labels extracted from reports, effectively bridging the cross-modal gap. The core component of OTDRG is Alignment \\&Fine-Tuning, where OT utilizes results from the encoding of label features and image visual features to minimize cross-modal distances, then integrating image and text features for LLMs fine-tuning. Additionally, we design a novel disease prediction module to predict disease labels contained in X-ray images during validation and testing. Evaluated on the MIMIC-CXR and IU X-Ray datasets, OTDRG achieves state-of-the-art performance in both natural language generation (NLG) and clinical efficacy (CE) metrics, delivering reports that are not only linguistically coherent but also clinically accurate."}
{"paperId": "ebd7d47ec5db20d2674bf26fd78a54949299c9e2", "url": "https://www.semanticscholar.org/paper/ebd7d47ec5db20d2674bf26fd78a54949299c9e2", "title": "From Pixels to Words - Towards Native Vision-Language Primitives at Scale", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-16", "authors": [{"authorId": "2382916737", "name": "Haiwen Diao"}, {"authorId": "2313697618", "name": "Mingxuan Li"}, {"authorId": "2218965006", "name": "Silei Wu"}, {"authorId": "2386099010", "name": "Linjun Dai"}, {"authorId": "2274490604", "name": "Xiaohua Wang"}, {"authorId": "153807698", "name": "Hanming Deng"}, {"authorId": "2362642365", "name": "Lewei Lu"}, {"authorId": "2384411640", "name": "Dahua Lin"}, {"authorId": "2363063462", "name": "Ziwei Liu"}], "abstract": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."}
{"paperId": "ebd80d400f2a9878de1b3fbc812f2a50b4f1663c", "url": "https://www.semanticscholar.org/paper/ebd80d400f2a9878de1b3fbc812f2a50b4f1663c", "title": "Comparative Study of Text-to-Image Generative Models", "venue": "2025 6th International Conference on Artificial Intelligence and Data Sciences (AiDAS)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AiDAS67696.2025.11213874?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AiDAS67696.2025.11213874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-09-02", "authors": [{"authorId": "2390654999", "name": "Muhammad Haziq Nordin"}, {"authorId": "2314614569", "name": "Nur Atiqah Sia Abdullah"}, {"authorId": "1765988", "name": "M. M. Rosli"}], "abstract": "Neural networks like Deep Learning have regained attention for their classification abilities, which deep generative models can create digital images from text, text-toimage synthesis (T2I) combines computer vision, natural language processing, and machine learning. This paper addresses the challenges of generating high-quality images containing multiple objects and developing reliable evaluation metrics for T2I synthesis. We compare three prominent T2I models GALIP (GAN-based), MUSE (Transformer-based), and Stable Diffusion (DM-based) to identify the most suitable model for various tasks. The methodology involves a comprehensive evaluation process for generative models in T2I generation. It includes a process flow with four phases: data preparation, model initialization, synthetic image generation and metrics computation. We also evaluate the performance of these models using Fréchet Inception Distance (FID), Inception Score (IS), CLIPScore, Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio (PSNR). The results indicate that GALIP excels in general tasks with diverse datasets like COCO due to its high IS and low FID. MUSE performs best in specialized datasets such as CUB and Oxford, with superior SSIM and PSNR scores. Stable Diffusion demonstrates versatility across all datasets. The paper highlights the impact of training data on model performance and underscores the need for improved evaluation metrics to better assess image fidelity, quality, and contextual relevance. Future work includes integrating advanced training techniques, sophisticated data augmentation, and human-preference aligned metrics to optimize T2I model performance."}
{"paperId": "ec544cd06e3d43bd98c90014d5fe27159c8be011", "url": "https://www.semanticscholar.org/paper/ec544cd06e3d43bd98c90014d5fe27159c8be011", "title": "ADVANCING ARTIFICIAL INTELLIGENCE: AN IN-DEPTH LOOK AT MACHINE LEARNING AND DEEP LEARNING ARCHITECTURES, METHODOLOGIES, APPLICATIONS, AND FUTURE TRENDS", "venue": "International Journal of Intelligent Data and Machine Learning", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55640/ijidml-v02i01-02?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55640/ijidml-v02i01-02, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2370030003", "name": "Dr. Tanay Deshpande"}, {"authorId": "2370572553", "name": "Dr. Kavita Sharma"}], "abstract": "Artificial Intelligence (AI) has emerged as a transformative force across various domains, with Machine Learning (ML) and its subset, Deep Learning (DL), at its core. This article provides a comprehensive exploration of ML and DL, delving into their fundamental concepts, diverse architectural paradigms, typical workflow, and wide-ranging applications. We discuss the evolution from traditional ML algorithms to complex deep neural networks, highlighting key methodologies like supervised, unsupervised, and reinforcement learning. The article outlines the practical workflow involved in developing ML and DL solutions, from data acquisition to deployment. Furthermore, it showcases the profound impact of these technologies across sectors such as computer vision, natural language processing, healthcare, finance, agriculture, and robotics. Finally, we explore emerging trends and future directions, including the growing importance of Explainable AI (XAI), ethical considerations, federated learning, and quantum machine learning, underscoring the continuous evolution and societal implications of this rapidly advancing field."}
{"paperId": "ec7939fabdd450b2e9333eb18e139e9a0202d9de", "url": "https://www.semanticscholar.org/paper/ec7939fabdd450b2e9333eb18e139e9a0202d9de", "title": "The Latest Research Progress of Attention Mechanism in Deep Learning", "venue": "Journal of Electronic Research and Application", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.26689/jera.v9i3.10597?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.26689/jera.v9i3.10597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-05-29", "authors": [{"authorId": "2364378065", "name": "Xu Jiang"}, {"authorId": "2364825027", "name": "Xiaoling Bai"}, {"authorId": "2365457148", "name": "Lifeng Yin"}], "abstract": "With the development of artificial intelligence and deep learning, the attention mechanism has become a key technology for enhancing the performance of complex tasks. This paper reviews the evolution of attention mechanisms, including soft attention, hard attention, and recent innovations such as multi-head latent attention and cross-attention. It focuses on the latest research outcomes, such as lightning attention, the PADRe polynomial attention replacement algorithm, the context anchor attention module, and improvements in attention mechanisms for large models. These advancements improve the efficiency and accuracy of models, expanding the application potential of attention mechanisms in fields such as computer vision, natural language processing, and remote sensing object detection, aiming to provide readers with a comprehensive understanding and stimulate innovative thinking."}
{"paperId": "ecdbed008d38328141d3eb73d806857ac665c57d", "url": "https://www.semanticscholar.org/paper/ecdbed008d38328141d3eb73d806857ac665c57d", "title": "CoLLM: A Large Language Model for Composed Image Retrieval", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-25", "authors": [{"authorId": "2052246746", "name": "Chuong Huynh"}, {"authorId": "2237592750", "name": "Jinyu Yang"}, {"authorId": "2311113085", "name": "Ashish Tawari"}, {"authorId": "2349243540", "name": "Mubarak Shah"}, {"authorId": "2293171936", "name": "Son Tran"}, {"authorId": "2352023831", "name": "Raffay Hamid"}, {"authorId": "3191220", "name": "Trishul M. Chilimbi"}, {"authorId": "2294333257", "name": "Abhinav Shrivastava"}], "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field. Project page is at collm-cvpr25.github.io."}
{"paperId": "ed02586eb1d8db49c75bff5555761cc58424ab02", "url": "https://www.semanticscholar.org/paper/ed02586eb1d8db49c75bff5555761cc58424ab02", "title": "Machine Learning, Artificial Intelligence and Policy Challenges in Economic and Health Sciences", "venue": "Scientific Hypotheses", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.69530/x3879h56?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.69530/x3879h56, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-30", "authors": [{"authorId": "2203877035", "name": "Farnaz Shirani Bidabadi"}, {"authorId": "2363476964", "name": "Mina Fahmy"}, {"authorId": "2363470185", "name": "Mehran Hemati"}], "abstract": "As artificial intelligence capabilities continue to advance at a rapid pace, machine learning has become integral to driving innovation across various industries. Machine learning refers to the ability of computer systems to learn from large amounts of data without being explicitly programmed. There are three main types of machine learning - supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves using labeled examples to train machine learning models to produce outputs based on given inputs. Unsupervised learning looks for hidden patterns in unlabeled data to perform tasks like clustering, association, and dimensionality reduction. Reinforcement learning focuses on learning via trial-and-error interactions with a dynamic environment, typically involving reward feedback loops. Within machine learning, two main approaches that have achieved significant success in recent years are deep learning and neural networks. Deep learning utilizes artificial neural networks, which are loosely modeled after biological neural connections in the brain. These networks can self-adjust through a process known as backpropagation to iteratively learn representations of data with multiple levels of abstraction. Deep learning algorithms have been applied to solve complex problems in computer vision, natural language processing, recommendation systems, and other domains by taking advantage of vast amounts of available data. As machine learning continues to power increasingly autonomous systems, addressing challenges around algorithmic bias, explain ability, privacy and security will be crucial to ensuring its transparent and ethical development and implementation."}
{"paperId": "ed1bac660dd5e4a5e59edb52aea9f9bc8fde7490", "url": "https://www.semanticscholar.org/paper/ed1bac660dd5e4a5e59edb52aea9f9bc8fde7490", "title": "PhishingHook: Catching Phishing Ethereum Smart Contracts leveraging EVM Opcodes", "venue": "Dependable Systems and Networks", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.19480, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-06-23", "authors": [{"authorId": "2302424585", "name": "Pasquale De Rosa"}, {"authorId": "2193297716", "name": "Simon Queyrut"}, {"authorId": "49030221", "name": "Yérom-David Bromberg"}, {"authorId": "2240547002", "name": "Pascal Felber"}, {"authorId": "2106027", "name": "V. Schiavoni"}], "abstract": "The Ethereum Virtual Machine (EVM) is a decentralized computing engine. It enables the Ethereum blockchain to execute smart contracts and decentralized applications (dApps). The increasing adoption of Ethereum sparked the rise of phishing activities. Phishing attacks often target users through deceptive means, e.g., fake websites, wallet scams, or malicious smart contracts, aiming to steal sensitive information or funds. A timely detection of phishing activities in the EVM is therefore crucial to preserve the user trust and network integrity. Some state-of-the art approaches to phishing detection in smart contracts rely on the online analysis of transactions and their traces. However, replaying transactions often exposes sensitive user data and interactions, with several security concerns. In this work, we present PhishingHook, a framework that applies machine learning techniques to detect phishing activities in smart contracts by directly analyzing the contract’s bytecode and its constituent opcodes. We evaluate the efficacy of such techniques in identifying malicious patterns, suspicious function calls, or anomalous behaviors within the contract’s code itself before it is deployed or interacted with. We experimentally compare 16 techniques, belonging to four main categories (Histogram Similarity Classifiers, Vision Models, Language Models and Vulnerability Detection Models), using 7,000 real-world malware smart contracts. Our results demonstrate the efficiency of PhishingHook in performing phishing classification systems, with about 90% average accuracy among all the models. We support experimental reproducibility, and we release our code and datasets to the research community."}
{"paperId": "ed386d621f3eb3a36b299449fd595b292c6dd56a", "url": "https://www.semanticscholar.org/paper/ed386d621f3eb3a36b299449fd595b292c6dd56a", "title": "Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.00177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-29", "authors": [{"authorId": "2152135478", "name": "Faizan Farooq Khan"}, {"authorId": "2345819164", "name": "Vladan Stojni'c"}, {"authorId": "8575458", "name": "Zakaria Laskar"}, {"authorId": "1712479", "name": "Mohamed Elhoseiny"}, {"authorId": "1706195", "name": "Giorgos Tolias"}], "abstract": "This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward open-vocabulary solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach. First, we transform the text query into a visual query using a generative diffusion model. Then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries. Source code is available at: https://github.com/faixan-khan/cletir"}
{"paperId": "ede39f60cbbfa480ad832d99eb198938f6973faf", "url": "https://www.semanticscholar.org/paper/ede39f60cbbfa480ad832d99eb198938f6973faf", "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.07642, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-11", "authors": [{"authorId": "2375765420", "name": "Tianyi Ma"}, {"authorId": "2375861560", "name": "Yue Zhang"}, {"authorId": "2351422239", "name": "Zehao Wang"}, {"authorId": "2317083519", "name": "Parisa Kordjamshidi"}], "abstract": "Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments."}
{"paperId": "efa3dfe4f215a7cd3a42d6865f18e85b1ac2b74d", "url": "https://www.semanticscholar.org/paper/efa3dfe4f215a7cd3a42d6865f18e85b1ac2b74d", "title": "Zero-Shot Learning and Few-Shot Learning with Generative AI: Bridging the Data Gap for Real-World Applications", "venue": "Integrated Journal for Research in Arts and Humanities", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55544/ijrah.5.1.24?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55544/ijrah.5.1.24, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-01-30", "authors": [{"authorId": "2349893789", "name": "Vinay kumar Gali"}, {"authorId": "2349851181", "name": "Raghav Agarwal"}], "abstract": "Modern artificial intelligence systems frequently rely on vast amounts of labeled data to achieve robust performance, yet many real-world scenarios suffer from limited data availability. This paper investigates the potential of integrating zero-shot and few-shot learning paradigms with generative AI models to bridge the persistent data gap. Zero-shot learning empowers models to recognize and classify instances from unseen categories by leveraging semantic descriptors, while few-shot learning focuses on adapting models to new classes using only a handful of examples. Generative AI techniques, such as advanced generative adversarial networks and transformer-based models, can synthesize realistic data samples that mimic complex distributions found in natural environments. By combining these approaches, our methodology offers a dual advantage: it not only enhances model generalization across diverse tasks but also mitigates the challenges posed by data scarcity. We demonstrate the effectiveness of this hybrid framework through experiments in domains including computer vision, natural language processing, and anomaly detection, where traditional data collection is prohibitive. Our analysis reveals that the strategic use of generated data significantly boosts learning outcomes, even when initial training samples are sparse. Furthermore, the adaptability of the proposed system makes it suitable for dynamic, real-world applications where new categories continuously emerge. Overall, this study provides a comprehensive overview of leveraging generative AI to enhance zero-shot and few-shot learning, paving the way for more resilient and scalable solutions in environments constrained by limited data resources. These innovations promise to reshape the future of machine learning by opening new pathways for robust AI development."}
{"paperId": "efcf6e845929e9d2d880c211242d70c8d0f4bd9c", "url": "https://www.semanticscholar.org/paper/efcf6e845929e9d2d880c211242d70c8d0f4bd9c", "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.16394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-20", "authors": [{"authorId": "9282980", "name": "Akhil Perincherry"}, {"authorId": "51050450", "name": "Jacob Krantz"}, {"authorId": "2351101990", "name": "Stefan Lee"}], "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or \"imaginations\", we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of ~1 point and up to ~0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/."}
{"paperId": "efd37e272ba421debbaec8188196f237869587fa", "url": "https://www.semanticscholar.org/paper/efd37e272ba421debbaec8188196f237869587fa", "title": "Advancements and challenges in the development of generative adversarial network (GANs) for deep learning", "venue": "Discover Networks", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s44354-025-00007-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s44354-025-00007-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-11-22", "authors": [{"authorId": "2385289037", "name": "Kashif Iqbal"}, {"authorId": "9024762", "name": "Atifa Rafique"}, {"authorId": "2357162399", "name": "Sara Qaisar"}, {"authorId": "2385290076", "name": "Mujahid Tabassum"}], "abstract": null}
{"paperId": "efdfef40962d20d8f5a591002dab7448da69e4a9", "url": "https://www.semanticscholar.org/paper/efdfef40962d20d8f5a591002dab7448da69e4a9", "title": "FlashBias: Fast Computation of Attention with Bias", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.12044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-17", "authors": [{"authorId": "2051867856", "name": "Haixu Wu"}, {"authorId": "2258740917", "name": "Minghao Guo"}, {"authorId": "2258878839", "name": "Yuezhou Ma"}, {"authorId": "2362290747", "name": "Yuanxu Sun"}, {"authorId": "2362290197", "name": "Jianmin Wang"}, {"authorId": "2249537525", "name": "Wojciech Matusik"}, {"authorId": "2054275000", "name": "Mingsheng Long"}], "abstract": "Attention with bias, which extends standard attention by introducing prior knowledge as an additive bias matrix to the query-key scores, has been widely deployed in vision, language, protein-folding and other advanced scientific models, underscoring its status as a key evolution of this foundational module. However, introducing bias terms creates a severe efficiency bottleneck in attention computation. It disrupts the tightly fused memory-compute pipeline that underlies the speed of accelerators like FlashAttention, thereby stripping away most of their performance gains and leaving biased attention computationally expensive. Surprisingly, despite its common usage, targeted efficiency optimization for attention with bias remains absent, which seriously hinders its application in complex tasks. Diving into the computation of FlashAttention, we prove that its optimal efficiency is determined by the rank of the attention weight matrix. Inspired by this theoretical result, this paper presents FlashBias based on the low-rank compressed sensing theory, which can provide fast-exact computation for many widely used attention biases and a fast-accurate approximation for biases in general formalizations. FlashBias can fully take advantage of the extremely optimized matrix multiplication operation in modern GPUs, achieving 1.5$\\times$ speedup for Pairformer in AlphaFold 3, and over 2$\\times$ speedup for attention with bias in vision and language models without loss of accuracy. Code is available at this repository: https://github.com/thuml/FlashBias."}
{"paperId": "f01dfb6bfd45b66dc73af4897ee0d9590ee40e16", "url": "https://www.semanticscholar.org/paper/f01dfb6bfd45b66dc73af4897ee0d9590ee40e16", "title": "Multimodal Representation Learning and Fusion", "venue": "arXiv.org", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.20494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-25", "authors": [{"authorId": "2371075587", "name": "Qihang Jin"}, {"authorId": "2371072077", "name": "Enze Ge"}, {"authorId": "2371270339", "name": "Yuhang Xie"}, {"authorId": "2341879231", "name": "Hongying Luo"}, {"authorId": "2312846636", "name": "Jun-Jie Song"}, {"authorId": "2319608188", "name": "Ziqian Bi"}, {"authorId": "2330158457", "name": "C. Liang"}, {"authorId": "2371108282", "name": "Jibin Guan"}, {"authorId": "2363178413", "name": "Joe Yeong"}, {"authorId": "2371145680", "name": "Junfeng Hao"}], "abstract": "Multi-modal learning is a fast growing area in artificial intelligence. It tries to help machines understand complex things by combining information from different sources, like images, text, and audio. By using the strengths of each modality, multi-modal learning allows AI systems to build stronger and richer internal representations. These help machines better interpretation, reasoning, and making decisions in real-life situations. This field includes core techniques such as representation learning (to get shared features from different data types), alignment methods (to match information across modalities), and fusion strategies (to combine them by deep learning models). Although there has been good progress, some major problems still remain. Like dealing with different data formats, missing or incomplete inputs, and defending against adversarial attacks. Researchers now are exploring new methods, such as unsupervised or semi-supervised learning, AutoML tools, to make models more efficient and easier to scale. And also more attention on designing better evaluation metrics or building shared benchmarks, make it easier to compare model performance across tasks and domains. As the field continues to grow, multi-modal learning is expected to improve many areas: computer vision, natural language processing, speech recognition, and healthcare. In the future, it may help to build AI systems that can understand the world in a way more like humans, flexible, context aware, and able to deal with real-world complexity."}
{"paperId": "f09c4df57686906bbbd6204d4897ccbd806cc9fa", "url": "https://www.semanticscholar.org/paper/f09c4df57686906bbbd6204d4897ccbd806cc9fa", "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.04127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-10-05", "authors": [{"authorId": "2384135121", "name": "Sean Moran"}], "abstract": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in information retrieval, underpinning large-scale applications in computer vision, natural language processing, and cross-modal search. Hashing-based methods provide an efficient solution by mapping high-dimensional data into compact binary codes that enable fast similarity computations in Hamming space. Over the past two decades, a substantial body of work has explored learning to hash, where projection and quantisation functions are optimised from data rather than chosen at random. This article offers a foundational survey of early learning-based hashing methods, with an emphasis on the core ideas that shaped the field. We review supervised, unsupervised, and semi-supervised approaches, highlighting how projection functions are designed to generate meaningful embeddings and how quantisation strategies convert these embeddings into binary codes. We also examine extensions to multi-bit and multi-threshold models, as well as early advances in cross-modal retrieval. Rather than providing an exhaustive account of the most recent methods, our goal is to introduce the conceptual foundations of learning-based hashing for ANN search. By situating these early models in their historical context, we aim to equip readers with a structured understanding of the principles, trade-offs, and open challenges that continue to inform current research in this area."}
{"paperId": "f0be7bc99796eb45c45a1077418ed164f5f148c8", "url": "https://www.semanticscholar.org/paper/f0be7bc99796eb45c45a1077418ed164f5f148c8", "title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 12, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-23", "authors": [{"authorId": "2341861315", "name": "Zukang Xu"}, {"authorId": "2284689913", "name": "Yuxuan Yue"}, {"authorId": "2275103965", "name": "Xing Hu"}, {"authorId": "2284800405", "name": "Zhihang Yuan"}, {"authorId": "2341714071", "name": "Zixu Jiang"}, {"authorId": "2341715066", "name": "Zhixuan Chen"}, {"authorId": "2267504904", "name": "Jiangyong Yu"}, {"authorId": "2275197667", "name": "Chen Xu"}, {"authorId": "2303517459", "name": "Sifan Zhou"}, {"authorId": "2274968021", "name": "Dawei Yang"}], "abstract": "Mamba is an efficient sequence model that rivals Transformers and demonstrates significant potential as a foundational architecture for various tasks. Quantization is commonly used in neural networks to reduce model size and computational latency. However, applying quantization to Mamba remains underexplored, and existing quantization methods, which have been effective for CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot suffers a 21% accuracy drop on Vim-T$^\\dagger$ even under W8A8). We have pioneered the exploration of this issue and identified several key challenges. First, significant outliers are present in gate projections, output projections, and matrix multiplications. Second, Mamba's unique parallel scan further amplifies these outliers, leading to uneven and heavy-tailed data distributions. Third, even with the application of the Hadamard transform, the variance across channels in weights and activations still remains inconsistent. To these ends, we propose MambaQuant, a post-training quantization (PTQ) framework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced rotation, rendering the rotation matrix adaptable to diverse channel distributions. 2) Smooth-Fused rotation, which equalizes channel variances and can merge additional parameters into model weights. Experiments show that MambaQuant can quantize both weights and activations into 8-bit with less than 1% accuracy loss for Mamba-based vision and language tasks. To the best of our knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba family, paving the way for further advancements in its application."}
{"paperId": "f12367a4a0f8a79ade0b9db692b00b10939bc9cf", "url": "https://www.semanticscholar.org/paper/f12367a4a0f8a79ade0b9db692b00b10939bc9cf", "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.14357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-16", "authors": [{"authorId": "2349269580", "name": "Xiaobei Zhao"}, {"authorId": "2375388873", "name": "Xingqi Lyu"}, {"authorId": "2349772700", "name": "Xiang Li"}], "abstract": "Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/SUM-AgriVLN."}
{"paperId": "f14c75cdebc2a7fc68432800bf180749debad148", "url": "https://www.semanticscholar.org/paper/f14c75cdebc2a7fc68432800bf180749debad148", "title": "FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.02114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2383304142", "name": "Ding-Ruei Shen"}], "abstract": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research."}
{"paperId": "f1ab95632784b3ab6b1e640e1e9dfa53a7df6676", "url": "https://www.semanticscholar.org/paper/f1ab95632784b3ab6b1e640e1e9dfa53a7df6676", "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.22805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-30", "authors": [{"authorId": "2348074443", "name": "Yuqi Pang"}, {"authorId": "2346093961", "name": "Bowen Yang"}, {"authorId": "2345894002", "name": "Yun Cao"}, {"authorId": "2376415122", "name": "Fan Rong"}, {"authorId": "2374159360", "name": "Xiaoyu Li"}, {"authorId": "2374187684", "name": "Chen He"}], "abstract": "Vision large language models (VLLMs) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a sparse Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA."}
{"paperId": "f1f273a3ef89d33853e6ba1a8b8bb03ce0d53ae9", "url": "https://www.semanticscholar.org/paper/f1f273a3ef89d33853e6ba1a8b8bb03ce0d53ae9", "title": "Just Noticeable Difference for Large Multimodal Models", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.00490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-01", "authors": [{"authorId": "2268795764", "name": "Zijian Chen"}, {"authorId": "2305908765", "name": "Yuan Tian"}, {"authorId": "2355565509", "name": "Yuze Sun"}, {"authorId": "2153198409", "name": "Wei Sun"}, {"authorId": "2116459218", "name": "Zicheng Zhang"}, {"authorId": "2266768297", "name": "Weisi Lin"}, {"authorId": "2266393212", "name": "Guangtao Zhai"}, {"authorId": "2268821294", "name": "Wenjun Zhang"}], "abstract": "Just noticeable difference (JND), the minimum change that the human visual system (HVS) can perceive, has been studied for decades. Although recent work has extended this line of research into machine vision, there has been a scarcity of studies systematically exploring its perceptual boundaries across multiple tasks and stimulus types, particularly in the current era of rapidly advancing large multimodal models (LMMs), where studying the multifaceted capabilities of models has become a mainstream focus. Moreover, the perceptual defects of LMMs are not investigated thoroughly, resulting in potential security issues and suboptimal response efficiency. In this paper, we take an initial attempt and demonstrate that there exist significant visual blind spots in current LMMs. To systemically quantify this characteristic, we propose a new concept, {\\bf LMM-JND}, together with its determination pipeline. Targeting uncovering the behavior commonalities in HVS-aligned visual perception tasks, we delve into several LMM families and construct a large-scale dataset, named VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12 distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle with basic comparison queries and fall significantly short of human-level visual performance. We further explore the effects of vision and language backbones and find a notable correlation between their design philosophy that may instruct the future refinement of LMMs for their visual acuity. Together, our research underscores the significance of LMM-JND as a unique perspective for studying LMMs, and predictable LMM-JND is crucial for security concerns. This work will be available at https://github.com/zijianchen98/LMM-JND."}
{"paperId": "f2aecb2c003d219ec17a9d901e2ce03ed15d3ced", "url": "https://www.semanticscholar.org/paper/f2aecb2c003d219ec17a9d901e2ce03ed15d3ced", "title": "AeroVerse-Review: Comprehensive survey on aerial embodied vision-and-language navigation", "venue": "The Innovation Informatics", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.59717/j.xinn-inform.2025.100015?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.59717/j.xinn-inform.2025.100015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2371990687", "name": "Fanglong Yao"}, {"authorId": "2317117846", "name": "Youzhi Liu"}, {"authorId": "2391283667", "name": "Wenyi Zhang"}, {"authorId": "2391485521", "name": "Zhengqiu Zhu"}, {"authorId": "2391689255", "name": "Chenglong Li"}, {"authorId": "2340944499", "name": "Nayu Liu"}, {"authorId": "2392598890", "name": "Peng Hu"}, {"authorId": "2317155302", "name": "Yuanchang Yue"}, {"authorId": "2391302520", "name": "Kaiwen Wei"}, {"authorId": "2391830986", "name": "Xin He"}, {"authorId": "2391851173", "name": "Xudong Zhao"}, {"authorId": "2391482327", "name": "Zihan Wei"}, {"authorId": "2391641329", "name": "Haotian Xu"}, {"authorId": "2391618758", "name": "Zhiyuan Wang"}, {"authorId": "2391475270", "name": "Gujie Shao"}, {"authorId": "2391739785", "name": "Liu Yang"}, {"authorId": "2393234590", "name": "Dan Zhao"}, {"authorId": "2374339189", "name": "Yong Yang"}], "abstract": "With the rapid advancement of unmanned aerial vehicle (UAV) technology, embedding intelligence into aerial platforms has become an increasingly important research direction. UAV-based vision-and-language navigation (UAV-VLN), as a representative paradigm of aerospace embodied intelligence, requires UAVs to understand natural language instructions and integrate multimodal perception to autonomously plan and execute navigation tasks in three-dimensional environments. This survey provides a comprehensive review of UAV-VLN research, covering simulation platforms, task definitions, core methodologies, datasets and evaluation metrics, application scenarios, as well as key challenges and future directions. We first present the design principles and capabilities of mainstream simulators, followed by a structured summary of methodological progress, including rule-based approaches, deep learning-driven models, and multi-agent collaborative strategies. We then discuss critical technical challenges in UAV-VLN, such as dynamic feasibility and control in 3D space, perception and generalization in complex environments, linguistic ambiguity and cross-modal semantic grounding, long-term spatiotemporal reasoning, and deployment under resource constraints. Based on these challenges, we outline promising future directions, including standardized benchmark development, Sim-to-Real and cross-domain transfer, pretrained large model integration, embodied world model, collaborative and interactive UAV-VLN, and embodied navigation of space-air-ground unmanned systems. This survey aims to provide a structured reference for future research and to guide the practical deployment of UAV-VLN systems."}
{"paperId": "f2e4971c22600ee75971a1b8ce3495f06816c308", "url": "https://www.semanticscholar.org/paper/f2e4971c22600ee75971a1b8ce3495f06816c308", "title": "Vision-Language Tracking With CLIP and Interactive Prompt Learning", "venue": "IEEE transactions on intelligent transportation systems (Print)", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TITS.2024.3520103?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TITS.2024.3520103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-01", "authors": [{"authorId": "2220210868", "name": "Hong Zhu"}, {"authorId": "2220709127", "name": "Qingyang Lu"}, {"authorId": "2258410043", "name": "Lei Xue"}, {"authorId": "2337591898", "name": "Pingping Zhang"}, {"authorId": "1643669386", "name": "Guangling Yuan"}], "abstract": "Vision-language tracking is a new rising topic in intelligent transportation systems, particularly significant in autonomous driving and road surveillance. It is a task that aims to combine visual and auxiliary linguistic modalities to co-locate the target object in a video sequence. Currently, multi-modal data scarcity and burdensome modality fusion have become two major factors in limiting the development of vision-language tracking. To tackle the issues, we propose an efficient and effective one-stage vision-language tracking framework (CPIPTrack) that unifies feature extraction and multi-modal fusion by interactive prompt learning. Feature extraction is performed by the high-performance vision-language foundation model CLIP, resulting in the impressive generalization ability inherited from the large-scale model. Modality fusion is simplified to a few lightweight prompts, leading to significant savings in computational resources. Specifically, we design three types of prompts to dynamically learn the layer-wise feature relationships between vision and language, facilitating rich context interactions while enabling the pre-trained CLIP adaptation. In this manner, discriminative target-oriented visual features can be extracted by language and template guidance, which are used for subsequent reasoning. Due to the elimination of extra heavy modality fusion, the proposed CPIPTrack shows high efficiency in both training and inference. CPIPTrack has been extensively evaluated on three benchmark datasets, and the experimental results demonstrate that it achieves a good performance-speed balance with an AUC of 66.0% on LaSOT and a runtime of 51.7 FPS on RTX2080 Super."}
{"paperId": "f3a9364f31dd0be9f531ab76ea0437c23945283f", "url": "https://www.semanticscholar.org/paper/f3a9364f31dd0be9f531ab76ea0437c23945283f", "title": "Panel - Multimodal Food Computing: Bridging Cultural Heritage, Artificial Intelligence, and Sustainable Food Futures", "venue": "Proceedings of the 1st International Workshop on Multi-modal Food Computing", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3746264.3761800?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3746264.3761800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Book"], "publicationDate": "2025-10-26", "authors": [{"authorId": "2210240773", "name": "Ramesh Jain"}, {"authorId": "2286066015", "name": "Yoko Yamakata"}, {"authorId": "2386302505", "name": "Maija Kale"}, {"authorId": "2302908723", "name": "Partha Pratim Das"}, {"authorId": "2337998225", "name": "Stavroula-Georgia Mougiakakou"}], "abstract": "Recent advances in artificial intelligence (AI) - spanning computer vision, natural language processing, large language models, causal machine learning, and intelligent agents - together with data from wearable sensors, mobile devices, internet sources, official guidelines, and multimodal content such as speech, text, images, and videos, are enabling a more holistic understanding of food that goes far beyond nutrition. These rich and complex data streams create opportunities to advance health, sustainability, and cultural identity, while also raising challenges of heterogeneity, standardization, interoperability, scalability, transparency, bias, and cultural sensitivity. This panel, Multimodal Food Computing: Bridging Cultural Heritage, Artificial Intelligence, and Sustainable Food Futures, will explore emerging trends, challenges, and applications. Topics include multimodal representations of food, cultural preservation, translating data into actionable insights, and trustworthy AI. Panelists from leading institutions will share perspectives in an interactive discussion, aiming to foster cross-disciplinary collaboration and responsible innovation."}
{"paperId": "f3c02dcb002cc46b624853d7e05743bfabfa85c0", "url": "https://www.semanticscholar.org/paper/f3c02dcb002cc46b624853d7e05743bfabfa85c0", "title": "The Central Role of Adaptive Optimization Algorithms in Deep Learning: A Cross-Domain Survey from CNNs to Transformers", "venue": "Applied and Computational Engineering", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54254/2755-2721/2025.tj23299?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54254/2755-2721/2025.tj23299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-05-22", "authors": [{"authorId": "2325556072", "name": "Rui-xuan Li"}, {"authorId": "2355261946", "name": "Rui Liu"}], "abstract": "This paper systematically investigates the co-evolution of adaptive optimization algorithms and deep learning architectures, analyzing their synergistic mechanisms across convolutional networks, recurrent models, generative adversarial networks, and Transformers. The author highlights how adaptive strategiessuch as gradient balancing, momentum acceleration, and variance normalizationaddress domain-specific challenges in computer vision, natural language processing, and multimodal tasks. A comparative analysis reveals performance trade-offs and architectural constraints, emphasizing the critical role of adaptive optimizers in large-scale distributed training and privacy-preserving scenarios. Emerging challenges in dynamic sparse activation, hardware heterogeneity, and multi-objective convergence are rigorously examined. The study concludes by advocating for unified theoretical frameworks that reconcile algorithmic adaptability with systemic scalability, proposing future directions in automated tuning, lightweight deployment, and cross-modal optimization to advance AI robustness and efficiency."}
{"paperId": "f48f1d97ccf9f5624e736b628bee97666c4d879c", "url": "https://www.semanticscholar.org/paper/f48f1d97ccf9f5624e736b628bee97666c4d879c", "title": "From Mock-ups to Code: A Conceptual Synthesis of AI-Driven Automatic Website Generation", "venue": "International Journal of Innovative Science and Research Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.38124/ijisrt/25nov339?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.38124/ijisrt/25nov339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-14", "authors": [{"authorId": "2211207350", "name": "Thisaranie Kaluarachchi"}], "abstract": "The emergence of artificial intelligence (AI) and machine learning has transformed many dimensions of software\n\nengineering, including the way websites are designed and developed. Traditionally, website creation has been a labour-\nintensive process involving manual translation of high-fidelity design artifacts such as sketches, wireframes, and mock-ups\n\ninto functional code. Despite the availability of visual editors and content management systems, the gap between design\nintent and implementation accuracy remains a persistent challenge. This research presents a comprehensive synthesis of\nresearch in AI-driven automatic website generation, with particular attention to the evolution of methods, their underlying\ncomputational models, and their integration with modern web design practices.\nThe discussion begins by tracing the development of website generation approaches from heuristic and template-based\nsystems to machine learning–assisted and deep learning–based frameworks. It categorizes existing methods into three major\nparadigms, mock-up-driven, example-based, and AI-driven website generation, and examines their methodological\nfoundations, advantages, and limitations. The synthesis integrates insights from computer vision, natural language\nprocessing, and code generation research to identify common principles underlying automated design translation.\nBuilding on this literature, the research introduces a conceptual framework that unifies the processes of visual input\ninterpretation, graphical user interface (GUI) element detection, semantic classification, hierarchical structuring, and code\nsynthesis. The proposed framework serves as both an analytical model and a design roadmap for future research in\nautomatic website generation. The research concludes by outlining emerging directions such as multimodal generative AI,\nhuman-in-the-loop design collaboration, and the integration of explainable AI principles in web automation. Overall, this\nsynthesis advances the theoretical understanding of design automation and provides a foundation for future innovations\nthat bridge the creative and computational aspects of web engineering."}
{"paperId": "f4c4648969681be4f32253caf6278e94f2111372", "url": "https://www.semanticscholar.org/paper/f4c4648969681be4f32253caf6278e94f2111372", "title": "Infusing fine-grained visual knowledge to Vision-Language Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.12137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-16", "authors": [{"authorId": "2143188680", "name": "Nikolaos-Antonios Ypsilantis"}, {"authorId": "2238016251", "name": "Kaifeng Chen"}, {"authorId": "2237982037", "name": "André Araujo"}, {"authorId": "150252852", "name": "Ondvrej Chum"}], "abstract": "Large-scale contrastive pre-training produces powerful Vision-and-Language Models (VLMs) capable of generating representations (embeddings) effective for a wide variety of visual and multimodal tasks. However, these pretrained embeddings remain suboptimal for fine-grained open-set visual retrieval, where state-of-the-art results require fine-tuning the vision encoder using annotated domain-specific samples. Naively performing such fine-tuning typically leads to catastrophic forgetting, severely diminishing the model's general-purpose visual and cross-modal capabilities. In this work, we propose a fine-tuning method explicitly designed to achieve optimal balance between fine-grained domain adaptation and retention of the pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual learning literature, we systematically analyze standard regularization techniques aimed at knowledge retention and propose an efficient and effective combination strategy. Additionally, we address the commonly overlooked yet critical aspects of validation set design and hyperparameter tuning to ensure reproducibility and robust generalization across datasets and pretrained models. We extensively evaluate our method on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks. Our approach consistently achieves strong results, notably retaining the visual-text alignment without utilizing any text data or the original text encoder during fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing ."}
{"paperId": "f5677f9000975e94fffbc24524c47ecfa91d27a9", "url": "https://www.semanticscholar.org/paper/f5677f9000975e94fffbc24524c47ecfa91d27a9", "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.24164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2155649080", "name": "Ngoc Dung Huynh"}, {"authorId": "3080469", "name": "Mohamed Reda Bouadjenek"}, {"authorId": "1630421985", "name": "Imran Razzak"}, {"authorId": "2296782788", "name": "Hakim Hacid"}, {"authorId": "2256860681", "name": "Sunil Aryal"}], "abstract": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language."}
{"paperId": "f58f063fa2e1541f8f653acbbdc45b887aa584dc", "url": "https://www.semanticscholar.org/paper/f58f063fa2e1541f8f653acbbdc45b887aa584dc", "title": "Context-Aware Semantic Segmentation: Enhancing Pixel-Level Understanding with Large Language Models for Advanced Vision Applications", "venue": "arXiv.org", "year": 2025, "citationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-25", "authors": [{"authorId": "2351995580", "name": "Ben Rahman"}], "abstract": "Semantic segmentation has made significant strides in pixel-level image understanding, yet it remains limited in capturing contextual and semantic relationships between objects. Current models, such as CNN and Transformer-based architectures, excel at identifying pixel-level features but fail to distinguish semantically similar objects (e.g.,\"doctor\"vs.\"nurse\"in a hospital scene) or understand complex contextual scenarios (e.g., differentiating a running child from a regular pedestrian in autonomous driving). To address these limitations, we proposed a novel Context-Aware Semantic Segmentation framework that integrates Large Language Models (LLMs) with state-of-the-art vision backbones. Our hybrid model leverages the Swin Transformer for robust visual feature extraction and GPT-4 for enriching semantic understanding through text embeddings. A Cross-Attention Mechanism is introduced to align vision and language features, enabling the model to reason about context more effectively. Additionally, Graph Neural Networks (GNNs) are employed to model object relationships within the scene, capturing dependencies that are overlooked by traditional models. Experimental results on benchmark datasets (e.g., COCO, Cityscapes) demonstrate that our approach outperforms the existing methods in both pixel-level accuracy (mIoU) and contextual understanding (mAP). This work bridges the gap between vision and language, paving the path for more intelligent and context-aware vision systems in applications including autonomous driving, medical imaging, and robotics."}
{"paperId": "f6054682589d21e7a4cb5ecb744ac85384b37b16", "url": "https://www.semanticscholar.org/paper/f6054682589d21e7a4cb5ecb744ac85384b37b16", "title": "The Architecture of Trust: A Framework for AI-Augmented Real Estate Valuation in the Era of Structured Data", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.02765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-04", "authors": [{"authorId": "3422124", "name": "Petteri Teikari"}, {"authorId": "2374479701", "name": "Mike Jarrell"}, {"authorId": "2374962069", "name": "Maryam Azh"}, {"authorId": "2374479212", "name": "Harri Pesola"}], "abstract": "The Uniform Appraisal Dataset (UAD) 3.6's mandatory 2026 implementation transforms residential property valuation from narrative reporting to structured, machine-readable formats. This paper provides the first comprehensive analysis of this regulatory shift alongside concurrent AI advances in computer vision, natural language processing, and autonomous systems. We develop a three-layer framework for AI-augmented valuation addressing technical implementation and institutional trust requirements. Our analysis reveals how regulatory standardization converging with AI capabilities enables fundamental market restructuring with profound implications for professional practice, efficiency, and systemic risk. We make four key contributions: (1) documenting institutional failures including inter-appraiser variability and systematic biases undermining valuation reliability; (2) developing an architectural framework spanning physical data acquisition, semantic understanding, and cognitive reasoning that integrates emerging technologies while maintaining professional oversight; (3) addressing trust requirements for high-stakes financial applications including regulatory compliance, algorithmic fairness, and uncertainty quantification; (4) proposing evaluation methodologies beyond generic AI benchmarks toward domain-specific protocols. Our findings indicate successful transformation requires not merely technological sophistication but careful human-AI collaboration, creating systems that augment rather than replace professional expertise while addressing historical biases and information asymmetries in real estate markets."}
{"paperId": "f6ea61a5b8f7a35493435595fbf2d2954b4b1009", "url": "https://www.semanticscholar.org/paper/f6ea61a5b8f7a35493435595fbf2d2954b4b1009", "title": "Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2511.02046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-11-03", "authors": [{"authorId": "2170389425", "name": "Soham Joshi"}, {"authorId": "2233304809", "name": "Shwet Kamal Mishra"}, {"authorId": "2334872857", "name": "Viswanath Gopalakrishnan"}], "abstract": "Creation of large-scale databases for Visual Question Answering tasks pertaining to the text data in a scene (text-VQA) involves skilful human annotation, which is tedious and challenging. With the advent of foundation models that handle vision and language modalities, and with the maturity of OCR systems, it is the need of the hour to establish an end-to-end pipeline that can synthesize Question-Answer (QA) pairs based on scene-text from a given image. We propose a pipeline for automated synthesis for text-VQA dataset that can produce faithful QA pairs, and which scales up with the availability of scene text data. Our proposed method harnesses the capabilities of multiple models and algorithms involving OCR detection and recognition (text spotting), region of interest (ROI) detection, caption generation, and question generation. These components are streamlined into a cohesive pipeline to automate the synthesis and validation of QA pairs. To the best of our knowledge, this is the first pipeline proposed to automatically synthesize and validate a large-scale text-VQA dataset comprising around 72K QA pairs based on around 44K images."}
{"paperId": "f6f60652475e8f61b129f0e3b972ba6c239bdb4b", "url": "https://www.semanticscholar.org/paper/f6f60652475e8f61b129f0e3b972ba6c239bdb4b", "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs", "venue": "Trans. Mach. Learn. Res.", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.11808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-16", "authors": [{"authorId": "2330333865", "name": "Kishan Gurumurthy"}, {"authorId": "2340218610", "name": "Himanshu Pal"}, {"authorId": "2333729318", "name": "Charu Sharma"}], "abstract": "Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings. Open-source code available on GitHub (https://github.com/SpringWiz11/Fed-GNODEFormer)."}
{"paperId": "f7526bd02a9f5fb82d5a246807ddc83cacf6c335", "url": "https://www.semanticscholar.org/paper/f7526bd02a9f5fb82d5a246807ddc83cacf6c335", "title": "Towards Training-free Anomaly Detection with Vision and Language Foundation Models", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "citationCount": 10, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-24", "authors": [{"authorId": "2333901831", "name": "Jinjin Zhang"}, {"authorId": "2293926901", "name": "Guodong Wang"}, {"authorId": "2229413102", "name": "Yizhou Jin"}, {"authorId": "2148909376", "name": "Di Huang"}], "abstract": "Anomaly detection is valuable for real-world applications, such as industrial quality inspection. However, most approaches focus on detecting local structural anomalies while neglecting compositional anomalies incorporating logical constraints. In this paper, we introduce LogSAD, a novel multi-modal framework that requires no training for both Logical and Structural Anomaly Detection. First, we propose a match-of-thought architecture that employs advanced large multi-modal models (i.e. GPT-4V) to generate matching proposals, formulating interests and compositional rules of thought for anomaly detection. Second, we elaborate on multi-granularity anomaly detection, consisting of patch tokens, sets of interests, and composition matching with vision and language foundation models. Subsequently, we present a calibration module to align anomaly scores from different detectors, followed by integration strategies for the final decision. Consequently, our approach addresses both logical and structural anomaly detection within a unified framework and achieves state-of-the-art results without the need for training, even when compared to supervised approaches, highlighting its robustness and effectiveness. Code is available at https://github.com/zhang0jhon/LogSAD."}
{"paperId": "f76db4b6d90f7abf9dc0a51c4cb909b35bf330dd", "url": "https://www.semanticscholar.org/paper/f76db4b6d90f7abf9dc0a51c4cb909b35bf330dd", "title": "A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs", "venue": "arXiv.org", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.04035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-08-06", "authors": [{"authorId": "2375048987", "name": "Zakariya Ba Alawi"}], "abstract": "This paper presents a comprehensive comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks, focusing on their usability, performance, and deployment trade-offs. We review each framework's programming paradigm and developer experience, contrasting TensorFlow's graph-based (now optionally eager) approach with PyTorch's dynamic, Pythonic style. We then compare model training speeds and inference performance across multiple tasks and data regimes, drawing on recent benchmarks and studies. Deployment flexibility is examined in depth - from TensorFlow's mature ecosystem (TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript support) to PyTorch's newer production tools (TorchScript compilation, ONNX export, and TorchServe). We also survey ecosystem and community support, including library integrations, industry adoption, and research trends (e.g., PyTorch's dominance in recent research publications versus TensorFlow's broader tooling in enterprise). Applications in computer vision, natural language processing, and other domains are discussed to illustrate how each framework is used in practice. Finally, we outline future directions and open challenges in deep learning framework design, such as unifying eager and graph execution, improving cross-framework interoperability, and integrating compiler optimizations (XLA, JIT) for improved speed. Our findings indicate that while both frameworks are highly capable for state-of-the-art deep learning, they exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored in research, whereas TensorFlow provides a fuller production-ready ecosystem - understanding these trade-offs is key for practitioners selecting the appropriate tool. We include charts, code snippets, and more than 20 references to academic papers and official documentation to support this comparative analysis"}
{"paperId": "f7fda04c653b44b043735cb4a3c108c84131e067", "url": "https://www.semanticscholar.org/paper/f7fda04c653b44b043735cb4a3c108c84131e067", "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation", "venue": "International Conference on Learning Representations", "year": 2025, "citationCount": 61, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.05485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-02-08", "authors": [{"authorId": "2342770333", "name": "Yi Li"}, {"authorId": "2342638505", "name": "Yuquan Deng"}, {"authorId": "2342621865", "name": "Jesse Zhang"}, {"authorId": "2333419032", "name": "Joel Jang"}, {"authorId": "2120036970", "name": "Marius Memmel"}, {"authorId": "1834058", "name": "Caelan Reed Garrett"}, {"authorId": "2257194867", "name": "Fabio Ramos"}, {"authorId": "2330398302", "name": "Dieter Fox"}, {"authorId": "2342635780", "name": "†. AnqiLi"}, {"authorId": "2342632973", "name": "†. AbhishekGupta"}, {"authorId": "47989608", "name": "Ankit Goyal"}, {"authorId": "2334017135", "name": "Nvidia"}], "abstract": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/"}
{"paperId": "f86daaee3614eb3be383e092d23d74cec89fa606", "url": "https://www.semanticscholar.org/paper/f86daaee3614eb3be383e092d23d74cec89fa606", "title": "A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation", "venue": "", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2512.00027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Review"], "publicationDate": "2025-11-06", "authors": [{"authorId": "2238007806", "name": "Nivedan Yakolli"}, {"authorId": "2080111465", "name": "A. Gautam"}, {"authorId": "2395712649", "name": "Abhijit Das"}, {"authorId": "2396448323", "name": "Yuankai Qi"}, {"authorId": "143809700", "name": "V. S. Shekhawat"}], "abstract": "Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response."}
{"paperId": "f88525f4c23b6255116d87faa1ff36d2f6e67dc0", "url": "https://www.semanticscholar.org/paper/f88525f4c23b6255116d87faa1ff36d2f6e67dc0", "title": "MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-27", "authors": [{"authorId": "2294759669", "name": "Prasham Titiya"}, {"authorId": "2330898567", "name": "Jainil Trivedi"}, {"authorId": "2257175735", "name": "Chitta Baral"}, {"authorId": "2365333893", "name": "Vivek Gupta"}], "abstract": "Multimodal tables those that integrate semi structured data with visual elements such as charts and maps are ubiquitous across real world domains, yet they pose a formidable challenge to current vision language models (VLMs). While Large Language models (LLMs) and VLMs have demonstrated strong capabilities in text and image understanding, their performance on complex, real world multimodal table reasoning remains unexplored. To bridge this gap, we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of 500 real world multimodal tables drawn from diverse real world sources, with a total of 4021 question answer pairs. MMTBENCH questions cover four question types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning types (Mathematical, Extrema Identification, Fact Verification, Vision Based, and Others), and eight table types (Single/Multiple Entity, Maps and Charts with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive evaluation of state of the art models on all types reveals substantial performance gaps, particularly on questions requiring visual-based reasoning and multi-step inference. These findings show the urgent need for improved architectures that more tightly integrate vision and language processing. By providing a challenging, high-quality resource that mirrors the complexity of real-world tasks, MMTBENCH underscores its value as a resource for future research on multimodal tables."}
{"paperId": "f94c91a1357f5195e4b4b5a71b714991a5e47aa6", "url": "https://www.semanticscholar.org/paper/f94c91a1357f5195e4b4b5a71b714991a5e47aa6", "title": "Performance Analysis of Vision-Language Pre-Training Models on Unbiased VQA Tasks", "venue": "2025 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACDSA65407.2025.11166129?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACDSA65407.2025.11166129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-07", "authors": [{"authorId": "2349611711", "name": "Aya Nuseir"}], "abstract": "This study evaluates the performance of Visual Language Pre-training (VLP) models on a colour recognition task using unbiased Visual Question Answering (VQA) datasets. We focused on three state-of-the-art multimodal VLP models: Vision-and-Language Transformer (ViLT), Generative Image-to-text Transformer (GIT), and Bootstrapping Language-Image Pre-training (BLIP). To assess these models, we developed three novel balanced datasets with binary question-answer pairs, each designed to detect the colour of objects. The datasets differ in image creation methods, including fully synthetic images, photographs of coloured objects, and varied objects with realistic backgrounds. Our results indicate that dataset characteristics significantly influence model performance. ViLT consistently performed well on simpler binary datasets, achieving an accuracy of around 62%, but struggled with more complex formats. GIT showed stable performance in simpler binary datasets but had difficulty with complex questions, especially in combined colour identification tasks. BLIP is highly adaptable, excelling in complex question formats with perfect accuracy in colour identification tasks. 1"}
{"paperId": "f9a10db458c02e6c9044d9eb9c068b0a723912ac", "url": "https://www.semanticscholar.org/paper/f9a10db458c02e6c9044d9eb9c068b0a723912ac", "title": "OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.05656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-06", "authors": [{"authorId": "2379650149", "name": "Bo Lyu"}, {"authorId": "2390960176", "name": "Yubo Cui"}, {"authorId": "2296852863", "name": "Tuo Shi"}, {"authorId": "2379934646", "name": "Ke Li"}], "abstract": "Neural architecture search (NAS) is a hard computationally expensive optimization problem with a discrete, vast, and spiky search space. One of the key research efforts dedicated to this space focuses on accelerating NAS via certain proxy evaluations of neural architectures. Different from the prevalent predictor-based methods using surrogate models and differentiable architecture search via supernetworks, we propose an optimization proxy to streamline the NAS as an end-to-end optimization framework, named OptiProxy-NAS. In particular, using a proxy representation, the NAS space is reformulated to be continuous, differentiable, and smooth. Thereby, any differentiable optimization method can be applied to the gradient-based search of the relaxed architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$ search spaces across three different domains including computer vision, natural language processing, and resource-constrained NAS fully demonstrate the superior search results and efficiency. Further experiments on low-fidelity scenarios verify the flexibility."}
{"paperId": "f9bcad33db7eb0781b81d86be5943004099301a6", "url": "https://www.semanticscholar.org/paper/f9bcad33db7eb0781b81d86be5943004099301a6", "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2510.03339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-10-02", "authors": [{"authorId": "2187458074", "name": "Sofiane Ennadir"}, {"authorId": "2333887053", "name": "Levente Z'olyomi"}, {"authorId": "2295510847", "name": "Oleg Smirnov"}, {"authorId": "2295697958", "name": "Tianze Wang"}, {"authorId": "2384123909", "name": "John Pertoft"}, {"authorId": "2333905810", "name": "Filip Cornell"}, {"authorId": "2328683627", "name": "Lele Cao"}], "abstract": "Transformer models have become the dominant backbone for sequence modeling, leveraging self-attention to produce contextualized token representations. These are typically aggregated into fixed-size vectors via pooling operations for downstream tasks. While much of the literature has focused on attention mechanisms, the role of pooling remains underexplored despite its critical impact on model behavior. In this paper, we introduce a theoretical framework that rigorously characterizes the expressivity of Transformer-based models equipped with widely used pooling methods by deriving closed-form bounds on their representational capacity and the ability to distinguish similar inputs. Our analysis extends to different variations of attention formulations, demonstrating that these bounds hold across diverse architectural variants. We empirically evaluate pooling strategies across tasks requiring both global and local contextual understanding, spanning three major modalities: computer vision, natural language processing, and time-series analysis. Results reveal consistent trends in how pooling choices affect accuracy, sensitivity, and optimization behavior. Our findings unify theoretical and empirical perspectives, providing practical guidance for selecting or designing pooling mechanisms suited to specific tasks. This work positions pooling as a key architectural component in Transformer models and lays the foundation for more principled model design beyond attention alone."}
{"paperId": "f9cda232e7c39dfe2efaf5707180c57d3131c3dc", "url": "https://www.semanticscholar.org/paper/f9cda232e7c39dfe2efaf5707180c57d3131c3dc", "title": "Retail Transformation in Healthcare: A Comprehensive Framework for Intelligent Eye Care Services Integration", "venue": "International Conference on Artificial Intelligence in Engineering and Technology", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IICAIET67254.2025.11265245?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IICAIET67254.2025.11265245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["Conference"], "publicationDate": "2025-08-26", "authors": [{"authorId": "2396514478", "name": "Kapil Kumar Reddy Poreddy"}, {"authorId": "30700723", "name": "A. K. Sahu"}], "abstract": "The integration of artificial intelligence (AI) into healthcare within retail settings marks a transformative shift in how customers engage with services and how healthcare is delivered. This paper presents a comprehensive analysis of AI-driven transformation in retail healthcare, specifically focusing on eye care services integration. We examine the implementation of advanced AI technologies including computer vision, natural language processing, optical character recognition, and large language models to create seamless healthcare experiences within traditional retail frameworks. Our research demonstrates how AI enables real-time eye examinations, personalized eyewear recommendations, digital prescription validation, and predictive health screening through consumer devices. The proposed framework transforms routine shopping experiences into proactive healthcare interactions, achieving significant improvements in customer satisfaction, healthcare accessibility, and clinical outcomes. Performance evaluation indicates 85% improvement in customer engagement rates and 92% accuracy in AI-powered prescription validation compared to traditional methods."}
{"paperId": "fa64e81ad6ff1676007d1c8c1d8087575f8d79cc", "url": "https://www.semanticscholar.org/paper/fa64e81ad6ff1676007d1c8c1d8087575f8d79cc", "title": "Proc4Gem: Foundation models for physical agency through procedural generation", "venue": "arXiv.org", "year": 2025, "citationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-11", "authors": [{"authorId": "2349544825", "name": "Yixin Lin"}, {"authorId": "2066450521", "name": "Jan Humplik"}, {"authorId": "2268435534", "name": "Sandy H. Huang"}, {"authorId": "40401956", "name": "Leonard Hasenclever"}, {"authorId": "2283762733", "name": "Francesco Romano"}, {"authorId": "2099818486", "name": "Stefano Saliceti"}, {"authorId": "2337362505", "name": "Daniel Zheng"}, {"authorId": "2135278508", "name": "José Enrique Chen"}, {"authorId": "2292141175", "name": "Catarina Barros"}, {"authorId": "69041729", "name": "Adrian Collister"}, {"authorId": "2349752516", "name": "Matt Young"}, {"authorId": "2218899126", "name": "Adil Dostmohamed"}, {"authorId": "2160887670", "name": "Ben Moran"}, {"authorId": "2264799885", "name": "Ken Caluwaerts"}, {"authorId": "6891740", "name": "M. Giustina"}, {"authorId": "2284725355", "name": "Joss Moore"}, {"authorId": "2349540261", "name": "Kieran Connell"}, {"authorId": "2275201917", "name": "Francesco Nori"}, {"authorId": "2801204", "name": "N. Heess"}, {"authorId": "1832575", "name": "Steven Bohez"}, {"authorId": "2631257", "name": "Arunkumar Byravan"}], "abstract": "In robot learning, it is common to either ignore the environment semantics, focusing on tasks like whole-body control which only require reasoning about robot-environment contacts, or conversely to ignore contact dynamics, focusing on grounding high-level movement in vision and language. In this work, we show that advances in generative modeling, photorealistic rendering, and procedural generation allow us to tackle tasks requiring both. By generating contact-rich trajectories with accurate physics in semantically-diverse simulations, we can distill behaviors into large multimodal models that directly transfer to the real world: a system we call Proc4Gem. Specifically, we show that a foundation model, Gemini, fine-tuned on only simulation data, can be instructed in language to control a quadruped robot to push an object with its body to unseen targets in unseen real-world environments. Our real-world results demonstrate the promise of using simulation to imbue foundation models with physical agency. Videos can be found at our website: https://sites.google.com/view/proc4gem"}
{"paperId": "fa6b7c42de7985b5f5703e577e2664bdea86746b", "url": "https://www.semanticscholar.org/paper/fa6b7c42de7985b5f5703e577e2664bdea86746b", "title": "Multimodal model for knee osteoarthritis KL grading from plain radiograph", "venue": "Journal of X-Ray Science and Technology", "year": 2025, "citationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/08953996251314765?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/08953996251314765, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-17", "authors": [{"authorId": "116912267", "name": "M. Ma'aitah"}, {"authorId": "20588751", "name": "A. Helwan"}, {"authorId": "2337615725", "name": "Abdelrahman Radwan"}, {"authorId": "2350566600", "name": "Adnan Mohammad Salem Manasreh"}, {"authorId": "2183557010", "name": "E. A. Alshareef"}], "abstract": "Knee osteoarthritis presents a significant health challenge for many adults globally. At present, there are no pharmacological treatments that can cure this medical condition. The primary method for managing the progress of knee osteoarthritis is through early identification. Currently, X-ray imaging serves as a key modality for predicting the onset of osteoarthritis. Nevertheless, the traditional manual interpretation of X-rays is susceptible to inaccuracies, largely due to the varying levels of expertise among radiologists. In this paper, we propose a multimodal model based on pre-trained vision and language models for the identification of the knee osteoarthritis severity Kellgren-Lawrence (KL) grading. Using Vision transformer and Pre-training of deep bidirectional transformers for language understanding (BERT) for images and texts embeddings extraction helps Transformer encoders extracts more distinctive hidden-states that facilitates the learning process of the neural network classifier. The multimodal model was trained and tested on the OAI dataset, and the results showed remarkable performance compared to the related works. Experimentally, the evaluation of the model on the test set comprising X-ray images demonstrated an overall accuracy of 82.85%, alongside a precision of 84.54% and a recall of 82.89%."}
{"paperId": "faade27347173e2abeded4a69c987d11b2e90dcb", "url": "https://www.semanticscholar.org/paper/faade27347173e2abeded4a69c987d11b2e90dcb", "title": "Advancing Code Generation from Visual Designs through Transformer-Based Architectures and Specialized Datasets", "venue": "Proc. ACM Hum. Comput. Interact.", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3734190?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3734190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-27", "authors": [{"authorId": "2170697820", "name": "Tommaso Calò"}, {"authorId": "66596061", "name": "Luigi De Russis"}], "abstract": null}
{"paperId": "fada01a21d9b87b04da34d60becccf3d9fa89b71", "url": "https://www.semanticscholar.org/paper/fada01a21d9b87b04da34d60becccf3d9fa89b71", "title": "Natural language processing as Digital Veda (डिजिटल वेद): a humanistic framework for language, ethics, and AI", "venue": "Digital Scholarship in the Humanities", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1093/llc/fqaf100?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/llc/fqaf100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-20", "authors": [{"authorId": "2334051908", "name": "Akshi Kumar"}, {"authorId": "73770278", "name": "S. R. Sangwan"}], "abstract": "\n This article conceptualizes Natural Language Processing (NLP) as the Digital Veda (डिजिटल वेद), framing it as a culturally rooted communicative infrastructure inspired by the Vedic tradition of structured knowledge preservation and ethical discourse. Drawing on India’s linguistic and philosophical heritage, it positions NLP not as a neutral tool, but as an evolving ecosystem shaped by human values, language ideologies, and socio-cultural narratives. By mapping the four Vedas to key NLP domains, Rigveda (language modelling), Yajurveda (syntax and pipelines), Samaveda (phonetics and speech), and Atharvaveda (applied AI), the study illustrates how contemporary language technologies mirror ancient systems of meaning-making. It offers a critical, decolonial lens on mainstream NLP, highlighting digital language hierarchies, the marginalization of low-resource Indian languages, and biases embedded in large language model (LLM) training data. The article further proposes a Vedic-inspired ethical AI framework, grounded in the principles of Dharma (righteous design), Ahimsa (non-harm), and Moksha (AI for truth and well-being). This interdisciplinary perspective contributes to a more inclusive, context-aware vision for language technologies, with practical applications in multilingual NLP, bias mitigation, and ethically aligned AI governance. It is particularly relevant for AI ethicists, digital humanists, NLP researchers, and policymakers committed to culturally informed, responsible innovation."}
{"paperId": "fb4976064fb87179f77105ac68bca82e1d24acc9", "url": "https://www.semanticscholar.org/paper/fb4976064fb87179f77105ac68bca82e1d24acc9", "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.16654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-20", "authors": [{"authorId": "2378858705", "name": "Chenghao Liu"}, {"authorId": "2376601931", "name": "Zhimu Zhou"}, {"authorId": "2376800622", "name": "Jiachen Zhang"}, {"authorId": "2376543541", "name": "Minghao Zhang"}, {"authorId": "2377237873", "name": "Songfang Huang"}, {"authorId": "2376533531", "name": "Huiling Duan"}], "abstract": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a\"black-box\"paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL)."}
{"paperId": "fb8c82bda994568fd85b8146b5d928fc504db465", "url": "https://www.semanticscholar.org/paper/fb8c82bda994568fd85b8146b5d928fc504db465", "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "venue": "arXiv.org", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.06727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-06-07", "authors": [{"authorId": "2366140787", "name": "Can Li"}, {"authorId": "2362840305", "name": "Ting Zhang"}, {"authorId": "2360256464", "name": "Mei Wang"}, {"authorId": "2340655868", "name": "Hua Huang"}], "abstract": "Large Multimodal Models have achieved remarkable progress in integrating vision and language, enabling strong performance across perception, reasoning, and domain-specific tasks. However, their capacity to reason over multiple, visually similar inputs remains insufficiently explored. Such fine-grained comparative reasoning is central to real-world tasks, especially in mathematics and education, where learners must often distinguish between nearly identical diagrams to identify correct solutions. To address this gap, we present VisioMath, a curated benchmark of 1,800 high-quality K-12 mathematics problems in which all candidate answers are diagrams with subtle visual similarities. A comprehensive evaluation of state-of-the-art LMMs, covering both leading closed-source systems and widely adopted open-source models, reveals a consistent decline in accuracy as inter-image similarity increases. Analysis indicates that the dominant failure mode stems from image-text misalignment: rather than grounding reasoning in textual cues, models often resort to shallow positional heuristics, resulting in systematic errors. We further explore three alignment-oriented strategies, spanning training-free approaches and finetuning, and achieve substantial accuracy gains. We hope that VisioMath will serve as a rigorous benchmark and catalyst for developing LMMs toward deeper diagram understanding, precise comparative reasoning, and grounded multi-image-text integration."}
{"paperId": "fbae0084553deea24dbac6bd6efbeea24f24d604", "url": "https://www.semanticscholar.org/paper/fbae0084553deea24dbac6bd6efbeea24f24d604", "title": "Applying cross-modal feature alignment and fusion for effective sarcasm detection", "venue": "Progress in Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s13748-025-00370-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s13748-025-00370-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-16", "authors": [{"authorId": "2361968586", "name": "Sarang P. Karun"}, {"authorId": "2380364646", "name": "Adithya Venugopalan"}], "abstract": null}
{"paperId": "fc1e2130f54c99d0c63b0d4cd50ac8ad1ea85e2e", "url": "https://www.semanticscholar.org/paper/fc1e2130f54c99d0c63b0d4cd50ac8ad1ea85e2e", "title": "Feature Design for Bridging SAM and CLIP Toward Referring Image Segmentation", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV61041.2025.00811?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV61041.2025.00811, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2312351", "name": "Koichiro Ito"}], "abstract": "Referring Image Segmentation (RIS) is a task aimed at segmenting objects expressed in natural language within an image. This task requires an understanding of the relationship between vision and language, along with precise segmentation capabilities. In the field of computer vision, CLIP and Segment anything model (SAM) have gained significant attention for their classification and the segmentation capabilities. Given that both models possess essential skills for RIS, combining them seems to be an effective strategy. In this paper, we propose a model that integrates CLIP and SAM to enhance RIS. Since SAM lacks classification capabilities, we developed a module that supplies the SAM mask decoder with features that specify the target object. We introduce a new module, which is trained on additional instance segmentation tasks. The features utilized and derived from this module serve as inputs for the SAM decoder. With these inputs, SAM is expected to effectively segment areas corresponding to the given naturallanguage expressions. We conducted experiments using the traditional RefCOCO/+/g, as well as the recently introduced gRefCOCO and Refrom datasets, demonstrating the advantages of our approach. Code will be available on https://github.com/hitachi-rd-cv/dfam."}
{"paperId": "fc8ab7ba2ab6d111c4eb7dfb51d760d2086b13f7", "url": "https://www.semanticscholar.org/paper/fc8ab7ba2ab6d111c4eb7dfb51d760d2086b13f7", "title": "Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2508.14082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-08-12", "authors": [{"authorId": "2339548318", "name": "Ye Su"}, {"authorId": "2065400224", "name": "H. Qiao"}, {"authorId": "2355656663", "name": "Wei Huang"}, {"authorId": "2339640445", "name": "Lin Chen"}], "abstract": "Semi-supervised regression (SSR), which aims to predict continuous scores of samples while reducing reliance on a large amount of labeled data, has recently received considerable attention across various applications, including computer vision, natural language processing, and audio and medical analysis. Existing semi-supervised methods typically apply consistency regularization on the general regression task by generating pseudo-labels. However, these methods heavily rely on the quality of pseudo-labels, and direct regression fails to learn the label distribution and can easily lead to overfitting. To address these challenges, we introduce an end-to-end Decoupled Representation distillation framework (DRILL) which is specially designed for the semi-supervised regression task where we transform the general regression task into a Discrete Distribution Estimation (DDE) task over multiple buckets to better capture the underlying label distribution and mitigate the risk of overfitting associated with direct regression. Then we employ the Decoupled Distribution Alignment (DDA) to align the target bucket and non-target bucket between teacher and student on the distribution of buckets, encouraging the student to learn more robust and generalized knowledge from the teacher. Extensive experiments conducted on datasets from diverse domains demonstrate that the proposed DRILL has strong generalization and outperforms the competing methods."}
{"paperId": "fc95bc853a16e37d50e3beeb90a53c50e63c9a6b", "url": "https://www.semanticscholar.org/paper/fc95bc853a16e37d50e3beeb90a53c50e63c9a6b", "title": "Asymmetric Visual Semantic Embedding Framework for Efficient Vision-Language Alignment", "venue": "AAAI Conference on Artificial Intelligence", "year": 2025, "citationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.06974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-10", "authors": [{"authorId": "2313463499", "name": "Yang Liu"}, {"authorId": "2317672838", "name": "Meng Liu"}, {"authorId": "2237407099", "name": "Shudong Huang"}, {"authorId": "2293779369", "name": "Jiancheng Lv"}], "abstract": "Learning visual semantic similarity is a critical challenge in bridging the gap between images and texts. However, there exist inherent variations between vision and language data, such as information density, i.e., images can contain textual information from multiple different views, which makes it difficult to compute the similarity between these two modalities accurately and efficiently. In this paper, we propose a novel framework called Asymmetric Visual Semantic Embedding (AVSE) to dynamically select features from various regions of images tailored to different textual inputs for similarity calculation.\nTo capture information from different views in the image, we design a radial bias sampling module to sample image patches and obtain image features from various views, Furthermore, AVSE introduces a novel module for efficient computation of visual semantic similarity between asymmetric image and text embeddings.\n Central to this module is the presumption of foundational semantic units within the embeddings, denoted as ``meta-semantic embeddings.\" It segments all embeddings into meta-semantic embeddings with the same dimension and calculates visual semantic similarity by finding the optimal match of meta-semantic embeddings of two modalities. \nOur proposed AVSE model is extensively evaluated on the large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority over recent state-of-the-art methods."}
{"paperId": "fd337dc4cb83330ae9d7593127eb899453335596", "url": "https://www.semanticscholar.org/paper/fd337dc4cb83330ae9d7593127eb899453335596", "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-25", "authors": [{"authorId": "2108051142", "name": "Yusen Zhang"}, {"authorId": "2354222124", "name": "Wenliang Zheng"}, {"authorId": "2357723858", "name": "Aashrith Madasu"}, {"authorId": "2055357805", "name": "Peng Shi"}, {"authorId": "83757854", "name": "Ryo Kamoi"}, {"authorId": "2358316739", "name": "Hao Zhou"}, {"authorId": "2328293004", "name": "Zhuoyang Zou"}, {"authorId": "2358521725", "name": "Shu Zhao"}, {"authorId": "1456156441", "name": "Sarkar Snigdha Sarathi Das"}, {"authorId": "2110652561", "name": "Vipul Gupta"}, {"authorId": "2266470204", "name": "Xiaoxin Lu"}, {"authorId": "2266469940", "name": "Nan Zhang"}, {"authorId": "2261450671", "name": "Ranran Haoran Zhang"}, {"authorId": "2357720946", "name": "Avitej Iyer"}, {"authorId": "2118614649", "name": "Renze Lou"}, {"authorId": "2265583938", "name": "Wenpeng Yin"}, {"authorId": "2329314489", "name": "Rui Zhang"}], "abstract": "High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research."}
{"paperId": "fd499b102e96b0eb5b4e575f804b4fbe8e459487", "url": "https://www.semanticscholar.org/paper/fd499b102e96b0eb5b4e575f804b4fbe8e459487", "title": "From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.04815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-07-07", "authors": [{"authorId": "25199716", "name": "Mihai Masala"}, {"authorId": "2243189217", "name": "Marius Leordeanu"}], "abstract": "The task of describing video content in natural language is commonly referred to as video captioning. Unlike conventional video captions, which are typically brief and widely available, long-form paragraph descriptions in natural language are scarce. This limitation of current datasets is due to the expensive human manual annotation required and to the highly challenging task of explaining the language formation process from the perspective of the underlying story, as a complex system of interconnected events in space and time. Through a thorough analysis of recently published methods and available datasets, we identify a general lack of published resources dedicated to the problem of describing videos in complex language, beyond the level of descriptions in the form of enumerations of simple captions. Furthermore, while state-of-the-art methods produce impressive results on the task of generating shorter captions from videos by direct end-to-end learning between the videos and text, the problem of explaining the relationship between vision and language is still beyond our reach. In this work, we propose a shared representation between vision and language, based on graphs of events in space and time, which can be obtained in an explainable and analytical way, to integrate and connect multiple vision tasks to produce the final natural language description. Moreover, we also demonstrate how our automated and explainable video description generation process can function as a fully automatic teacher to effectively train direct, end-to-end neural student pathways, within a self-supervised neuro-analytical system. We validate that our explainable neuro-analytical approach generates coherent, rich and relevant textual descriptions on videos collected from multiple varied datasets, using both standard evaluation metrics, human annotations and consensus from ensembles of state-of-the-art VLMs."}
{"paperId": "fdf105b4517f491fcdd4cc9f42edde2989639fa4", "url": "https://www.semanticscholar.org/paper/fdf105b4517f491fcdd4cc9f42edde2989639fa4", "title": "CLAIRE: Composable Chiplet Libraries for AI Inference", "venue": "Design, Automation and Test in Europe", "year": 2025, "citationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23919/DATE64628.2025.10992960?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23919/DATE64628.2025.10992960, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-03-31", "authors": [{"authorId": "2218867248", "name": "P. S. Nalla"}, {"authorId": "2362579224", "name": "Emad Haque"}, {"authorId": "2293577816", "name": "Yaotian Liu"}, {"authorId": "1761662", "name": "S. Sapatnekar"}, {"authorId": "2293551719", "name": "Jeff Zhang"}, {"authorId": "2280953081", "name": "Chaitali Chakrabarti"}, {"authorId": "2293652588", "name": "Yu Cao"}], "abstract": "Artificial intelligence has made a significant impact on fields like computer vision, Natural Language Processing (NLP), healthcare, and robotics. However, recent AI models, such as GPT-4 and LLaMAv3, demand significant number of computational resources, pushing monolithic chips to their technological and practical limits. 2.5D chiplet-based heterogeneous architectures have been proposed to address these technological and practical limits. While chiplet optimization for models like Convolutional Neural Networks (CNNs) is well-established, scaling this approach to accommodate diverse AI inference models with different computing primitives, data volumes, and different chiplet sizes is very challenging. A set of hardened IPs and chiplet libraries optimized for a broad range of AI applications is proposed in this work. We derive the set of chiplet configurations that are composable, scalable and reusable by employing an analytical framework trained on a diverse set of AI algorithms. Testing these set of library synthesized configurations on a different set of algorithms, we achieve a $1.99\\times-3.99\\times$ improvement in non-recurring engineering (NRE) chiplet design costs, with minimal performance overhead compared to custom chiplet-based ASIC designs. Similar to soft IPs for SoC development, the library of chiplets improves flexibility, reusability, and efficiency for AI hardware designs."}
{"paperId": "fe7a7373076c5cfe10785da4cadb27eb2a198ad7", "url": "https://www.semanticscholar.org/paper/fe7a7373076c5cfe10785da4cadb27eb2a198ad7", "title": "Breaking Away From AI: The Ontological and Ethical Evolution of Machine Learning", "venue": "IEEE Access", "year": 2025, "citationCount": 9, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2025.3553032", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3553032?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3553032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2164600937", "name": "Enrico Barbierato"}, {"authorId": "2280212941", "name": "Alice Gatti"}, {"authorId": "146092686", "name": "A. Incremona"}, {"authorId": "2226580092", "name": "Andrea Pozzi"}, {"authorId": "2451187", "name": "Daniele Toti"}], "abstract": "Machine Learning (ML) has historically been associated with Artificial Intelligence (AI) but has developed into an independent discipline. This paper argues for the ontological independence of ML, driven by its unique methodologies, applications, and ethical considerations. A bibliometric analysis reveals that ML research output (494,572 publications from 2017–2023) surpasses AI (283,762 publications) by 74%, reflecting its rapid growth and specialization. Unlike AI’s pursuit of general intelligence and symbolic reasoning, ML focuses on data-driven performance optimization, with impactful applications in computer vision, natural language processing (NLP), and autonomous systems. The study highlights ethical challenges—such as addressing algorithmic bias (50 occurrences), fairness (2,778 publications), and environmental sustainability (283 related works)—which emphasize the need for dedicated ethical frameworks tailored to ML. These findings propose a conceptual and practical separation between ML and AI to enable targeted research, interdisciplinary collaboration, and solutions to challenges like explainability, transparency, and sustainability. The paper underscores the importance of recognizing ML’s independence in advancing both fields."}
{"paperId": "fea931faedb293201a5b79abc98aaffccdb495b3", "url": "https://www.semanticscholar.org/paper/fea931faedb293201a5b79abc98aaffccdb495b3", "title": "Click&Describe: Multimodal Grounding and Tracking for Aerial Objects", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV61041.2025.00586?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV61041.2025.00586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-02-26", "authors": [{"authorId": "2349099251", "name": "Rupanjali Kukal"}, {"authorId": "2349098120", "name": "Jay Patravali"}, {"authorId": "2306760580", "name": "Fuxun Yu"}, {"authorId": "2349208151", "name": "Simranjit Singh"}, {"authorId": "2474219", "name": "Nikolaos Karianakis"}, {"authorId": "2339364764", "name": "R. Madhok"}], "abstract": "The fusion of multiple modalities, such as vision and language, has led to significant progress in grounding and tracking tasks. However, this success has not yet translated to aerial single-object tracking (SOT) due to the lack of text annotations in existing aerial SOT datasets. To overcome this limitation, we provide text annotations for five existing aerial datasets, designed to support and promote multi-modal research in the aerial tracking domain. Furthermore, to address challenges such as small object dimensions, similar-looking objects, and target size fluctuations, we introduce a third input modality: click (or point prompt). We seamlessly integrate click and language information in the model's input to offer a user-friendly and interactive alternative to precise bounding box annotations. This enables approximate target specification with reduced effort and time. We introduce CLaVi, a novel multimodal framework that redefines input interaction by in-corporating multiple modalities. This integration improves target localization and tracking efficiency, providing a significant advancement in the way input is provided to the model. Furthermore, we conduct experiments on the five datasets, to provide AerTrack-460 benchmark, to validate the effectiveness of our approach. AerTrack-460 benchmark shows competitive performance and, in some cases, outperforms previous language-based grounding and tracking techniques, setting a strong baseline for future research. Code and data will be made available soon."}
{"paperId": "feb70f39da4331e287c5adcb6000b67509b661e9", "url": "https://www.semanticscholar.org/paper/feb70f39da4331e287c5adcb6000b67509b661e9", "title": "Cross-modal Prompt-Driven Network for low-resource vision-to-language generation", "venue": "Engineering applications of artificial intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2024.109591?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2024.109591, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-01", "authors": [{"authorId": "2330586305", "name": "Yuena Jiang"}, {"authorId": "2330570628", "name": "Yanxun Chang"}], "abstract": null}
{"paperId": "ff5ea56c797820d21bd2300eaebf6239272220e8", "url": "https://www.semanticscholar.org/paper/ff5ea56c797820d21bd2300eaebf6239272220e8", "title": "Agent-in-the-loop to distill expert knowledge into artificial intelligence models: a survey", "venue": "Artificial Intelligence Review", "year": 2025, "citationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10462-025-11255-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10462-025-11255-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2025-06-04", "authors": [{"authorId": "2292733359", "name": "Jiayuan Gao"}, {"authorId": "2145025105", "name": "Yingwei Zhang"}, {"authorId": "2259969518", "name": "Yiqiang Chen"}, {"authorId": "2366382823", "name": "Yihan Dong"}, {"authorId": "2390464017", "name": "Yuanzhe Chen"}, {"authorId": "2243324804", "name": "Shuchao Song"}, {"authorId": "2292488219", "name": "Boshi Tang"}, {"authorId": "2365997390", "name": "Yang Gu"}], "abstract": null}
{"paperId": "ff94d380c8def355bd7a2abb2ee6cf584331d6f5", "url": "https://www.semanticscholar.org/paper/ff94d380c8def355bd7a2abb2ee6cf584331d6f5", "title": "Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.11840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-15", "authors": [{"authorId": "1990830174", "name": "Tim Lebailly"}, {"authorId": "2352797635", "name": "Vijay Veerabadran"}, {"authorId": "2150275", "name": "Satwik Kottur"}, {"authorId": "2352797461", "name": "Karl Ridgeway"}, {"authorId": "67294118", "name": "Michael L. Iuzzolino"}], "abstract": "Generative vision-language models (VLMs) exhibit strong high-level image understanding but lack spatially dense alignment between vision and language modalities, as our findings indicate. Orthogonal to advancements in generative VLMs, another line of research has focused on representation learning for vision-language alignment, targeting zero-shot inference for dense tasks like segmentation. In this work, we bridge these two directions by densely aligning images with synthetic descriptions generated by VLMs. Synthetic captions are inexpensive, scalable, and easy to generate, making them an excellent source of high-level semantic understanding for dense alignment methods. Empirically, our approach outperforms prior work on standard zero-shot open-vocabulary segmentation benchmarks/datasets, while also being more data-efficient."}
{"paperId": "ffd6c6b7970bc4a5524b1e5af190f6ca51b922d8", "url": "https://www.semanticscholar.org/paper/ffd6c6b7970bc4a5524b1e5af190f6ca51b922d8", "title": "Multi-Sourced Compositional Generalization in Visual Question Answering", "venue": "International Joint Conference on Artificial Intelligence", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-05-29", "authors": [{"authorId": "2294251099", "name": "Chuanhao Li"}, {"authorId": "2335860022", "name": "Wenbo Ye"}, {"authorId": "2233231737", "name": "Zhen Li"}, {"authorId": "2302812287", "name": "Yuwei Wu"}, {"authorId": "2249719808", "name": "Yunde Jia"}], "abstract": "Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-and-language (V&L) recently. Due to the multi-modal nature of V&L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions. However, the generalization ability over multi-sourced novel compositions, i.e., multi-sourced compositional generalization (MSCG) remains unexplored. In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities. Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model. This process helps the model learn consistent representations for the same semantic primitives across different modalities. To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities. The GQA-MSCG dataset is available at https://github.com/NeverMoreLCH/MSCG."}
{"paperId": "ffee64da2f249225ade4d8b12cc4ba9a26a878a5", "url": "https://www.semanticscholar.org/paper/ffee64da2f249225ade4d8b12cc4ba9a26a878a5", "title": "Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection", "venue": "arXiv.org", "year": 2025, "citationCount": 0, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2509.10850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "publicationTypes": ["JournalArticle"], "publicationDate": "2025-09-13", "authors": [{"authorId": "2365998904", "name": "Huynh T. T. Tran"}, {"authorId": "2342530769", "name": "Jacob Sander"}, {"authorId": "2342484279", "name": "Achraf Cohen"}, {"authorId": "3116427", "name": "B. Jalaeian"}, {"authorId": "2281162708", "name": "Nathaniel D. Bastian"}], "abstract": "Transfer learning is commonly utilized in various fields such as computer vision, natural language processing, and medical imaging due to its impressive capability to address subtasks and work with different datasets. However, its application in cybersecurity has not been thoroughly explored. In this paper, we present an innovative neurosymbolic AI framework designed for network intrusion detection systems, which play a crucial role in combating malicious activities in cybersecurity. Our framework leverages transfer learning and uncertainty quantification. The findings indicate that transfer learning models, trained on large and well-structured datasets, outperform neural-based models that rely on smaller datasets, paving the way for a new era in cybersecurity solutions."}
